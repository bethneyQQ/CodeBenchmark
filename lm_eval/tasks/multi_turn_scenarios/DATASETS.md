# Multi-Turn Scenarios - Dataset Documentation

## Dataset Overview

This document describes the datasets used by different multi-turn scenarios in the framework.

## Dataset Structure

All scenario datasets follow a consistent JSON Lines format with scenario-specific fields.

### Common Fields

```json
{
  "problem_id": "unique_identifier",
  "difficulty": "easy|medium|hard",
  "category": "scenario_category",
  "metadata": {
    "created": "timestamp",
    "version": "dataset_version"
  }
}
```

## Scenario-Specific Datasets

### 1. Code Review Dataset

**File**: `code_review_problems.jsonl`  
**Size**: 150+ problems  
**Languages**: Python, JavaScript, Java, C++, Go

```json
{
  "problem_id": "code_review_001",
  "code": "def factorial(n):\n    if n == 0:\n        return 1\n    return n * factorial(n-1)",
  "context": "Review this recursive factorial implementation",
  "language": "python",
  "complexity": "beginner",
  "focus_areas": ["efficiency", "edge_cases", "error_handling"],
  "expected_issues": [
    "No input validation",
    "Stack overflow for large n", 
    "No handling of negative numbers"
  ],
  "difficulty": "easy"
}
```

**Categories**:
- **Algorithms** (40%): Sorting, searching, dynamic programming
- **Data Structures** (30%): Lists, trees, graphs, hash tables  
- **System Code** (20%): File I/O, network, concurrency
- **Web Development** (10%): API, database, frontend

### 2. Iterative Problem Solving Dataset

**File**: `problem_solving_challenges.jsonl`  
**Size**: 100+ problems  
**Focus**: Optimization and refinement

```json
{
  "problem_id": "optimization_001",
  "problem": "Design an efficient algorithm to find the k largest elements in a stream of integers",
  "constraints": [
    "Memory usage should be O(k)",
    "Handle stream of unknown size",
    "Maintain insertion order for ties"
  ],
  "initial_requirements": "Basic functionality",
  "refinement_stages": [
    {
      "stage": 1,
      "feedback": "Consider space complexity",
      "hint": "Think about heap data structures"
    },
    {
      "stage": 2, 
      "feedback": "Optimize for very large streams",
      "hint": "Can you reduce constant factors?"
    }
  ],
  "difficulty": "hard"
}
```

**Problem Types**:
- **Algorithm Optimization** (50%): Time/space complexity improvements
- **System Design** (30%): Scalability and architecture
- **Code Quality** (20%): Refactoring and maintainability

### 3. Teaching Dialogue Dataset

**File**: `teaching_topics.jsonl`  
**Size**: 80+ topics  
**Levels**: Beginner, Intermediate, Advanced

```json
{
  "problem_id": "teach_binary_search",
  "topic": "Binary Search Algorithm",
  "level": "beginner",
  "learning_objectives": [
    "Understand divide and conquer principle",
    "Learn when binary search is applicable",
    "Implement binary search correctly",
    "Analyze time complexity"
  ],
  "student_background": "Basic programming knowledge, familiar with arrays",
  "key_concepts": [
    "Sorted array requirement",
    "Two-pointer technique", 
    "Loop invariants",
    "Edge cases"
  ],
  "common_misconceptions": [
    "Works on unsorted arrays",
    "Always faster than linear search",
    "Implementation details"
  ],
  "assessment_questions": [
    "When is binary search applicable?",
    "What is the time complexity?",
    "How do you handle edge cases?"
  ],
  "difficulty": "medium"
}
```

**Subject Areas**:
- **Algorithms & Data Structures** (40%): Core CS concepts
- **Programming Fundamentals** (25%): Variables, loops, functions
- **System Concepts** (20%): Memory, processes, networking  
- **Software Engineering** (15%): Design patterns, testing

### 4. Conversational Dataset

**File**: `conversation_scenarios.jsonl`  
**Size**: 120+ scenarios  
**Types**: Help-seeking, information exchange, creative discussion

```json
{
  "problem_id": "ml_learning_guidance",
  "initial_query": "I'm interested in learning machine learning but don't know where to start. I have some programming experience in Python.",
  "context": "Career development conversation",
  "user_background": {
    "programming_level": "intermediate",
    "math_background": "basic calculus",
    "time_availability": "10 hours/week",
    "goals": ["career transition", "practical projects"]
  },
  "conversation_goal": "Provide structured learning path",
  "expected_topics": [
    "Learning roadmap",
    "Resource recommendations", 
    "Project suggestions",
    "Timeline planning"
  ],
  "conversation_style": "supportive_mentor",
  "difficulty": "medium"
}
```

**Conversation Types**:
- **Educational Guidance** (30%): Learning paths, advice
- **Technical Help** (25%): Problem-solving assistance
- **Creative Discussion** (20%): Brainstorming, ideation
- **Information Seeking** (25%): Research, explanations

## Dataset Statistics

### Overall Statistics

| Metric | Value |
|--------|--------|
| Total Problems | 450+ |
| Average Length | 250 words |
| Language Coverage | 8 languages |
| Difficulty Distribution | Easy: 30%, Medium: 45%, Hard: 25% |

### Per-Scenario Breakdown

| Scenario | Count | Avg Turns | Complexity |
|----------|--------|-----------|------------|
| Code Review | 150 | 3 | Medium |
| Problem Solving | 100 | 3-4 | High |  
| Teaching | 80 | 3-5 | Medium |
| Conversational | 120 | 3 | Low-Medium |

## Data Quality Assurance

### Validation Process

1. **Content Review**: Expert review for technical accuracy
2. **Difficulty Calibration**: Consistent difficulty scoring  
3. **Diversity Check**: Balanced representation across categories
4. **Format Validation**: JSON schema compliance
5. **Bias Assessment**: Regular bias audits

### Quality Metrics

- **Inter-annotator Agreement**: Îº = 0.82 (substantial agreement)
- **Content Coverage**: 95%+ of common scenarios covered
- **Technical Accuracy**: 98%+ verified by domain experts
- **Bias Score**: <0.15 across demographic dimensions

## Adding Custom Datasets

### Format Requirements

```json
{
  "problem_id": "required_unique_id",
  "difficulty": "easy|medium|hard",
  "category": "your_category",
  
  // Scenario-specific fields
  "your_data_field": "value",
  
  // Optional metadata
  "metadata": {
    "source": "data_source",
    "created": "2024-01-01",
    "verified": true
  }
}
```

### Validation Script

```python
from lm_eval.tasks.multi_turn_scenarios.utils import validate_dataset

# Validate your dataset
validation_result = validate_dataset("path/to/your/dataset.jsonl")
if validation_result.is_valid:
    print("Dataset is valid!")
else:
    print("Issues found:", validation_result.errors)
```

## Dataset Updates

### Version History

- **v1.0** (2024-01): Initial release with 300 problems
- **v1.1** (2024-03): Added 100 conversation scenarios  
- **v1.2** (2024-06): Enhanced code review dataset with more languages
- **v1.3** (2024-09): Current version with 450+ problems

### Update Process

1. Community contributions via pull requests
2. Expert review and validation
3. Quality assurance testing
4. Version increment and release
5. Documentation updates

## Usage Examples

### Loading Datasets

```python
from lm_eval.tasks.multi_turn_scenarios.example_tasks import create_sample_datasets

# Load all scenario datasets
datasets = create_sample_datasets()

# Access specific scenario data
code_review_data = datasets["code_review_3_turn"]
teaching_data = datasets["teaching_dialogue"]

print(f"Code review problems: {len(code_review_data)}")
print(f"Teaching topics: {len(teaching_data)}")
```

### Custom Data Loading

```python
import json

# Load custom dataset
def load_custom_dataset(filepath):
    problems = []
    with open(filepath, 'r') as f:
        for line in f:
            problems.append(json.loads(line))
    return problems

my_data = load_custom_dataset("my_scenarios.jsonl")
```

## Contributing Data

### Guidelines

1. **Relevance**: Ensure problems fit the scenario type
2. **Quality**: High-quality, realistic problems
3. **Diversity**: Avoid duplication, ensure variety
4. **Format**: Follow established JSON schema
5. **Attribution**: Include proper source attribution

### Submission Process

1. Fork the repository
2. Add data to appropriate `.jsonl` file
3. Run validation scripts
4. Submit pull request with description
5. Address review feedback
6. Merge after approval

## License

All datasets are released under the same license as the lm-evaluation-harness project. Contributors retain attribution rights while granting usage rights to the community.