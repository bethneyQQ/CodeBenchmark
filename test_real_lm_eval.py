#!/usr/bin/env python3
"""
æµ‹è¯•çœŸå®çš„lm-eval APIæ‰§è¡Œ
"""

import requests
import json
import time
import sys

def test_real_lm_eval_api():
    """æµ‹è¯•çœŸå®çš„lm-eval API"""
    base_url = "http://localhost:8000"
    
    print("ğŸ¯ æµ‹è¯•çœŸå®çš„ LM-Eval API")
    print("=" * 50)
    
    # 1. å¥åº·æ£€æŸ¥
    print("1ï¸âƒ£ å¥åº·æ£€æŸ¥...")
    response = requests.get(f"{base_url}/health")
    if response.status_code == 200:
        health = response.json()
        print(f"âœ… æœåŠ¡å¥åº·")
        print(f"   - ç‰ˆæœ¬: {health['version']}")
        print(f"   - å¯ç”¨ä»»åŠ¡: {health['available_tasks']}")
        print(f"   - å¯ç”¨æ¨¡å‹: {health['available_models']}")
    else:
        print("âŒ æœåŠ¡ä¸å¯ç”¨")
        return False
    
    # 2. ç™»å½•
    print("\n2ï¸âƒ£ ç”¨æˆ·ç™»å½•...")
    login_response = requests.post(
        f"{base_url}/auth/login",
        json={"username": "admin", "password": "admin123"}
    )
    
    if login_response.status_code == 200:
        auth_data = login_response.json()
        token = auth_data["access_token"]
        headers = {"Authorization": f"Bearer {token}"}
        print(f"âœ… ç™»å½•æˆåŠŸ: {auth_data['user_info']['username']}")
    else:
        print("âŒ ç™»å½•å¤±è´¥")
        return False
    
    # 3. è·å–çœŸå®ä»»åŠ¡åˆ—è¡¨
    print("\n3ï¸âƒ£ è·å–çœŸå®ä»»åŠ¡åˆ—è¡¨...")
    tasks_response = requests.get(f"{base_url}/tasks?limit=10", headers=headers)
    
    if tasks_response.status_code == 200:
        tasks = tasks_response.json()
        print(f"âœ… å‘ç° {len(tasks)} ä¸ªçœŸå®ä»»åŠ¡:")
        for task in tasks[:5]:
            print(f"   - {task['task_id']}: {task['name']} ({task['difficulty']})")
        if len(tasks) > 5:
            print(f"   ... è¿˜æœ‰ {len(tasks) - 5} ä¸ªä»»åŠ¡")
    else:
        print("âŒ è·å–ä»»åŠ¡å¤±è´¥")
        return False
    
    # 4. è·å–æ¨¡å‹åˆ—è¡¨
    print("\n4ï¸âƒ£ è·å–æ¨¡å‹åˆ—è¡¨...")
    models_response = requests.get(f"{base_url}/models", headers=headers)
    
    if models_response.status_code == 200:
        models = models_response.json()
        print(f"âœ… å‘ç° {len(models)} ä¸ªæ¨¡å‹:")
        for model in models:
            print(f"   - {model['model_id']}: {model['name']} ({model['provider']})")
    else:
        print("âŒ è·å–æ¨¡å‹å¤±è´¥")
        return False
    
    # 5. åˆ›å»ºçœŸå®è¯„ä¼°ä»»åŠ¡
    print("\n5ï¸âƒ£ åˆ›å»ºçœŸå®è¯„ä¼°ä»»åŠ¡...")
    
    # é€‰æ‹©ä¸€ä¸ªç®€å•çš„ä»»åŠ¡è¿›è¡Œæµ‹è¯•
    test_task = "single_turn_scenarios_function_generation"
    test_model = "dummy"  # ä½¿ç”¨dummyæ¨¡å‹é¿å…APIå¯†é’¥é—®é¢˜
    
    eval_request = {
        "model_id": test_model,
        "task_ids": [test_task],
        "configuration": {
            "limit": 2,  # åªæµ‹è¯•2ä¸ªæ ·æœ¬
            "temperature": 0.7
        },
        "metadata": {
            "experiment_name": "real_api_test",
            "description": "æµ‹è¯•çœŸå®APIæ‰§è¡Œlm-evalä»»åŠ¡"
        }
    }
    
    eval_response = requests.post(
        f"{base_url}/evaluations",
        json=eval_request,
        headers=headers
    )
    
    if eval_response.status_code == 200:
        eval_data = eval_response.json()
        evaluation_id = eval_data["evaluation_id"]
        print(f"âœ… è¯„ä¼°ä»»åŠ¡å·²åˆ›å»º: {evaluation_id}")
        print(f"   - çŠ¶æ€: {eval_data['status']}")
        print(f"   - ä»»åŠ¡: {test_task}")
        print(f"   - æ¨¡å‹: {test_model}")
    else:
        print(f"âŒ åˆ›å»ºè¯„ä¼°å¤±è´¥: {eval_response.text}")
        return False
    
    # 6. ç›‘æ§è¯„ä¼°è¿›åº¦
    print("\n6ï¸âƒ£ ç›‘æ§è¯„ä¼°è¿›åº¦...")
    max_wait_time = 120  # æœ€å¤šç­‰å¾…2åˆ†é’Ÿ
    start_time = time.time()
    
    while time.time() - start_time < max_wait_time:
        status_response = requests.get(
            f"{base_url}/evaluations/{evaluation_id}",
            headers=headers
        )
        
        if status_response.status_code == 200:
            status_data = status_response.json()
            status = status_data["status"]
            
            print(f"   çŠ¶æ€: {status}")
            
            if status == "completed":
                print("âœ… è¯„ä¼°å®Œæˆï¼")
                break
            elif status == "failed":
                error = status_data.get("error", "æœªçŸ¥é”™è¯¯")
                print(f"âŒ è¯„ä¼°å¤±è´¥: {error}")
                return False
            
            time.sleep(5)
        else:
            print("âŒ è·å–çŠ¶æ€å¤±è´¥")
            return False
    else:
        print("âš ï¸ è¯„ä¼°è¶…æ—¶")
        return False
    
    # 7. è·å–è¯„ä¼°ç»“æœ
    print("\n7ï¸âƒ£ è·å–è¯„ä¼°ç»“æœ...")
    results_response = requests.get(
        f"{base_url}/results/{evaluation_id}?include_details=true",
        headers=headers
    )
    
    if results_response.status_code == 200:
        results = results_response.json()
        print("âœ… è·å–ç»“æœæˆåŠŸ!")
        
        print(f"\nğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦:")
        print(f"   - è¯„ä¼°ID: {results['evaluation_id']}")
        print(f"   - æ¨¡å‹: {results['model_id']}")
        
        if "summary_metrics" in results:
            summary = results["summary_metrics"]
            print(f"   - æ€»ä½“åˆ†æ•°: {summary.get('overall_score', 'N/A')}")
            print(f"   - å®Œæˆä»»åŠ¡æ•°: {summary.get('completed_tasks', 0)}/{summary.get('total_tasks', 0)}")
        
        if "task_results" in results:
            print(f"\nğŸ“‹ ä»»åŠ¡è¯¦ç»†ç»“æœ:")
            for task_result in results["task_results"]:
                print(f"   ä»»åŠ¡: {task_result['task_id']}")
                print(f"     - çŠ¶æ€: {task_result['status']}")
                print(f"     - åˆ†æ•°: {task_result.get('score', 'N/A')}")
                if "metrics" in task_result:
                    print(f"     - æŒ‡æ ‡: {task_result['metrics']}")
                print(f"     - æ‰§è¡Œæ—¶é—´: {task_result.get('execution_time', 'N/A')}s")
        
        # æ˜¾ç¤ºåŸå§‹è¾“å‡ºçš„ä¸€éƒ¨åˆ†
        if "raw_output" in results:
            raw_output = results["raw_output"]
            if raw_output:
                print(f"\nğŸ“ åŸå§‹è¾“å‡ºé¢„è§ˆ:")
                lines = raw_output.split('\n')[:10]
                for line in lines:
                    if line.strip():
                        print(f"   {line}")
                if len(raw_output.split('\n')) > 10:
                    print("   ...")
        
        return True
    else:
        print(f"âŒ è·å–ç»“æœå¤±è´¥: {results_response.text}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    try:
        success = test_real_lm_eval_api()
        
        if success:
            print("\nğŸ‰ çœŸå®APIæµ‹è¯•æˆåŠŸå®Œæˆï¼")
            print("\nğŸ’¡ æ¥ä¸‹æ¥ä½ å¯ä»¥:")
            print("   1. å°è¯•ä¸åŒçš„ä»»åŠ¡å’Œæ¨¡å‹ç»„åˆ")
            print("   2. é…ç½®APIå¯†é’¥ä½¿ç”¨çœŸå®çš„AIæ¨¡å‹")
            print("   3. è°ƒæ•´è¯„ä¼°å‚æ•°è¿›è¡Œæ›´æ·±å…¥çš„æµ‹è¯•")
            print("   4. æŸ¥çœ‹å®Œæ•´çš„APIæ–‡æ¡£: http://localhost:8000/docs")
            return 0
        else:
            print("\nğŸ’¥ æµ‹è¯•å¤±è´¥ï¼è¯·æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€å’Œé…ç½®")
            return 1
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return 0
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•å¼‚å¸¸: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())