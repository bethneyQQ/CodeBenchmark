#!/usr/bin/env python3
"""
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨single-turn-scenariosä»»åŠ¡è¿›è¡Œè¯„ä¼°

è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†å¦‚ä½•ï¼š
1. è¿è¡Œä¸åŒçš„single-turn-scenariosä»»åŠ¡
2. ä½¿ç”¨ä¸åŒçš„æ¨¡å‹
3. åº”ç”¨ä¸åŒçš„è¿‡æ»¤å™¨
4. æŸ¥çœ‹ç»“æœ
"""

import subprocess
import json
import os
from datetime import datetime

def run_evaluation(model, model_args, task, limit=1, output_dir="results"):
    """è¿è¡Œå•ä¸ªè¯„ä¼°ä»»åŠ¡"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"{output_dir}/{task}_{model.replace('-', '_')}_{timestamp}.json"
    
    cmd = [
        "python", "-m", "lm_eval",
        "--model", model,
        "--model_args", model_args,
        "--tasks", task,
        "--limit", str(limit),
        "--output_path", output_file,
        "--predict_only"  # åªç”Ÿæˆé¢„æµ‹ï¼Œä¸è®¡ç®—å¤æ‚çš„metrics
    ]
    
    print(f"ğŸš€ è¿è¡Œè¯„ä¼°: {task}")
    print(f"   æ¨¡å‹: {model} ({model_args})")
    print(f"   è¾“å‡º: {output_file}")
    print(f"   å‘½ä»¤: {' '.join(cmd)}")
    print()
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
        
        if result.returncode == 0:
            print(f"âœ… è¯„ä¼°æˆåŠŸå®Œæˆ: {task}")
            return output_file, True
        else:
            print(f"âŒ è¯„ä¼°å¤±è´¥: {task}")
            print(f"é”™è¯¯è¾“å‡º: {result.stderr}")
            return None, False
            
    except subprocess.TimeoutExpired:
        print(f"â° è¯„ä¼°è¶…æ—¶: {task}")
        return None, False
    except Exception as e:
        print(f"ğŸ’¥ è¯„ä¼°å¼‚å¸¸: {task} - {e}")
        return None, False

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºsingle-turn-scenariosçš„ä½¿ç”¨"""
    
    print("=" * 60)
    print("ğŸ§ª Single Turn Scenarios è¯„ä¼°æ¼”ç¤º")
    print("=" * 60)
    print()
    
    # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
    os.makedirs("results", exist_ok=True)
    
    # é…ç½®è¯„ä¼°å‚æ•°
    model = "claude-local"
    model_args = "model=claude-3-haiku-20240307"
    
    # è¦æµ‹è¯•çš„ä»»åŠ¡åˆ—è¡¨
    tasks_to_test = [
        "single_turn_scenarios_function_generation",
        "single_turn_scenarios_code_completion", 
        # "single_turn_scenarios_bug_fix",  # æš‚æ—¶è·³è¿‡ï¼Œå› ä¸ºmetricsæœ‰é—®é¢˜
        # "single_turn_scenarios_algorithm_implementation",
    ]
    
    successful_evaluations = []
    failed_evaluations = []
    
    # è¿è¡Œæ¯ä¸ªä»»åŠ¡
    for task in tasks_to_test:
        output_file, success = run_evaluation(
            model=model,
            model_args=model_args,
            task=task,
            limit=1  # åªæµ‹è¯•1ä¸ªæ ·æœ¬
        )
        
        if success:
            successful_evaluations.append((task, output_file))
        else:
            failed_evaluations.append(task)
        
        print("-" * 40)
        print()
    
    # æ€»ç»“ç»“æœ
    print("ğŸ“Š è¯„ä¼°æ€»ç»“")
    print("=" * 60)
    print(f"âœ… æˆåŠŸ: {len(successful_evaluations)} ä¸ªä»»åŠ¡")
    print(f"âŒ å¤±è´¥: {len(failed_evaluations)} ä¸ªä»»åŠ¡")
    print()
    
    if successful_evaluations:
        print("æˆåŠŸçš„è¯„ä¼°:")
        for task, output_file in successful_evaluations:
            print(f"  - {task}")
            print(f"    ç»“æœæ–‡ä»¶: {output_file}")
        print()
    
    if failed_evaluations:
        print("å¤±è´¥çš„è¯„ä¼°:")
        for task in failed_evaluations:
            print(f"  - {task}")
        print()
    
    # å±•ç¤ºå¦‚ä½•æŸ¥çœ‹ç»“æœ
    if successful_evaluations:
        print("ğŸ” å¦‚ä½•æŸ¥çœ‹ç»“æœ:")
        print("=" * 60)
        
        task, output_file = successful_evaluations[0]
        print(f"ç¤ºä¾‹: æŸ¥çœ‹ {task} çš„ç»“æœ")
        print()
        
        # æŸ¥æ‰¾å®é™…çš„è¾“å‡ºæ–‡ä»¶ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
        import glob
        pattern = output_file.replace('.json', '_*.json')
        actual_files = glob.glob(pattern)
        
        if actual_files:
            actual_file = actual_files[0]
            print(f"ä¸»è¦ç»“æœæ–‡ä»¶: {actual_file}")
            
            # æŸ¥æ‰¾æ ·æœ¬æ–‡ä»¶
            sample_pattern = actual_file.replace('.json', '').replace('results/', 'results/samples_') + '.jsonl'
            sample_files = glob.glob(sample_pattern)
            
            if sample_files:
                sample_file = sample_files[0]
                print(f"æ ·æœ¬è¾“å‡ºæ–‡ä»¶: {sample_file}")
                
                print()
                print("æŸ¥çœ‹æ ·æœ¬è¾“å‡ºçš„å‘½ä»¤:")
                print(f"  cat {sample_file}")
                print()
                print("æˆ–è€…ç”¨PythonæŸ¥çœ‹:")
                print(f"  python -c \"import json; print(json.load(open('{sample_file}'))['resps'][0][0][:200])\"")
    
    print()
    print("ğŸ¯ æ›´å¤šä½¿ç”¨ç¤ºä¾‹:")
    print("=" * 60)
    print()
    
    print("1. è¿è¡Œç‰¹å®šéš¾åº¦çš„ä»»åŠ¡:")
    print("   python -m lm_eval --model claude-local --model_args model=claude-3-haiku-20240307 \\")
    print("     --tasks single_turn_scenarios_function_generation \\")
    print("     --metadata '{\"difficulty\":\"simple\"}' --limit 2")
    print()
    
    print("2. è¿è¡Œç‰¹å®šç¼–ç¨‹è¯­è¨€çš„ä»»åŠ¡:")
    print("   python -m lm_eval --model claude-local --model_args model=claude-3-haiku-20240307 \\")
    print("     --tasks single_turn_scenarios_code_completion \\")
    print("     --metadata '{\"language\":\"python\"}' --limit 2")
    print()
    
    print("3. è¿è¡Œå¤šä¸ªä»»åŠ¡:")
    print("   python -m lm_eval --model claude-local --model_args model=claude-3-haiku-20240307 \\")
    print("     --tasks single_turn_scenarios_function_generation,single_turn_scenarios_code_completion \\")
    print("     --limit 2")
    print()
    
    print("4. ä½¿ç”¨ä¸åŒçš„ä¸Šä¸‹æ–‡æ¨¡å¼:")
    print("   python -m lm_eval --model claude-local --model_args model=claude-3-haiku-20240307 \\")
    print("     --tasks single_turn_scenarios_function_generation \\")
    print("     --metadata '{\"context_mode\":\"minimal_context\"}' --limit 2")
    print()
    
    print("âœ¨ è¯„ä¼°æ¼”ç¤ºå®Œæˆ!")

if __name__ == "__main__":
    main()