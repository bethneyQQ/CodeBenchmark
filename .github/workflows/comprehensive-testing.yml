name: Comprehensive Testing Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run nightly tests at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.9'
  NODE_VERSION: '18'

jobs:
  # Unit Tests
  unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,testing,evaluation_engine]
        pip install pytest-cov pytest-xdist pytest-benchmark
    
    - name: Run unit tests
      run: |
        pytest evaluation_engine_tests/unit/ \
          --cov=evaluation_engine \
          --cov-report=xml \
          --cov-report=html \
          --cov-fail-under=90 \
          --junitxml=junit/unit-test-results.xml \
          -v --tb=short
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unit-tests
        name: codecov-unit-${{ matrix.python-version }}
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: unit-test-results-${{ matrix.python-version }}
        path: |
          junit/
          htmlcov/

  # Integration Tests
  integration-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: testpass
          POSTGRES_DB: testdb
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,testing,evaluation_engine]
        pip install pytest-asyncio pytest-timeout
    
    - name: Set up test environment
      run: |
        mkdir -p test_data test_results test_cache
        echo "TEST_DATABASE_URL=postgresql://postgres:testpass@localhost:5432/testdb" >> $GITHUB_ENV
        echo "TEST_REDIS_URL=redis://localhost:6379/0" >> $GITHUB_ENV
    
    - name: Run integration tests
      run: |
        pytest evaluation_engine_tests/integration/ \
          --timeout=300 \
          --junitxml=junit/integration-test-results.xml \
          -v --tb=short
    
    - name: Upload integration test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: junit/

  # End-to-End Tests
  e2e-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    services:
      docker:
        image: docker:dind
        options: --privileged
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,testing,evaluation_engine]
        pip install pytest-docker pytest-httpserver
    
    - name: Build test containers
      run: |
        docker build -t evaluation-engine-test -f Dockerfile .
        docker-compose -f docker-compose.test.yml build
    
    - name: Run E2E tests
      run: |
        pytest evaluation_engine_tests/e2e/ \
          --timeout=600 \
          --junitxml=junit/e2e-test-results.xml \
          -v --tb=short \
          -m "e2e and not slow"
    
    - name: Run slow E2E tests (nightly only)
      if: github.event_name == 'schedule'
      run: |
        pytest evaluation_engine_tests/e2e/ \
          --timeout=1800 \
          --junitxml=junit/e2e-slow-test-results.xml \
          -v --tb=short \
          -m "e2e and slow"
    
    - name: Upload E2E test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: e2e-test-results
        path: junit/

  # Performance Tests
  performance-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    if: github.event_name == 'schedule' || contains(github.event.head_commit.message, '[perf-test]')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,testing,evaluation_engine]
        pip install pytest-benchmark psutil memory-profiler
    
    - name: Run performance benchmarks
      run: |
        pytest evaluation_engine_tests/performance/ \
          --benchmark-only \
          --benchmark-json=benchmark-results.json \
          --junitxml=junit/performance-test-results.xml \
          -v --tb=short
    
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: |
          benchmark-results.json
          junit/
    
    - name: Performance regression check
      run: |
        python scripts/check_performance_regression.py benchmark-results.json

  # Security Tests
  security-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,testing,evaluation_engine,security]
        pip install bandit safety semgrep
    
    - name: Run security linting with Bandit
      run: |
        bandit -r evaluation_engine/ -f json -o bandit-report.json
        bandit -r evaluation_engine/ -f txt
    
    - name: Run dependency security check with Safety
      run: |
        safety check --json --output safety-report.json
        safety check
    
    - name: Run SAST with Semgrep
      run: |
        semgrep --config=auto evaluation_engine/ --json --output=semgrep-report.json
        semgrep --config=auto evaluation_engine/
    
    - name: Run security tests
      run: |
        pytest evaluation_engine_tests/security/ \
          --junitxml=junit/security-test-results.xml \
          -v --tb=short \
          -m "security and not slow"
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json
          semgrep-report.json
          junit/

  # Code Quality
  code-quality:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install black isort flake8 mypy pylint
        pip install -e .[dev]
    
    - name: Check code formatting with Black
      run: black --check --diff evaluation_engine/ evaluation_engine_tests/
    
    - name: Check import sorting with isort
      run: isort --check-only --diff evaluation_engine/ evaluation_engine_tests/
    
    - name: Lint with flake8
      run: flake8 evaluation_engine/ evaluation_engine_tests/ --max-line-length=100
    
    - name: Type checking with mypy
      run: mypy evaluation_engine/ --ignore-missing-imports
    
    - name: Lint with pylint
      run: pylint evaluation_engine/ --output-format=json > pylint-report.json || true
    
    - name: Upload code quality reports
      uses: actions/upload-artifact@v3
      with:
        name: code-quality-reports
        path: pylint-report.json

  # Documentation Tests
  documentation-tests:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
        pip install sphinx sphinx-rtd-theme docutils
    
    - name: Check documentation build
      run: |
        cd docs
        make html
    
    - name: Test docstrings
      run: |
        python -m doctest evaluation_engine/**/*.py
    
    - name: Check README links
      run: |
        python scripts/check_readme_links.py

  # Multi-platform Tests
  multi-platform-tests:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.9', '3.11']
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[testing]
    
    - name: Run core tests
      run: |
        pytest evaluation_engine_tests/unit/test_unified_framework.py \
          evaluation_engine_tests/unit/test_task_registry.py \
          evaluation_engine_tests/unit/test_metrics_engine.py \
          -v --tb=short

  # Test Report Generation
  test-report:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests, security-tests]
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download all test artifacts
      uses: actions/download-artifact@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install report dependencies
      run: |
        pip install jinja2 junit-xml-parser
    
    - name: Generate comprehensive test report
      run: |
        python scripts/generate_test_report.py \
          --unit-results unit-test-results-*/junit/ \
          --integration-results integration-test-results/junit/ \
          --e2e-results e2e-test-results/junit/ \
          --security-results security-reports/junit/ \
          --output comprehensive-test-report.html
    
    - name: Upload comprehensive report
      uses: actions/upload-artifact@v3
      with:
        name: comprehensive-test-report
        path: comprehensive-test-report.html
    
    - name: Comment PR with test results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const reportPath = 'comprehensive-test-report.html';
          if (fs.existsSync(reportPath)) {
            const report = fs.readFileSync(reportPath, 'utf8');
            // Extract summary from HTML report
            const summaryMatch = report.match(/<div class="summary">(.*?)<\/div>/s);
            const summary = summaryMatch ? summaryMatch[1] : 'Test report generated';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## Test Results Summary\n\n${summary}\n\n[View Full Report](${process.env.GITHUB_SERVER_URL}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`
            });
          }

  # Deployment Tests (for main branch)
  deployment-tests:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests, security-tests]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,testing,evaluation_engine]
    
    - name: Test package build
      run: |
        python -m build
        pip install dist/*.whl
        python -c "import evaluation_engine; print('Package installed successfully')"
    
    - name: Test Docker build
      run: |
        docker build -t evaluation-engine:test .
        docker run --rm evaluation-engine:test python -c "import evaluation_engine; print('Docker build successful')"
    
    - name: Run smoke tests
      run: |
        pytest evaluation_engine_tests/e2e/test_complete_evaluation_pipeline.py::TestCompleteEvaluationPipeline::test_complete_single_turn_pipeline \
          -v --tb=short