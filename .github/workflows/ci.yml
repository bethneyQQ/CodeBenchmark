name: AI Evaluation Engine CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.11'

jobs:
  # Linting and code quality
  lint:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install ruff black isort mypy
        pip install -e ".[dev]"
    
    - name: Run ruff
      run: ruff check .
    
    - name: Run black
      run: black --check .
    
    - name: Run isort
      run: isort --check-only .
    
    - name: Run mypy
      run: mypy lm_eval evaluation_engine --ignore-missing-imports

  # Unit tests
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11', '3.12']
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,testing]"
    
    - name: Run unit tests
      run: |
        pytest lm_eval/tasks/single_turn_scenarios/tests/unit/ -v --cov=lm_eval --cov=evaluation_engine
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      if: matrix.python-version == '3.11'

  # Integration tests
  integration-test:
    runs-on: ubuntu-latest
    needs: [lint, test]
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
      
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_DB: ai_evaluation_test
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,testing,api]"
    
    - name: Run integration tests
      env:
        REDIS_URL: redis://localhost:6379
        DATABASE_URL: postgresql://test_user:test_password@localhost:5432/ai_evaluation_test
      run: |
        pytest lm_eval/tasks/single_turn_scenarios/tests/integration/ -v

  # Docker build and security scan
  docker-test:
    runs-on: ubuntu-latest
    needs: [lint, test]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build Docker image
      run: |
        docker build -t ai-evaluation-engine:test .
    
    - name: Test Docker image
      run: |
        docker run --rm ai-evaluation-engine:test python -c "import lm_eval; import evaluation_engine; print('Docker build successful')"
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: 'ai-evaluation-engine:test'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

  # lm-eval compatibility tests
  lm-eval-compatibility:
    runs-on: ubuntu-latest
    needs: [lint, test]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,testing]"
    
    - name: Test lm-eval task discovery
      run: |
        python -c "
        from lm_eval.api.registry import get_task_dict
        tasks = get_task_dict()
        single_turn_tasks = [t for t in tasks.keys() if 'single_turn_scenarios' in t]
        multi_turn_tasks = [t for t in tasks.keys() if 'multi_turn_scenarios' in t]
        print(f'Found {len(single_turn_tasks)} single-turn tasks')
        print(f'Found {len(multi_turn_tasks)} multi-turn tasks')
        assert len(single_turn_tasks) > 0, 'No single-turn tasks found'
        print('lm-eval compatibility test passed')
        "
    
    - name: Test basic evaluation workflow
      run: |
        python -m lm_eval --model dummy --tasks single_turn_scenarios_function_generation --limit 1 --log_samples

  # Performance benchmarks
  performance-test:
    runs-on: ubuntu-latest
    needs: [integration-test]
    if: github.event_name == 'schedule' || contains(github.event.head_commit.message, '[benchmark]')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,testing]"
        pip install pytest-benchmark
    
    - name: Run performance tests
      run: |
        pytest lm_eval/tasks/single_turn_scenarios/tests/performance/ -v --benchmark-only

  # Security audit
  security-audit:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit semgrep
        pip install -e ".[dev]"
    
    - name: Run safety check
      run: safety check
    
    - name: Run bandit security linter
      run: bandit -r lm_eval evaluation_engine -f json -o bandit-report.json || true
    
    - name: Run semgrep
      run: |
        python -m semgrep --config=auto --json --output=semgrep-report.json . || true
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      with:
        name: security-reports
        path: |
          bandit-report.json
          semgrep-report.json

  # Deploy to staging (on main branch)
  deploy-staging:
    runs-on: ubuntu-latest
    needs: [integration-test, docker-test, lm-eval-compatibility]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment..."
        # Add actual deployment steps here
        echo "Staging deployment completed"

  # Release (on tags)
  release:
    runs-on: ubuntu-latest
    needs: [integration-test, docker-test, lm-eval-compatibility, security-audit]
    if: startsWith(github.ref, 'refs/tags/v')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Build package
      run: |
        python -m pip install --upgrade pip build
        python -m build
    
    - name: Create Release
      uses: actions/create-release@v1
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        tag_name: ${{ github.ref }}
        release_name: Release ${{ github.ref }}
        draft: false
        prerelease: false