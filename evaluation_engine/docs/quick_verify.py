#!/usr/bin/env python3
"""
å¿«é€ŸéªŒè¯ AI Evaluation Engine å®‰è£…å’Œé…ç½®
"""

import sys
import os
import subprocess
import json
from pathlib import Path

def print_status(message):
    print(f"ğŸ” {message}")

def print_success(message):
    print(f"âœ… {message}")

def print_error(message):
    print(f"âŒ {message}")

def print_warning(message):
    print(f"âš ï¸  {message}")

def check_python_environment():
    """æ£€æŸ¥Pythonç¯å¢ƒ"""
    print_status("æ£€æŸ¥Pythonç¯å¢ƒ...")
    
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    version = sys.version_info
    if version.major == 3 and version.minor >= 9:
        print_success(f"Pythonç‰ˆæœ¬: {version.major}.{version.minor}.{version.micro}")
    else:
        print_error(f"Pythonç‰ˆæœ¬è¿‡ä½: {version.major}.{version.minor}.{version.micro}ï¼Œéœ€è¦3.9+")
        return False
    
    # æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒ
    if hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
        print_success("è™šæ‹Ÿç¯å¢ƒå·²æ¿€æ´»")
    else:
        print_warning("æœªæ£€æµ‹åˆ°è™šæ‹Ÿç¯å¢ƒï¼Œå»ºè®®ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ")
    
    return True

def check_dependencies():
    """æ£€æŸ¥å…³é”®ä¾èµ–"""
    print_status("æ£€æŸ¥å…³é”®ä¾èµ–...")
    
    dependencies = [
        ('lm_eval', 'lm-evalæ ¸å¿ƒåº“'),
        ('datasets', 'HuggingFace datasets'),
        ('transformers', 'HuggingFace transformers'),
    ]
    
    # evaluation_engine å¯èƒ½ä¸åœ¨Pythonè·¯å¾„ä¸­ï¼Œå•ç‹¬æ£€æŸ¥
    try:
        import evaluation_engine
        print_success("evaluation engine - å·²å®‰è£…")
    except ImportError:
        print_warning("evaluation engine - æœªå®‰è£…ï¼ˆå¯èƒ½åœ¨å¼€å‘æ¨¡å¼ä¸‹æ­£å¸¸ï¼‰")
    
    all_good = True
    for module, description in dependencies:
        try:
            __import__(module)
            print_success(f"{description} - å·²å®‰è£…")
        except ImportError:
            print_error(f"{description} - æœªå®‰è£…")
            all_good = False
    
    return all_good

def check_api_keys():
    """æ£€æŸ¥APIå¯†é’¥é…ç½®"""
    print_status("æ£€æŸ¥APIå¯†é’¥é…ç½®...")
    
    api_keys = [
        ('ANTHROPIC_API_KEY', 'Anthropic Claude'),
        ('OPENAI_API_KEY', 'OpenAI GPT'),
        ('DEEPSEEK_API_KEY', 'DeepSeek'),
        ('DASHSCOPE_API_KEY', 'é€šä¹‰åƒé—®'),
    ]
    
    configured_keys = 0
    for key, service in api_keys:
        if os.getenv(key):
            print_success(f"{service} APIå¯†é’¥ - å·²é…ç½®")
            configured_keys += 1
        else:
            print_warning(f"{service} APIå¯†é’¥ - æœªé…ç½®")
    
    if configured_keys == 0:
        print_warning("æœªé…ç½®ä»»ä½•APIå¯†é’¥ï¼Œåªèƒ½ä½¿ç”¨dummyæ¨¡å‹æµ‹è¯•")
    else:
        print_success(f"å·²é…ç½® {configured_keys} ä¸ªAPIå¯†é’¥")
    
    return configured_keys > 0

def check_tasks():
    """æ£€æŸ¥ä»»åŠ¡æ³¨å†Œ"""
    print_status("æ£€æŸ¥ä»»åŠ¡æ³¨å†Œ...")
    
    try:
        result = subprocess.run([
            sys.executable, '-m', 'lm_eval', '--tasks', 'list'
        ], capture_output=True, text=True, timeout=30)
        
        if result.returncode == 0:
            tasks = result.stdout
            single_turn_tasks = [line for line in tasks.split('\n') if 'single_turn_scenarios' in line]
            
            if single_turn_tasks:
                print_success(f"æ‰¾åˆ° {len(single_turn_tasks)} ä¸ªsingle_turn_scenariosä»»åŠ¡")
                print("  ä¸»è¦ä»»åŠ¡:")
                for task in single_turn_tasks[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                    print(f"    - {task.strip()}")
                if len(single_turn_tasks) > 5:
                    print(f"    ... è¿˜æœ‰ {len(single_turn_tasks) - 5} ä¸ªä»»åŠ¡")
                return True
            else:
                print_error("æœªæ‰¾åˆ°single_turn_scenariosä»»åŠ¡")
                return False
        else:
            print_error(f"ä»»åŠ¡åˆ—è¡¨è·å–å¤±è´¥: {result.stderr}")
            return False
            
    except Exception as e:
        print_error(f"ä»»åŠ¡æ£€æŸ¥å¤±è´¥: {e}")
        return False

def run_dummy_test():
    """è¿è¡Œdummyæ¨¡å‹æµ‹è¯•"""
    print_status("è¿è¡Œdummyæ¨¡å‹æµ‹è¯•...")
    
    try:
        cmd = [
            sys.executable, '-m', 'lm_eval',
            '--model', 'dummy',
            '--tasks', 'single_turn_scenarios_function_generation',
            '--limit', '1',
            '--predict_only',
            '--output_path', 'results/dummy_test'
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
        
        if result.returncode == 0:
            print_success("Dummyæ¨¡å‹æµ‹è¯•é€šè¿‡")
            return True
        else:
            print_error(f"Dummyæ¨¡å‹æµ‹è¯•å¤±è´¥: {result.stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        print_error("Dummyæ¨¡å‹æµ‹è¯•è¶…æ—¶")
        return False
    except Exception as e:
        print_error(f"Dummyæ¨¡å‹æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def run_api_test():
    """è¿è¡ŒAPIæ¨¡å‹æµ‹è¯•ï¼ˆå¦‚æœæœ‰APIå¯†é’¥ï¼‰"""
    print_status("è¿è¡ŒAPIæ¨¡å‹æµ‹è¯•...")
    
    # æ£€æŸ¥å¯ç”¨çš„APIå¯†é’¥
    if os.getenv('ANTHROPIC_API_KEY'):
        model = 'claude-local'
        model_args = 'model=claude-3-haiku-20240307'
        service = 'Claude'
    elif os.getenv('OPENAI_API_KEY'):
        model = 'openai-completions'
        model_args = 'model=gpt-3.5-turbo'
        service = 'OpenAI'
    elif os.getenv('DEEPSEEK_API_KEY'):
        model = 'deepseek'
        model_args = 'model=deepseek-coder'
        service = 'DeepSeek'
    elif os.getenv('DASHSCOPE_API_KEY'):
        model = 'dashscope'
        model_args = 'model=qwen-turbo'
        service = 'é€šä¹‰åƒé—®'
    else:
        print_warning("æœªé…ç½®APIå¯†é’¥ï¼Œè·³è¿‡APIæ¨¡å‹æµ‹è¯•")
        return True
    
    try:
        cmd = [
            sys.executable, '-m', 'lm_eval',
            '--model', model,
            '--model_args', model_args,
            '--tasks', 'single_turn_scenarios_function_generation',
            '--limit', '1',
            '--predict_only',
            '--output_path', f'results/api_test_{service.lower().replace(" ", "_")}'
        ]
        
        print_status(f"æµ‹è¯• {service} æ¨¡å‹...")
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
        
        if result.returncode == 0:
            print_success(f"{service} æ¨¡å‹æµ‹è¯•é€šè¿‡")
            return True
        else:
            print_error(f"{service} æ¨¡å‹æµ‹è¯•å¤±è´¥: {result.stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        print_error(f"{service} æ¨¡å‹æµ‹è¯•è¶…æ—¶")
        return False
    except Exception as e:
        print_error(f"{service} æ¨¡å‹æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def check_analysis_tools():
    """æ£€æŸ¥åˆ†æå·¥å…·"""
    print_status("æ£€æŸ¥åˆ†æå·¥å…·...")
    
    try:
        # æ£€æŸ¥åˆ†æå·¥å…·æ˜¯å¦å¯ä»¥å¯¼å…¥
        sys.path.append('lm_eval/tasks/single_turn_scenarios/analysis_tools')
        
        tools = [
            ('scenario_analysis', 'ScenarioAnalyzer'),
            ('compare_models', 'ModelComparator'),
            ('context_impact', 'ContextAnalyzer'),
            ('generate_report', 'ReportGenerator'),
        ]
        
        available_tools = 0
        for module, class_name in tools:
            try:
                mod = __import__(module)
                if hasattr(mod, class_name):
                    print_success(f"{class_name} - å¯ç”¨")
                    available_tools += 1
                else:
                    print_warning(f"{class_name} - ç±»æœªæ‰¾åˆ°")
            except ImportError:
                print_warning(f"{module} - æ¨¡å—æœªæ‰¾åˆ°")
        
        if available_tools > 0:
            print_success(f"åˆ†æå·¥å…·æ£€æŸ¥å®Œæˆï¼Œ{available_tools}/{len(tools)} ä¸ªå·¥å…·å¯ç”¨")
            return True
        else:
            print_error("åˆ†æå·¥å…·ä¸å¯ç”¨")
            return False
            
    except Exception as e:
        print_error(f"åˆ†æå·¥å…·æ£€æŸ¥å¤±è´¥: {e}")
        return False

def main():
    """ä¸»éªŒè¯æµç¨‹"""
    print("ğŸ”§ AI Evaluation Engine å¿«é€ŸéªŒè¯")
    print("=" * 50)
    
    checks = [
        ("Pythonç¯å¢ƒ", check_python_environment),
        ("ä¾èµ–åŒ…", check_dependencies),
        ("APIå¯†é’¥", check_api_keys),
        ("ä»»åŠ¡æ³¨å†Œ", check_tasks),
        ("Dummyæµ‹è¯•", run_dummy_test),
        ("APIæµ‹è¯•", run_api_test),
        ("åˆ†æå·¥å…·", check_analysis_tools),
    ]
    
    passed = 0
    total = len(checks)
    
    for name, check_func in checks:
        print(f"\n--- {name} ---")
        try:
            if check_func():
                passed += 1
        except Exception as e:
            print_error(f"{name} æ£€æŸ¥å¼‚å¸¸: {e}")
    
    print("\n" + "=" * 50)
    print(f"éªŒè¯å®Œæˆ: {passed}/{total} é¡¹æ£€æŸ¥é€šè¿‡")
    
    if passed == total:
        print_success("ğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ª")
        print("\nä¸‹ä¸€æ­¥:")
        print("1. æŸ¥çœ‹ç”¨æˆ·èœå•: cat evaluation_engine/docs/user_menu.md")
        print("2. è¿è¡Œå®Œæ•´è¯„ä¼°: python demo_single_turn_scenarios.py")
        print("3. è¿è¡Œåˆ†æå·¥å…·: python demo_analysis_tools.py")
    elif passed >= total - 2:
        print_success("âœ… åŸºæœ¬åŠŸèƒ½æ­£å¸¸ï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨")
        print("\nå»ºè®®:")
        print("1. é…ç½®æ›´å¤šAPIå¯†é’¥ä»¥æµ‹è¯•ä¸åŒæ¨¡å‹")
        print("2. æŸ¥çœ‹ç”¨æˆ·èœå•äº†è§£å®Œæ•´åŠŸèƒ½")
    else:
        print_error("âŒ å­˜åœ¨é‡è¦é—®é¢˜ï¼Œè¯·æ£€æŸ¥å®‰è£…")
        print("\nå»ºè®®:")
        print("1. é‡æ–°è¿è¡Œå®‰è£…è„šæœ¬: bash evaluation_engine/docs/quick_setup.sh")
        print("2. æ£€æŸ¥ä¾èµ–å®‰è£…: pip install -e .[dev,api,testing,evaluation_engine]")
        print("3. æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)