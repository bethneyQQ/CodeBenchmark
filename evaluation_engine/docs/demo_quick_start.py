#!/usr/bin/env python3
"""
AI Evaluation Engine å¿«é€Ÿæ¼”ç¤º
å±•ç¤ºå¦‚ä½•è¿è¡Œè¯„ä¼°å¹¶ç”Ÿæˆåˆ†ææŠ¥å‘Š
"""

import subprocess
import json
import os
import sys
import glob
from datetime import datetime
from pathlib import Path

def print_status(message):
    print(f"ğŸ” {message}")

def print_success(message):
    print(f"âœ… {message}")

def print_error(message):
    print(f"âŒ {message}")

def print_warning(message):
    print(f"âš ï¸  {message}")

def run_evaluation_demo():
    """è¿è¡Œè¯„ä¼°æ¼”ç¤º"""
    print("ğŸš€ AI Evaluation Engine å¿«é€Ÿæ¼”ç¤º")
    print("=" * 50)
    
    # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
    os.makedirs("results", exist_ok=True)
    
    # æ£€æŸ¥APIå¯†é’¥
    api_available = False
    model_config = None
    
    if os.getenv('ANTHROPIC_API_KEY'):
        model_config = ('claude-local', 'model=claude-3-haiku-20240307', 'Claude Haiku')
        api_available = True
    elif os.getenv('OPENAI_API_KEY'):
        model_config = ('openai-completions', 'model=gpt-3.5-turbo', 'GPT-3.5 Turbo')
        api_available = True
    elif os.getenv('DEEPSEEK_API_KEY'):
        model_config = ('deepseek', 'model=deepseek-coder', 'DeepSeek Coder')
        api_available = True
    elif os.getenv('DASHSCOPE_API_KEY'):
        model_config = ('dashscope', 'model=qwen-turbo', 'Qwen Turbo')
        api_available = True
    
    if not api_available:
        print_warning("æœªæ£€æµ‹åˆ°APIå¯†é’¥ï¼Œå°†ä½¿ç”¨dummyæ¨¡å‹æ¼”ç¤º")
        model_config = ('dummy', '', 'Dummy Model')
    
    model, model_args, model_name = model_config
    
    print_status(f"ä½¿ç”¨æ¨¡å‹: {model_name}")
    print()
    
    # è¿è¡Œè¯„ä¼°
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"results/demo_{timestamp}.json"
    
    cmd = [
        sys.executable, "-m", "lm_eval",
        "--model", model,
        "--tasks", "single_turn_scenarios_function_generation",
        "--limit", "2",  # åªæµ‹è¯•2ä¸ªæ ·æœ¬
        "--output_path", output_file,
        "--log_samples"
    ]
    
    if model_args:
        cmd.extend(["--model_args", model_args])
    
    print_status("è¿è¡Œè¯„ä¼°...")
    print(f"å‘½ä»¤: {' '.join(cmd)}")
    print()
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
        
        if result.returncode == 0:
            print_success("è¯„ä¼°å®Œæˆï¼")
            
            # æŸ¥æ‰¾å®é™…ç”Ÿæˆçš„æ–‡ä»¶
            actual_files = glob.glob(f"{output_file}_*.json")
            if actual_files:
                actual_file = actual_files[0]
                print_status(f"ç»“æœæ–‡ä»¶: {actual_file}")
                
                # æ˜¾ç¤ºç»“æœæ‘˜è¦
                try:
                    with open(actual_file, 'r') as f:
                        data = json.load(f)
                    
                    print("\nğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦:")
                    print("-" * 30)
                    
                    if 'results' in data:
                        for task_name, task_results in data['results'].items():
                            print(f"ä»»åŠ¡: {task_name}")
                            for metric, value in task_results.items():
                                if isinstance(value, (int, float)):
                                    print(f"  {metric}: {value}")
                                else:
                                    print(f"  {metric}: {str(value)[:50]}...")
                    
                    if 'config' in data:
                        config = data['config']
                        print(f"\né…ç½®ä¿¡æ¯:")
                        print(f"  æ¨¡å‹: {config.get('model', 'unknown')}")
                        print(f"  æ ·æœ¬æ•°: {config.get('limit', 'unknown')}")
                    
                except Exception as e:
                    print_warning(f"æ— æ³•è§£æç»“æœæ–‡ä»¶: {e}")
                
                # æŸ¥æ‰¾æ ·æœ¬æ–‡ä»¶
                sample_pattern = actual_file.replace('.json', '').replace('results/', 'results/samples_') + '.jsonl'
                sample_files = glob.glob(sample_pattern)
                
                if sample_files:
                    sample_file = sample_files[0]
                    print_status(f"æ ·æœ¬æ–‡ä»¶: {sample_file}")
                    
                    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬
                    try:
                        with open(sample_file, 'r') as f:
                            first_line = f.readline().strip()
                            if first_line:
                                sample_data = json.loads(first_line)
                                print("\nğŸ“ æ ·æœ¬ç¤ºä¾‹:")
                                print("-" * 30)
                                print(f"è¾“å…¥: {sample_data.get('doc', {}).get('prompt', 'N/A')[:100]}...")
                                if 'resps' in sample_data and sample_data['resps']:
                                    response = sample_data['resps'][0][0] if sample_data['resps'][0] else 'N/A'
                                    print(f"è¾“å‡º: {response[:200]}...")
                    except Exception as e:
                        print_warning(f"æ— æ³•è¯»å–æ ·æœ¬æ–‡ä»¶: {e}")
                
                return actual_file
            else:
                print_warning("æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶")
                return None
        else:
            print_error("è¯„ä¼°å¤±è´¥")
            print(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
            return None
            
    except subprocess.TimeoutExpired:
        print_error("è¯„ä¼°è¶…æ—¶")
        return None
    except Exception as e:
        print_error(f"è¯„ä¼°å¼‚å¸¸: {e}")
        return None

def run_analysis_demo(result_file=None):
    """è¿è¡Œåˆ†æå·¥å…·æ¼”ç¤º"""
    print("\nğŸ” åˆ†æå·¥å…·æ¼”ç¤º")
    print("=" * 50)
    
    # æŸ¥æ‰¾ç»“æœæ–‡ä»¶
    if not result_file:
        result_files = glob.glob("results/demo_*.json") + glob.glob("results/validation_*.json")
        if not result_files:
            print_warning("æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶ï¼Œè·³è¿‡åˆ†ææ¼”ç¤º")
            return
        result_file = result_files[0]
    
    print_status(f"åˆ†ææ–‡ä»¶: {result_file}")
    
    try:
        # æ·»åŠ åˆ†æå·¥å…·è·¯å¾„
        sys.path.append('lm_eval/tasks/single_turn_scenarios/analysis_tools')
        
        # åŠ è½½ç»“æœæ•°æ®
        with open(result_file, 'r') as f:
            data = json.load(f)
        
        # è½¬æ¢ä¸ºåˆ†æå·¥å…·æœŸæœ›çš„æ ¼å¼
        sample_data = []
        if 'results' in data:
            for task_name, task_results in data['results'].items():
                sample_data.append({
                    'task': task_name,
                    'model': data.get('config', {}).get('model', 'unknown'),
                    'scenario': task_name.replace('single_turn_scenarios_', ''),
                    'difficulty': 'simple',
                    'language': 'python',
                    'context_mode': 'no_context',
                    'metrics': task_results
                })
        
        if not sample_data:
            print_warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„åˆ†ææ•°æ®")
            return
        
        print_success(f"åŠ è½½äº† {len(sample_data)} ä¸ªåˆ†ææ ·æœ¬")
        
        # æµ‹è¯•åˆ†æå·¥å…·
        tools_tested = 0
        
        # æµ‹è¯• ScenarioAnalyzer
        try:
            from scenario_analysis import ScenarioAnalyzer
            analyzer = ScenarioAnalyzer(sample_data)
            print_success("ScenarioAnalyzer - åˆå§‹åŒ–æˆåŠŸ")
            
            if hasattr(analyzer, 'df') and len(analyzer.df) > 0:
                print(f"  æ•°æ®æ¡†å½¢çŠ¶: {analyzer.df.shape}")
            tools_tested += 1
        except Exception as e:
            print_warning(f"ScenarioAnalyzer - å¤±è´¥: {e}")
        
        # æµ‹è¯• ModelComparator
        try:
            from compare_models import ModelComparator
            comparator = ModelComparator(sample_data)
            print_success("ModelComparator - åˆå§‹åŒ–æˆåŠŸ")
            tools_tested += 1
        except Exception as e:
            print_warning(f"ModelComparator - å¤±è´¥: {e}")
        
        # æµ‹è¯• ContextAnalyzer
        try:
            from context_impact import ContextAnalyzer
            context_analyzer = ContextAnalyzer(sample_data)
            print_success("ContextAnalyzer - åˆå§‹åŒ–æˆåŠŸ")
            tools_tested += 1
        except Exception as e:
            print_warning(f"ContextAnalyzer - å¤±è´¥: {e}")
        
        # æµ‹è¯• ReportGenerator
        try:
            from generate_report import ReportGenerator
            generator = ReportGenerator(sample_data)
            print_success("ReportGenerator - åˆå§‹åŒ–æˆåŠŸ")
            tools_tested += 1
        except Exception as e:
            print_warning(f"ReportGenerator - å¤±è´¥: {e}")
        
        print(f"\nğŸ“Š åˆ†æå·¥å…·æµ‹è¯•å®Œæˆ: {tools_tested}/4 ä¸ªå·¥å…·å¯ç”¨")
        
        if tools_tested > 0:
            print_success("åˆ†æå·¥å…·æ¼”ç¤ºæˆåŠŸï¼")
            print("\nå¯ç”¨çš„åˆ†æåŠŸèƒ½:")
            print("- åœºæ™¯åˆ†æ (ScenarioAnalyzer)")
            print("- æ¨¡å‹æ¯”è¾ƒ (ModelComparator)")
            print("- ä¸Šä¸‹æ–‡å½±å“åˆ†æ (ContextAnalyzer)")
            print("- æŠ¥å‘Šç”Ÿæˆ (ReportGenerator)")
        else:
            print_warning("åˆ†æå·¥å…·ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥å®‰è£…")
        
    except Exception as e:
        print_error(f"åˆ†ææ¼”ç¤ºå¤±è´¥: {e}")

def show_next_steps():
    """æ˜¾ç¤ºåç»­æ­¥éª¤"""
    print("\nğŸ¯ åç»­æ­¥éª¤")
    print("=" * 50)
    
    print("1. æŸ¥çœ‹å®Œæ•´ç”¨æˆ·èœå•:")
    print("   cat evaluation_engine/docs/user_menu.md")
    
    print("\n2. è¿è¡Œæ›´å¤šè¯„ä¼°:")
    print("   python -m lm_eval --model claude-local --model_args model=claude-3-haiku-20240307 \\")
    print("     --tasks single_turn_scenarios_function_generation,single_turn_scenarios_code_completion \\")
    print("     --limit 5 --output_path results/my_test.json")
    
    print("\n3. è¿è¡Œå®Œæ•´æ¼”ç¤º:")
    print("   python demo_single_turn_scenarios.py")
    
    print("\n4. è¿è¡Œåˆ†æå·¥å…·:")
    print("   python demo_analysis_tools.py")
    
    print("\n5. æŸ¥çœ‹æµ‹è¯•å¥—ä»¶:")
    print("   python -m pytest evaluation_engine/tests/ -v")
    
    print("\n6. é…ç½®æ›´å¤šAPIå¯†é’¥:")
    print("   export ANTHROPIC_API_KEY='your_key'")
    print("   export OPENAI_API_KEY='your_key'")
    print("   export DEEPSEEK_API_KEY='your_key'")
    print("   export DASHSCOPE_API_KEY='your_key'")

def main():
    """ä¸»æ¼”ç¤ºæµç¨‹"""
    try:
        # è¿è¡Œè¯„ä¼°æ¼”ç¤º
        result_file = run_evaluation_demo()
        
        # è¿è¡Œåˆ†ææ¼”ç¤º
        run_analysis_demo(result_file)
        
        # æ˜¾ç¤ºåç»­æ­¥éª¤
        show_next_steps()
        
        print("\nğŸ‰ å¿«é€Ÿæ¼”ç¤ºå®Œæˆï¼")
        print("AI Evaluation Engine å·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨äº†ï¼")
        
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()