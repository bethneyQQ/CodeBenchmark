#!/usr/bin/env python3
"""
AI Evaluation Engine å®Œæ•´å·¥ä½œæµç¨‹ç¤ºä¾‹

è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†ä¸€ä¸ªå®Œæ•´çš„AIæ¨¡å‹è¯„ä¼°å·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
1. ç¯å¢ƒåˆå§‹åŒ–å’Œé…ç½®
2. æ¨¡å‹é…ç½®å’Œæ³¨å†Œ
3. ä»»åŠ¡é€‰æ‹©å’Œé…ç½®
4. æ‰¹é‡è¯„ä¼°æ‰§è¡Œ
5. ç»“æœåˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ
6. æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–å»ºè®®
"""

import os
import sys
import json
import asyncio
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append('.')

# å¯¼å…¥evaluation engineç»„ä»¶
from evaluation_engine.core.unified_framework import (
    UnifiedEvaluationFramework,
    EvaluationRequest,
    EvaluationResult,
    ExecutionStatus
)
from evaluation_engine.core.advanced_model_config import (
    AdvancedModelConfigurationManager,
    ModelConfiguration,
    TaskType,
    OptimizationStrategy,
    RateLimitConfig,
    PerformanceMonitor
)
from evaluation_engine.core.model_adapters import ModelType

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class CompleteWorkflowManager:
    """å®Œæ•´å·¥ä½œæµç¨‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.framework = UnifiedEvaluationFramework()
        self.config_manager = AdvancedModelConfigurationManager()
        self.performance_monitor = PerformanceMonitor()
        self.results: Dict[str, EvaluationResult] = {}
        self.workflow_config = {}
        
    def initialize_environment(self) -> bool:
        """åˆå§‹åŒ–è¯„ä¼°ç¯å¢ƒ"""
        
        logger.info("ğŸ”§ åˆå§‹åŒ–è¯„ä¼°ç¯å¢ƒ")
        
        try:
            # æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡
            required_env_vars = {
                'ANTHROPIC_API_KEY': 'Anthropic Claude API',
                'OPENAI_API_KEY': 'OpenAI GPT API',
                'DEEPSEEK_API_KEY': 'DeepSeek API',
                'DASHSCOPE_API_KEY': 'é€šä¹‰åƒé—® API'
            }
            
            available_apis = {}
            for env_var, description in required_env_vars.items():
                if os.getenv(env_var):
                    available_apis[env_var] = description
                    logger.info(f"âœ… {description} - å·²é…ç½®")
                else:
                    logger.warning(f"âš ï¸  {description} - æœªé…ç½®")
            
            if not available_apis:
                logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•APIå¯†é’¥ï¼Œæ— æ³•è¿›è¡Œå®é™…è¯„ä¼°")
                return False
            
            # åˆ›å»ºå¿…è¦çš„ç›®å½•
            directories = ['results', 'logs', 'cache', 'reports']
            for directory in directories:
                Path(directory).mkdir(exist_ok=True)
                logger.info(f"ğŸ“ åˆ›å»ºç›®å½•: {directory}")
            
            # å¯åŠ¨æ€§èƒ½ç›‘æ§
            self.performance_monitor.start_monitoring()
            logger.info("ğŸ“ˆ æ€§èƒ½ç›‘æ§å·²å¯åŠ¨")
            
            logger.info(f"âœ… ç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨API: {len(available_apis)}ä¸ª")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def setup_model_configurations(self) -> Dict[str, ModelConfiguration]:
        """è®¾ç½®æ¨¡å‹é…ç½®"""
        
        logger.info("âš™ï¸ è®¾ç½®æ¨¡å‹é…ç½®")
        
        configurations = {}
        
        # Claudeé…ç½®
        if os.getenv('ANTHROPIC_API_KEY'):
            claude_config = ModelConfiguration(
                model_id="claude-3-haiku",
                model_type=ModelType.ANTHROPIC_CLAUDE,
                temperature=0.7,
                max_tokens=2048,
                top_p=0.9,
                api_key=os.getenv('ANTHROPIC_API_KEY'),
                rate_limit_config=RateLimitConfig(
                    requests_per_minute=60,
                    tokens_per_minute=100000,
                    concurrent_requests=3
                ),
                max_cost_per_request=1.0,
                daily_budget=50.0,
                target_response_time=5.0,
                target_success_rate=0.95
            )
            configurations['claude'] = claude_config
            self.config_manager.register_model_configuration('claude', claude_config)
            logger.info("âœ… Claudeé…ç½®å·²æ³¨å†Œ")
        
        # OpenAIé…ç½®
        if os.getenv('OPENAI_API_KEY'):
            openai_config = ModelConfiguration(
                model_id="gpt-3.5-turbo",
                model_type=ModelType.OPENAI_GPT,
                temperature=0.7,
                max_tokens=2048,
                api_key=os.getenv('OPENAI_API_KEY'),
                rate_limit_config=RateLimitConfig(
                    requests_per_minute=60,
                    tokens_per_minute=90000,
                    concurrent_requests=3
                ),
                max_cost_per_request=0.5,
                daily_budget=30.0
            )
            configurations['openai'] = openai_config
            self.config_manager.register_model_configuration('openai', openai_config)
            logger.info("âœ… OpenAIé…ç½®å·²æ³¨å†Œ")
        
        # DeepSeeké…ç½®
        if os.getenv('DEEPSEEK_API_KEY'):
            deepseek_config = ModelConfiguration(
                model_id="deepseek-coder",
                model_type=ModelType.DEEPSEEK,
                temperature=0.7,
                max_tokens=2048,
                api_key=os.getenv('DEEPSEEK_API_KEY'),
                rate_limit_config=RateLimitConfig(
                    requests_per_minute=100,
                    tokens_per_minute=200000,
                    concurrent_requests=5
                ),
                max_cost_per_request=0.1,
                daily_budget=20.0
            )
            configurations['deepseek'] = deepseek_config
            self.config_manager.register_model_configuration('deepseek', deepseek_config)
            logger.info("âœ… DeepSeeké…ç½®å·²æ³¨å†Œ")
        
        # é€šä¹‰åƒé—®é…ç½®
        if os.getenv('DASHSCOPE_API_KEY'):
            qwen_config = ModelConfiguration(
                model_id="qwen-plus",
                model_type=ModelType.DASHSCOPE,
                temperature=0.7,
                max_tokens=2048,
                api_key=os.getenv('DASHSCOPE_API_KEY'),
                rate_limit_config=RateLimitConfig(
                    requests_per_minute=60,
                    tokens_per_minute=120000,
                    concurrent_requests=3
                ),
                max_cost_per_request=0.3,
                daily_budget=25.0
            )
            configurations['qwen'] = qwen_config
            self.config_manager.register_model_configuration('qwen', qwen_config)
            logger.info("âœ… é€šä¹‰åƒé—®é…ç½®å·²æ³¨å†Œ")
        
        logger.info(f"âœ… æ¨¡å‹é…ç½®å®Œæˆï¼Œå…±æ³¨å†Œ {len(configurations)} ä¸ªæ¨¡å‹")
        return configurations
    
    def define_evaluation_plan(self) -> Dict[str, Any]:
        """å®šä¹‰è¯„ä¼°è®¡åˆ’"""
        
        logger.info("ğŸ“‹ å®šä¹‰è¯„ä¼°è®¡åˆ’")
        
        # åŸºç¡€è¯„ä¼°è®¡åˆ’
        plan = {
            "experiment_name": "comprehensive_ai_evaluation",
            "description": "å…¨é¢çš„AIæ¨¡å‹è¯„ä¼°å®éªŒ",
            "timestamp": datetime.now().isoformat(),
            
            # æ¨¡å‹é…ç½®
            "models": [],
            
            # ä»»åŠ¡ç»„
            "task_groups": [
                {
                    "name": "basic_coding",
                    "description": "åŸºç¡€ç¼–ç¨‹ä»»åŠ¡",
                    "tasks": [
                        "single_turn_scenarios_function_generation",
                        "single_turn_scenarios_code_completion"
                    ],
                    "priority": 1
                },
                {
                    "name": "advanced_coding", 
                    "description": "é«˜çº§ç¼–ç¨‹ä»»åŠ¡",
                    "tasks": [
                        "single_turn_scenarios_bug_fix",
                        "single_turn_scenarios_algorithm_implementation"
                    ],
                    "priority": 2
                },
                {
                    "name": "specialized_tasks",
                    "description": "ä¸“ä¸šåŒ–ä»»åŠ¡",
                    "tasks": [
                        "single_turn_scenarios_api_design",
                        "single_turn_scenarios_system_design"
                    ],
                    "priority": 3
                }
            ],
            
            # è¯„ä¼°é…ç½®
            "evaluation_configs": [
                {
                    "name": "conservative",
                    "description": "ä¿å®ˆé…ç½® - ä½æ¸©åº¦ï¼Œé«˜ç¡®å®šæ€§",
                    "parameters": {
                        "temperature": 0.3,
                        "max_gen_toks": 1024,
                        "top_p": 0.8
                    }
                },
                {
                    "name": "balanced",
                    "description": "å¹³è¡¡é…ç½® - ä¸­ç­‰æ¸©åº¦",
                    "parameters": {
                        "temperature": 0.7,
                        "max_gen_toks": 1024,
                        "top_p": 0.9
                    }
                },
                {
                    "name": "creative",
                    "description": "åˆ›é€ æ€§é…ç½® - é«˜æ¸©åº¦ï¼Œæ›´å¤šéšæœºæ€§",
                    "parameters": {
                        "temperature": 0.9,
                        "max_gen_toks": 1024,
                        "top_p": 0.95
                    }
                }
            ],
            
            # æ‰§è¡Œé…ç½®
            "execution": {
                "limit": 3,  # æ¯ä¸ªä»»åŠ¡çš„æ ·æœ¬æ•°é‡
                "batch_size": 1,
                "timeout": 300,  # 5åˆ†é’Ÿè¶…æ—¶
                "max_concurrent": 3,
                "retry_attempts": 2
            }
        }
        
        # æ ¹æ®å¯ç”¨çš„APIæ·»åŠ æ¨¡å‹
        model_mappings = {
            'ANTHROPIC_API_KEY': {
                "name": "claude",
                "model": "claude-local",
                "model_args": "model=claude-3-haiku-20240307",
                "provider": "anthropic"
            },
            'OPENAI_API_KEY': {
                "name": "openai",
                "model": "openai-completions", 
                "model_args": "model=gpt-3.5-turbo",
                "provider": "openai"
            },
            'DEEPSEEK_API_KEY': {
                "name": "deepseek",
                "model": "deepseek",
                "model_args": "model=deepseek-coder",
                "provider": "deepseek"
            },
            'DASHSCOPE_API_KEY': {
                "name": "qwen",
                "model": "dashscope",
                "model_args": "model=qwen-plus",
                "provider": "dashscope"
            }
        }
        
        for env_var, model_info in model_mappings.items():
            if os.getenv(env_var):
                plan["models"].append(model_info)
        
        self.workflow_config = plan
        logger.info(f"âœ… è¯„ä¼°è®¡åˆ’å·²å®šä¹‰: {len(plan['models'])} ä¸ªæ¨¡å‹, {len(plan['task_groups'])} ä¸ªä»»åŠ¡ç»„")
        
        return plan
    
    async def execute_evaluation_batch(self, 
                                     model_info: Dict[str, str],
                                     task_group: Dict[str, Any],
                                     config: Dict[str, Any]) -> Optional[EvaluationResult]:
        """æ‰§è¡Œå•ä¸ªè¯„ä¼°æ‰¹æ¬¡"""
        
        batch_name = f"{model_info['name']}_{task_group['name']}_{config['name']}"
        logger.info(f"ğŸš€ å¼€å§‹è¯„ä¼°æ‰¹æ¬¡: {batch_name}")
        
        try:
            # åˆ›å»ºè¯„ä¼°è¯·æ±‚
            request = EvaluationRequest(
                model=model_info["model"],
                tasks=task_group["tasks"],
                limit=self.workflow_config["execution"]["limit"],
                batch_size=self.workflow_config["execution"]["batch_size"],
                gen_kwargs=config["parameters"],
                output_base_path=f"results/{batch_name}",
                log_samples=True,
                verbosity="INFO",
                predict_only=False,
                random_seed=42
            )
            
            # å¦‚æœæœ‰model_argsï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
            if "model_args" in model_info and model_info["model_args"]:
                # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„lm_evalæ¥å£è°ƒæ•´
                pass
            
            # æ‰§è¡Œè¯„ä¼°
            start_time = datetime.now()
            result = self.framework.evaluate(request)
            end_time = datetime.now()
            
            # è®°å½•æ€§èƒ½æ•°æ®
            execution_time = (end_time - start_time).total_seconds()
            success = result.status == ExecutionStatus.COMPLETED
            
            self.performance_monitor.record_performance(
                model_id=model_info["name"],
                response_time=execution_time,
                success=success,
                cost=0.01,  # æ¨¡æ‹Ÿæˆæœ¬
                quality=0.8 if success else 0.0  # æ¨¡æ‹Ÿè´¨é‡åˆ†æ•°
            )
            
            # å­˜å‚¨ç»“æœ
            self.results[batch_name] = result
            
            if success:
                avg_score = sum(result.metrics_summary.values()) / len(result.metrics_summary) if result.metrics_summary else 0
                logger.info(f"âœ… {batch_name} å®Œæˆ - å¹³å‡åˆ†æ•°: {avg_score:.3f}, æ—¶é—´: {execution_time:.1f}s")
            else:
                logger.error(f"âŒ {batch_name} å¤±è´¥: {result.error}")
            
            return result
            
        except Exception as e:
            logger.error(f"ğŸ’¥ {batch_name} å¼‚å¸¸: {e}")
            return None
    
    async def run_comprehensive_evaluation(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢è¯„ä¼°"""
        
        logger.info("ğŸ¯ å¼€å§‹å…¨é¢è¯„ä¼°")
        
        plan = self.workflow_config
        total_batches = len(plan["models"]) * len(plan["task_groups"]) * len(plan["evaluation_configs"])
        completed_batches = 0
        
        logger.info(f"ğŸ“Š è®¡åˆ’æ‰§è¡Œ {total_batches} ä¸ªè¯„ä¼°æ‰¹æ¬¡")
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œè¯„ä¼°
        max_workers = min(plan["execution"]["max_concurrent"], total_batches)
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰è¯„ä¼°ä»»åŠ¡
            futures = []
            
            for model_info in plan["models"]:
                for task_group in plan["task_groups"]:
                    for config in plan["evaluation_configs"]:
                        future = executor.submit(
                            asyncio.run,
                            self.execute_evaluation_batch(model_info, task_group, config)
                        )
                        futures.append((f"{model_info['name']}_{task_group['name']}_{config['name']}", future))
            
            # æ”¶é›†ç»“æœ
            for batch_name, future in futures:
                try:
                    result = future.result(timeout=plan["execution"]["timeout"])
                    completed_batches += 1
                    
                    progress = (completed_batches / total_batches) * 100
                    logger.info(f"ğŸ“ˆ è¿›åº¦: {completed_batches}/{total_batches} ({progress:.1f}%)")
                    
                except Exception as e:
                    logger.error(f"âŒ æ‰¹æ¬¡ {batch_name} æ‰§è¡Œå¤±è´¥: {e}")
        
        logger.info(f"âœ… å…¨é¢è¯„ä¼°å®Œæˆ: {completed_batches}/{total_batches} ä¸ªæ‰¹æ¬¡æˆåŠŸ")
        
        return {
            "total_batches": total_batches,
            "completed_batches": completed_batches,
            "success_rate": completed_batches / total_batches if total_batches > 0 else 0,
            "results": self.results
        }
    
    def generate_comprehensive_report(self, evaluation_summary: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        
        logger.info("ğŸ“Š ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š")
        
        report = {
            "experiment_info": {
                "name": self.workflow_config["experiment_name"],
                "description": self.workflow_config["description"],
                "timestamp": datetime.now().isoformat(),
                "execution_summary": evaluation_summary
            },
            "model_performance": {},
            "task_analysis": {},
            "configuration_analysis": {},
            "performance_insights": {},
            "recommendations": []
        }
        
        # åˆ†ææ¨¡å‹æ€§èƒ½
        model_scores = {}
        model_times = {}
        
        for batch_name, result in self.results.items():
            if result.status == ExecutionStatus.COMPLETED and result.metrics_summary:
                parts = batch_name.split('_')
                model_name = parts[0]
                task_group = parts[1]
                config_name = parts[2]
                
                # æ”¶é›†åˆ†æ•°æ•°æ®
                if model_name not in model_scores:
                    model_scores[model_name] = []
                    model_times[model_name] = []
                
                avg_score = sum(result.metrics_summary.values()) / len(result.metrics_summary)
                exec_time = self.framework._calculate_execution_time(result)
                
                model_scores[model_name].append(avg_score)
                model_times[model_name].append(exec_time)
        
        # è®¡ç®—æ¨¡å‹ç»Ÿè®¡
        for model_name in model_scores:
            scores = model_scores[model_name]
            times = model_times[model_name]
            
            if scores:
                report["model_performance"][model_name] = {
                    "average_score": sum(scores) / len(scores),
                    "best_score": max(scores),
                    "worst_score": min(scores),
                    "score_std": self._calculate_std(scores),
                    "average_time": sum(times) / len(times),
                    "total_evaluations": len(scores),
                    "consistency": 1.0 - (max(scores) - min(scores)) if len(scores) > 1 else 1.0
                }
        
        # åˆ†æä»»åŠ¡ç»„æ€§èƒ½
        task_group_performance = {}
        for batch_name, result in self.results.items():
            if result.status == ExecutionStatus.COMPLETED and result.metrics_summary:
                parts = batch_name.split('_')
                task_group = parts[1]
                
                if task_group not in task_group_performance:
                    task_group_performance[task_group] = []
                
                avg_score = sum(result.metrics_summary.values()) / len(result.metrics_summary)
                task_group_performance[task_group].append(avg_score)
        
        for task_group, scores in task_group_performance.items():
            if scores:
                report["task_analysis"][task_group] = {
                    "average_score": sum(scores) / len(scores),
                    "best_score": max(scores),
                    "difficulty_level": self._assess_difficulty(sum(scores) / len(scores))
                }
        
        # åˆ†æé…ç½®æ€§èƒ½
        config_performance = {}
        for batch_name, result in self.results.items():
            if result.status == ExecutionStatus.COMPLETED and result.metrics_summary:
                parts = batch_name.split('_')
                config_name = parts[2]
                
                if config_name not in config_performance:
                    config_performance[config_name] = []
                
                avg_score = sum(result.metrics_summary.values()) / len(result.metrics_summary)
                config_performance[config_name].append(avg_score)
        
        for config_name, scores in config_performance.items():
            if scores:
                report["configuration_analysis"][config_name] = {
                    "average_score": sum(scores) / len(scores),
                    "effectiveness": "high" if sum(scores) / len(scores) > 0.8 else "medium" if sum(scores) / len(scores) > 0.6 else "low"
                }
        
        # ç”Ÿæˆæ€§èƒ½æ´å¯Ÿ
        if model_scores:
            best_model = max(model_scores.keys(), key=lambda m: sum(model_scores[m]) / len(model_scores[m]))
            fastest_model = min(model_times.keys(), key=lambda m: sum(model_times[m]) / len(model_times[m]))
            
            report["performance_insights"] = {
                "best_overall_model": best_model,
                "fastest_model": fastest_model,
                "most_consistent_model": max(model_scores.keys(), 
                                           key=lambda m: report["model_performance"][m]["consistency"]),
                "total_models_tested": len(model_scores),
                "total_successful_evaluations": sum(len(scores) for scores in model_scores.values())
            }
        
        # ç”Ÿæˆå»ºè®®
        recommendations = []
        
        if model_scores:
            best_model = report["performance_insights"]["best_overall_model"]
            recommendations.append(f"æ¨èä½¿ç”¨ {best_model} æ¨¡å‹ï¼Œæ•´ä½“æ€§èƒ½æœ€ä½³")
            
            # æ£€æŸ¥ä¸€è‡´æ€§
            for model_name, perf in report["model_performance"].items():
                if perf["consistency"] < 0.7:
                    recommendations.append(f"{model_name} æ¨¡å‹æ€§èƒ½ä¸å¤Ÿç¨³å®šï¼Œå»ºè®®è°ƒä¼˜å‚æ•°")
        
        if config_performance:
            best_config = max(config_performance.keys(), 
                            key=lambda c: sum(config_performance[c]) / len(config_performance[c]))
            recommendations.append(f"æ¨èä½¿ç”¨ {best_config} é…ç½®ï¼Œå¹³å‡æ€§èƒ½æœ€ä½³")
        
        if task_group_performance:
            difficult_tasks = [task for task, perf in report["task_analysis"].items() 
                             if perf["difficulty_level"] == "hard"]
            if difficult_tasks:
                recommendations.append(f"ä»¥ä¸‹ä»»åŠ¡ç»„è¾ƒä¸ºå›°éš¾ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šä¼˜åŒ–: {', '.join(difficult_tasks)}")
        
        report["recommendations"] = recommendations
        
        logger.info("âœ… ç»¼åˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        return report
    
    def _calculate_std(self, values: List[float]) -> float:
        """è®¡ç®—æ ‡å‡†å·®"""
        if len(values) < 2:
            return 0.0
        
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return variance ** 0.5
    
    def _assess_difficulty(self, avg_score: float) -> str:
        """è¯„ä¼°ä»»åŠ¡éš¾åº¦"""
        if avg_score >= 0.8:
            return "easy"
        elif avg_score >= 0.6:
            return "medium"
        else:
            return "hard"
    
    def save_results_and_report(self, report: Dict[str, Any]):
        """ä¿å­˜ç»“æœå’ŒæŠ¥å‘Š"""
        
        logger.info("ğŸ’¾ ä¿å­˜ç»“æœå’ŒæŠ¥å‘Š")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜ç»¼åˆæŠ¥å‘Š
        report_file = f"reports/comprehensive_report_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ“„ ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_results = {}
        for batch_name, result in self.results.items():
            detailed_results[batch_name] = {
                "evaluation_id": result.evaluation_id,
                "status": result.status.value,
                "start_time": result.start_time.isoformat(),
                "end_time": result.end_time.isoformat() if result.end_time else None,
                "execution_time": self.framework._calculate_execution_time(result),
                "metrics_summary": result.metrics_summary,
                "analysis": result.analysis,
                "error": result.error
            }
        
        results_file = f"reports/detailed_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ“Š è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_file}")
        
        # ç”Ÿæˆç®€åŒ–çš„MarkdownæŠ¥å‘Š
        self._generate_markdown_report(report, f"reports/summary_report_{timestamp}.md")
        
        return report_file, results_file
    
    def _generate_markdown_report(self, report: Dict[str, Any], filename: str):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"# {report['experiment_info']['name']}\n\n")
            f.write(f"**æè¿°**: {report['experiment_info']['description']}\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {report['experiment_info']['timestamp']}\n\n")
            
            # æ‰§è¡Œæ‘˜è¦
            exec_summary = report['experiment_info']['execution_summary']
            f.write("## æ‰§è¡Œæ‘˜è¦\n\n")
            f.write(f"- æ€»æ‰¹æ¬¡æ•°: {exec_summary['total_batches']}\n")
            f.write(f"- å®Œæˆæ‰¹æ¬¡æ•°: {exec_summary['completed_batches']}\n")
            f.write(f"- æˆåŠŸç‡: {exec_summary['success_rate']:.1%}\n\n")
            
            # æ¨¡å‹æ€§èƒ½
            if report['model_performance']:
                f.write("## æ¨¡å‹æ€§èƒ½æ’å\n\n")
                sorted_models = sorted(report['model_performance'].items(),
                                     key=lambda x: x[1]['average_score'], reverse=True)
                
                for i, (model_name, perf) in enumerate(sorted_models, 1):
                    f.write(f"### {i}. {model_name}\n")
                    f.write(f"- å¹³å‡åˆ†æ•°: {perf['average_score']:.3f}\n")
                    f.write(f"- æœ€ä½³åˆ†æ•°: {perf['best_score']:.3f}\n")
                    f.write(f"- å¹³å‡æ‰§è¡Œæ—¶é—´: {perf['average_time']:.1f}s\n")
                    f.write(f"- ä¸€è‡´æ€§: {perf['consistency']:.3f}\n")
                    f.write(f"- æ€»è¯„ä¼°æ•°: {perf['total_evaluations']}\n\n")
            
            # ä»»åŠ¡åˆ†æ
            if report['task_analysis']:
                f.write("## ä»»åŠ¡ç»„åˆ†æ\n\n")
                for task_group, analysis in report['task_analysis'].items():
                    f.write(f"### {task_group}\n")
                    f.write(f"- å¹³å‡åˆ†æ•°: {analysis['average_score']:.3f}\n")
                    f.write(f"- æœ€ä½³åˆ†æ•°: {analysis['best_score']:.3f}\n")
                    f.write(f"- éš¾åº¦ç­‰çº§: {analysis['difficulty_level']}\n\n")
            
            # å»ºè®®
            if report['recommendations']:
                f.write("## å»ºè®®\n\n")
                for rec in report['recommendations']:
                    f.write(f"- {rec}\n")
                f.write("\n")
        
        logger.info(f"ğŸ“ MarkdownæŠ¥å‘Šå·²ä¿å­˜: {filename}")
    
    def display_summary(self, report: Dict[str, Any]):
        """æ˜¾ç¤ºè¯„ä¼°æ‘˜è¦"""
        
        print("\n" + "="*80)
        print("ğŸ‰ AI Evaluation Engine å®Œæ•´å·¥ä½œæµç¨‹æ‰§è¡Œå®Œæˆ")
        print("="*80)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"\nğŸ“‹ å®éªŒä¿¡æ¯:")
        print(f"   åç§°: {report['experiment_info']['name']}")
        print(f"   æè¿°: {report['experiment_info']['description']}")
        
        # æ‰§è¡Œæ‘˜è¦
        exec_summary = report['experiment_info']['execution_summary']
        print(f"\nğŸ“Š æ‰§è¡Œæ‘˜è¦:")
        print(f"   æ€»æ‰¹æ¬¡æ•°: {exec_summary['total_batches']}")
        print(f"   å®Œæˆæ‰¹æ¬¡æ•°: {exec_summary['completed_batches']}")
        print(f"   æˆåŠŸç‡: {exec_summary['success_rate']:.1%}")
        
        # æ¨¡å‹æ€§èƒ½æ’å
        if report['model_performance']:
            print(f"\nğŸ† æ¨¡å‹æ€§èƒ½æ’å:")
            sorted_models = sorted(report['model_performance'].items(),
                                 key=lambda x: x[1]['average_score'], reverse=True)
            
            for i, (model_name, perf) in enumerate(sorted_models, 1):
                print(f"   {i}. {model_name}: {perf['average_score']:.3f} "
                      f"(æ—¶é—´: {perf['average_time']:.1f}s, ä¸€è‡´æ€§: {perf['consistency']:.3f})")
        
        # æ€§èƒ½æ´å¯Ÿ
        if report['performance_insights']:
            insights = report['performance_insights']
            print(f"\nğŸ’¡ æ€§èƒ½æ´å¯Ÿ:")
            print(f"   æœ€ä½³æ•´ä½“æ¨¡å‹: {insights['best_overall_model']}")
            print(f"   æœ€å¿«æ¨¡å‹: {insights['fastest_model']}")
            print(f"   æœ€ä¸€è‡´æ¨¡å‹: {insights['most_consistent_model']}")
            print(f"   æµ‹è¯•æ¨¡å‹æ€»æ•°: {insights['total_models_tested']}")
            print(f"   æˆåŠŸè¯„ä¼°æ€»æ•°: {insights['total_successful_evaluations']}")
        
        # å»ºè®®
        if report['recommendations']:
            print(f"\nğŸ”§ å»ºè®®:")
            for rec in report['recommendations']:
                print(f"   â€¢ {rec}")
        
        print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Šå’Œç»“æœæ–‡ä»¶å·²ä¿å­˜åˆ° reports/ ç›®å½•")
        print("="*80)
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        
        logger.info("ğŸ§¹ æ¸…ç†èµ„æº")
        
        try:
            self.performance_monitor.stop_monitoring()
            logger.info("âœ… æ€§èƒ½ç›‘æ§å·²åœæ­¢")
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸš€ AI Evaluation Engine å®Œæ•´å·¥ä½œæµç¨‹")
    print("="*80)
    
    workflow_manager = CompleteWorkflowManager()
    
    try:
        # 1. åˆå§‹åŒ–ç¯å¢ƒ
        if not workflow_manager.initialize_environment():
            print("âŒ ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
            return
        
        # 2. è®¾ç½®æ¨¡å‹é…ç½®
        configurations = workflow_manager.setup_model_configurations()
        if not configurations:
            print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹é…ç½®ï¼Œé€€å‡ºç¨‹åº")
            return
        
        # 3. å®šä¹‰è¯„ä¼°è®¡åˆ’
        plan = workflow_manager.define_evaluation_plan()
        
        # 4. è¿è¡Œå…¨é¢è¯„ä¼°
        evaluation_summary = await workflow_manager.run_comprehensive_evaluation()
        
        # 5. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = workflow_manager.generate_comprehensive_report(evaluation_summary)
        
        # 6. ä¿å­˜ç»“æœå’ŒæŠ¥å‘Š
        report_file, results_file = workflow_manager.save_results_and_report(report)
        
        # 7. æ˜¾ç¤ºæ‘˜è¦
        workflow_manager.display_summary(report)
        
        print(f"\nâœ… å·¥ä½œæµç¨‹æ‰§è¡Œå®Œæˆï¼")
        print(f"ğŸ“„ ä¸»è¦æŠ¥å‘Š: {report_file}")
        print(f"ğŸ“Š è¯¦ç»†ç»“æœ: {results_file}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        logger.error(f"âŒ å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        workflow_manager.cleanup()


if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´å·¥ä½œæµç¨‹
    asyncio.run(main())