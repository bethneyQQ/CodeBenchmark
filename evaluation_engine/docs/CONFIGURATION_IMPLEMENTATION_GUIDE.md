# Evaluation Engine é…ç½®å®ç°æ–¹å¼è¯¦è§£

## ğŸ¯ æ¦‚è¿°

Evaluation Engineä¸­çš„taskå’Œmetricsé…ç½®é‡‡ç”¨äº†**æ··åˆé…ç½®æ–¹å¼**ï¼Œç»“åˆäº†æ–‡ä»¶é…ç½®ã€ä»£ç é…ç½®å’ŒAPIåŠ¨æ€é…ç½®ä¸‰ç§æ–¹å¼ï¼Œä»¥æä¾›æœ€å¤§çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚

## ğŸ“ é…ç½®æ–‡ä»¶å­˜æ”¾ä½ç½®

### 1. Taské…ç½®æ–‡ä»¶ä½ç½®

#### ä¸»è¦ä»»åŠ¡é…ç½®ç›®å½•
```
lm_eval/tasks/
â”œâ”€â”€ single_turn_scenarios/          # å•è½®åœºæ™¯ä»»åŠ¡
â”‚   â”œâ”€â”€ function_generation.yaml    # å‡½æ•°ç”Ÿæˆä»»åŠ¡é…ç½®
â”‚   â”œâ”€â”€ code_completion.yaml        # ä»£ç è¡¥å…¨ä»»åŠ¡é…ç½®
â”‚   â”œâ”€â”€ bug_fix.yaml                # Bugä¿®å¤ä»»åŠ¡é…ç½®
â”‚   â”œâ”€â”€ context_configs.json        # ä¸Šä¸‹æ–‡é…ç½®
â”‚   â””â”€â”€ problems.jsonl              # æ•°æ®é›†æ–‡ä»¶
â”œâ”€â”€ multi_turn_scenarios/           # å¤šè½®åœºæ™¯ä»»åŠ¡
â”‚   â”œâ”€â”€ debugging_session/          # è°ƒè¯•ä¼šè¯åœºæ™¯
â”‚   â”œâ”€â”€ code_review_process/        # ä»£ç å®¡æŸ¥åœºæ™¯
â”‚   â””â”€â”€ collaborative_development/  # åä½œå¼€å‘åœºæ™¯
â”œâ”€â”€ multi_turn_coding/              # å¤šè½®ç¼–ç¨‹ä»»åŠ¡
â”‚   â”œâ”€â”€ multi_turn_coding.yaml      # ä¸»é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ multi_turn_config.json      # è¿è¡Œæ—¶é…ç½®
â”‚   â””â”€â”€ problems.jsonl              # æ•°æ®é›†æ–‡ä»¶
â””â”€â”€ python_coding/                  # Pythonç¼–ç¨‹ä»»åŠ¡
    â”œâ”€â”€ function_generation.yaml    # å‡½æ•°ç”Ÿæˆé…ç½®
    â”œâ”€â”€ code_completion.yaml        # ä»£ç è¡¥å…¨é…ç½®
    â””â”€â”€ problems.jsonl              # æ•°æ®é›†æ–‡ä»¶
```

#### Evaluation Engineæ‰©å±•é…ç½®
```
evaluation_engine/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ task_registration.py        # ä»»åŠ¡æ³¨å†Œç³»ç»Ÿ
â”‚   â”œâ”€â”€ metrics_engine.py           # æŒ‡æ ‡å¼•æ“
â”‚   â”œâ”€â”€ extended_tasks.py           # æ‰©å±•ä»»åŠ¡ç±»
â”‚   â””â”€â”€ advanced_model_config.py    # é«˜çº§æ¨¡å‹é…ç½®
â”œâ”€â”€ api/
â”‚   â””â”€â”€ models.py                   # APIæ•°æ®æ¨¡å‹
â””â”€â”€ docs/
    â””â”€â”€ config_templates.json       # é…ç½®æ¨¡æ¿ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
```

#### åŠ¨æ€é…ç½®å­˜å‚¨
```
tasks/custom/                       # APIåˆ›å»ºçš„è‡ªå®šä¹‰ä»»åŠ¡
â”œâ”€â”€ custom_task_1.json
â”œâ”€â”€ custom_task_2.json
â””â”€â”€ ...

security/                          # å®‰å…¨é…ç½®
â”œâ”€â”€ alerts/
â””â”€â”€ vuln_db.json

results/                           # ç»“æœå’Œé…ç½®ç¼“å­˜
â”œâ”€â”€ validation_*.json
â””â”€â”€ samples_*.jsonl
```

### 2. Metricsé…ç½®ä½ç½®

#### å†…ç½®æŒ‡æ ‡é…ç½®
```python
# evaluation_engine/core/metrics_engine.py
class MetricsEngine:
    def _initialize_calculators(self) -> Dict[str, Callable]:
        return {
            # æ ‡å‡†NLPæŒ‡æ ‡
            'bleu': self._calculate_bleu,
            'rouge_1': self._calculate_rouge_1,
            'rouge_2': self._calculate_rouge_2,
            'rouge_l': self._calculate_rouge_l,
            'meteor': self._calculate_meteor,
            'exact_match': self._calculate_exact_match,
            
            # ä»£ç è´¨é‡æŒ‡æ ‡
            'syntax_valid': self._calculate_syntax_validity,
            'cyclomatic_complexity': self._calculate_cyclomatic_complexity,
            'security_score': self._calculate_security_score,
            
            # åŠŸèƒ½æŒ‡æ ‡
            'pass_at_1': self._calculate_pass_at_k,
            'pass_at_5': self._calculate_pass_at_k,
            'execution_success': self._calculate_execution_success,
            
            # å¤šè½®æŒ‡æ ‡
            'context_retention': self._calculate_context_retention,
            'conversation_coherence': self._calculate_conversation_coherence,
        }
```

#### ä»»åŠ¡ç‰¹å®šæŒ‡æ ‡é…ç½®
```yaml
# lm_eval/tasks/single_turn_scenarios/function_generation.yaml
metric_list:
  - metric: exact_match
    aggregation: mean
    higher_is_better: true
  - metric: !function metrics.syntax_validity
    aggregation: mean
    higher_is_better: true
  - metric: !function metrics.code_quality_score
    aggregation: mean
    higher_is_better: true
    weight: 0.3
```

## âš™ï¸ é…ç½®å®ç°æ–¹å¼è¯¦è§£

### 1. YAMLæ–‡ä»¶é…ç½®ï¼ˆä¸»è¦æ–¹å¼ï¼‰

#### åŸºç¡€ä»»åŠ¡é…ç½®ç»“æ„
```yaml
# ä»»åŠ¡åŸºæœ¬ä¿¡æ¯
task: single_turn_scenarios_function_generation
dataset_kwargs:
  metadata:
    scenario: "function_generation"

# æ•°æ®åŠ è½½
custom_dataset: !function utils.load_dataset
test_split: test

# æ–‡æ¡£å¤„ç†
process_docs: !function utils.process_docs
doc_to_text: !function utils.doc_to_text
doc_to_target: !function utils.doc_to_target

# è¾“å‡ºç±»å‹
output_type: generate_until

# æŒ‡æ ‡é…ç½®
metric_list:
  - metric: exact_match
    aggregation: mean
    higher_is_better: true
  - metric: !function metrics.syntax_validity
    aggregation: mean
    higher_is_better: true

# ç”Ÿæˆå‚æ•°
generation_kwargs:
  temperature: 0.0
  max_gen_toks: 512
  until: []
  do_sample: false

# è¿‡æ»¤å™¨
filter_list:
  - name: "extract_code"
    filter:
      - function: "custom"
        filter_fn: !function utils.extract_code_response

# å…ƒæ•°æ®
metadata:
  version: 1.0
  scenario: "function_generation"
  description: "Function implementation tasks"
```

#### é«˜çº§é…ç½®ç¤ºä¾‹
```yaml
# å¤šè½®ä»»åŠ¡é…ç½®
task: multi_turn_scenarios_debugging_session
scenario_config:
  scenario_type: "multi_turn"
  max_turns: 5
  conversation_timeout: 300
  enable_context_retention: true
  
  turns:
    - turn_id: "initial_problem"
      role: "user"
      template: "I have a bug in my code: {code}\nError: {error_message}"
      expected_format: "analysis"
      
    - turn_id: "diagnosis"
      role: "assistant"
      template: "Let me analyze this issue..."
      validation_rules:
        - "must_identify_root_cause"
        - "must_suggest_solution"

# ä¸Šä¸‹æ–‡é…ç½®
context_config:
  context_mode: "domain_context"
  context_sources:
    - type: "documentation"
      path: "docs/debugging_guide.md"
      weight: 0.4
    - type: "best_practices"
      path: "standards/debugging_practices.md"
      weight: 0.6

# å®‰å…¨é…ç½®
security_config:
  enable_code_execution: false
  sandbox_environment: "docker"
  allowed_imports: ["os", "sys", "json"]
  resource_limits:
    max_execution_time: 10
    max_memory_mb: 256
```

### 2. JSONé…ç½®æ–‡ä»¶

#### ä¸Šä¸‹æ–‡é…ç½®
```json
// lm_eval/tasks/single_turn_scenarios/context_configs.json
{
  "no_context": {
    "template": "{{prompt}}",
    "description": "Pure problem with no additional context"
  },
  "minimal_context": {
    "template": "{{prompt}}\n\nRequirements:\n{{requirements}}",
    "description": "Basic constraints and requirements"
  },
  "full_context": {
    "template": "Company Standards:\n{{company_standards}}\n\nProblem:\n{{prompt}}\n\nBest Practices:\n{{best_practices}}",
    "description": "Complete company standards and best practices"
  },
  "domain_context": {
    "template": "Domain: {{domain}}\n\nSpecialist Requirements:\n{{domain_requirements}}\n\nProblem:\n{{prompt}}",
    "description": "Domain-specific professional requirements"
  }
}
```

#### è¿è¡Œæ—¶é…ç½®
```json
// lm_eval/tasks/multi_turn_coding/multi_turn_config.json
{
  "model": "claude-3-haiku-20240307",
  "debug": false,
  "allowed_tools": ["Bash", "Python", "FileEditor"],
  "cwd": "./output",
  "max_problems": 5,
  "timeout_per_phase": 300,
  "cleanup_between_runs": true,
  "metrics_config": {
    "enable_code_execution": true,
    "security_checks": true,
    "performance_analysis": false
  }
}
```

### 3. Pythonä»£ç é…ç½®

#### ä»»åŠ¡æ³¨å†Œå’Œé…ç½®
```python
# evaluation_engine/core/task_registration.py
@dataclass
class TaskMetadata:
    task_id: str
    name: str
    description: str
    category: str
    difficulty: str
    tags: List[str] = field(default_factory=list)
    dependencies: List[str] = field(default_factory=list)
    estimated_time: Optional[int] = None
    resource_requirements: Dict[str, Any] = field(default_factory=dict)
    version: str = "1.0.0"

@dataclass
class ScenarioConfig:
    scenario_id: str
    scenario_type: str
    max_turns: int
    conversation_timeout: int
    enable_context_retention: bool
    turns: List[Dict[str, Any]] = field(default_factory=list)
    scenario_metrics: List[str] = field(default_factory=list)
    success_criteria: Dict[str, float] = field(default_factory=dict)

# æ³¨å†Œä»»åŠ¡
extended_registry.register_advanced_task(
    task_class=CustomTask,
    task_name="custom_python_debugging",
    metadata=TaskMetadata(
        task_id="custom_python_debugging",
        name="Python Debugging Task",
        description="Debug Python code and fix errors",
        category="single_turn",
        difficulty="advanced",
        tags=["debugging", "python", "error-fixing"]
    )
)
```

#### æŒ‡æ ‡é…ç½®
```python
# evaluation_engine/core/metrics_engine.py
class MetricsEngine:
    def register_custom_metric(self, 
                             name: str, 
                             calculator: Callable,
                             metric_type: MetricType = MetricType.CUSTOM):
        """æ³¨å†Œè‡ªå®šä¹‰æŒ‡æ ‡"""
        self.custom_metrics[name] = {
            'calculator': calculator,
            'metric_type': metric_type
        }
        self.metric_calculators[name] = calculator
    
    def create_composite_metric(self,
                              name: str,
                              component_metrics: List[str],
                              weights: Optional[List[float]] = None,
                              aggregation_method: str = 'weighted_average'):
        """åˆ›å»ºå¤åˆæŒ‡æ ‡"""
        self.composite_metrics[name] = {
            'components': component_metrics,
            'weights': weights or [1.0] * len(component_metrics),
            'aggregation_method': aggregation_method
        }

# ä½¿ç”¨ç¤ºä¾‹
metrics_engine = MetricsEngine()

# æ³¨å†Œè‡ªå®šä¹‰æŒ‡æ ‡
def calculate_code_elegance(prediction: str, reference: str) -> float:
    # è‡ªå®šä¹‰æŒ‡æ ‡è®¡ç®—é€»è¾‘
    return 0.8

metrics_engine.register_custom_metric(
    "code_elegance", 
    calculate_code_elegance,
    MetricType.CODE_QUALITY
)

# åˆ›å»ºå¤åˆæŒ‡æ ‡
metrics_engine.create_composite_metric(
    "overall_code_quality",
    ["syntax_valid", "code_elegance", "performance_score"],
    [0.3, 0.4, 0.3],
    "weighted_average"
)
```

### 4. APIåŠ¨æ€é…ç½®

#### é€šè¿‡APIåˆ›å»ºä»»åŠ¡é…ç½®
```python
# é€šè¿‡APIåˆ›å»ºè‡ªå®šä¹‰ä»»åŠ¡
task_config = {
    "task_id": "api_created_task",
    "name": "API Created Task",
    "category": "single_turn",
    "difficulty": "intermediate",
    "configuration": {
        "dataset_config": {
            "dataset_path": "custom_data.jsonl",
            "preprocessing": {
                "normalize_whitespace": True,
                "validate_syntax": True
            }
        },
        "evaluation_config": {
            "metrics": ["accuracy", "quality", "performance"],
            "evaluation_criteria": {
                "accuracy": {"weight": 0.4, "threshold": 0.8},
                "quality": {"weight": 0.3, "threshold": 0.7},
                "performance": {"weight": 0.3, "threshold": 0.6}
            }
        },
        "generation_config": {
            "temperature": 0.5,
            "max_tokens": 1024,
            "stop_sequences": ["```"]
        }
    }
}

# ä¿å­˜åˆ°æ–‡ä»¶
import json
with open(f"tasks/custom/{task_config['task_id']}.json", 'w') as f:
    json.dump(task_config, f, indent=2)
```

## ğŸ”§ é…ç½®åŠ è½½å’Œå¤„ç†æµç¨‹

### 1. é…ç½®åŠ è½½é¡ºåº
```python
def load_task_configuration(task_name: str):
    """é…ç½®åŠ è½½ä¼˜å…ˆçº§"""
    
    # 1. æ£€æŸ¥APIåŠ¨æ€é…ç½®
    api_config_path = f"tasks/custom/{task_name}.json"
    if Path(api_config_path).exists():
        return load_json_config(api_config_path)
    
    # 2. æ£€æŸ¥YAMLé…ç½®æ–‡ä»¶
    yaml_config_path = f"lm_eval/tasks/*/{task_name}.yaml"
    yaml_files = glob.glob(yaml_config_path)
    if yaml_files:
        return load_yaml_config(yaml_files[0])
    
    # 3. æ£€æŸ¥ä»£ç æ³¨å†Œçš„é…ç½®
    if task_name in extended_registry.task_metadata:
        return extended_registry.get_task_metadata(task_name)
    
    # 4. ä½¿ç”¨é»˜è®¤é…ç½®
    return create_default_config(task_name)
```

### 2. é…ç½®åˆå¹¶ç­–ç•¥
```python
def merge_configurations(base_config, override_config):
    """é…ç½®åˆå¹¶ç­–ç•¥"""
    merged = base_config.copy()
    
    # æ·±åº¦åˆå¹¶å­—å…¸
    for key, value in override_config.items():
        if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
            merged[key] = merge_configurations(merged[key], value)
        else:
            merged[key] = value
    
    return merged

# é…ç½®ç»§æ‰¿ç¤ºä¾‹
base_config = load_yaml_config("base_task.yaml")
specific_config = load_json_config("specific_overrides.json")
final_config = merge_configurations(base_config, specific_config)
```

### 3. é…ç½®éªŒè¯
```python
def validate_task_configuration(config: Dict[str, Any]) -> List[str]:
    """é…ç½®éªŒè¯"""
    errors = []
    
    # å¿…éœ€å­—æ®µéªŒè¯
    required_fields = ["task", "output_type", "metric_list"]
    for field in required_fields:
        if field not in config:
            errors.append(f"Missing required field: {field}")
    
    # æŒ‡æ ‡é…ç½®éªŒè¯
    if "metric_list" in config:
        for metric in config["metric_list"]:
            if "metric" not in metric:
                errors.append("Metric configuration missing 'metric' field")
            if "aggregation" not in metric:
                errors.append("Metric configuration missing 'aggregation' field")
    
    # ç”Ÿæˆå‚æ•°éªŒè¯
    if "generation_kwargs" in config:
        gen_kwargs = config["generation_kwargs"]
        if "temperature" in gen_kwargs:
            temp = gen_kwargs["temperature"]
            if not 0.0 <= temp <= 2.0:
                errors.append("Temperature must be between 0.0 and 2.0")
    
    return errors
```

## ğŸ“Š é…ç½®æœ€ä½³å®è·µ

### 1. æ–‡ä»¶ç»„ç»‡ç»“æ„
```
evaluation_engine/
â”œâ”€â”€ configs/                    # æ¨èçš„é…ç½®ç›®å½•ç»“æ„
â”‚   â”œâ”€â”€ tasks/                  # ä»»åŠ¡é…ç½®
â”‚   â”‚   â”œâ”€â”€ base/              # åŸºç¡€é…ç½®æ¨¡æ¿
â”‚   â”‚   â”œâ”€â”€ single_turn/       # å•è½®ä»»åŠ¡é…ç½®
â”‚   â”‚   â”œâ”€â”€ multi_turn/        # å¤šè½®ä»»åŠ¡é…ç½®
â”‚   â”‚   â””â”€â”€ custom/            # è‡ªå®šä¹‰ä»»åŠ¡é…ç½®
â”‚   â”œâ”€â”€ metrics/               # æŒ‡æ ‡é…ç½®
â”‚   â”‚   â”œâ”€â”€ standard.yaml      # æ ‡å‡†æŒ‡æ ‡é…ç½®
â”‚   â”‚   â”œâ”€â”€ code_quality.yaml  # ä»£ç è´¨é‡æŒ‡æ ‡
â”‚   â”‚   â””â”€â”€ custom.yaml        # è‡ªå®šä¹‰æŒ‡æ ‡
â”‚   â”œâ”€â”€ models/                # æ¨¡å‹é…ç½®
â”‚   â”‚   â”œâ”€â”€ claude.yaml        # Claudeæ¨¡å‹é…ç½®
â”‚   â”‚   â”œâ”€â”€ openai.yaml        # OpenAIæ¨¡å‹é…ç½®
â”‚   â”‚   â””â”€â”€ custom.yaml        # è‡ªå®šä¹‰æ¨¡å‹é…ç½®
â”‚   â””â”€â”€ environments/          # ç¯å¢ƒé…ç½®
â”‚       â”œâ”€â”€ development.yaml   # å¼€å‘ç¯å¢ƒ
â”‚       â”œâ”€â”€ testing.yaml       # æµ‹è¯•ç¯å¢ƒ
â”‚       â””â”€â”€ production.yaml    # ç”Ÿäº§ç¯å¢ƒ
```

### 2. é…ç½®æ¨¡æ¿åŒ–
```yaml
# configs/tasks/base/single_turn_template.yaml
task: "{{ task_name }}"
dataset_kwargs:
  metadata:
    scenario: "{{ scenario }}"

custom_dataset: !function utils.load_dataset
test_split: test
output_type: generate_until

process_docs: !function utils.process_docs
doc_to_text: !function utils.doc_to_text
doc_to_target: !function utils.doc_to_target

metric_list: "{{ metrics | default(default_metrics) }}"

generation_kwargs:
  temperature: "{{ temperature | default(0.0) }}"
  max_gen_toks: "{{ max_tokens | default(512) }}"
  until: []
  do_sample: false

metadata:
  version: "{{ version | default('1.0') }}"
  scenario: "{{ scenario }}"
  description: "{{ description }}"
```

### 3. ç¯å¢ƒç‰¹å®šé…ç½®
```python
# ç¯å¢ƒé…ç½®ç®¡ç†
class ConfigManager:
    def __init__(self, environment: str = "development"):
        self.environment = environment
        self.base_config = self.load_base_config()
        self.env_config = self.load_environment_config(environment)
        
    def get_task_config(self, task_name: str) -> Dict[str, Any]:
        """è·å–ä»»åŠ¡é…ç½®ï¼Œåº”ç”¨ç¯å¢ƒç‰¹å®šè¦†ç›–"""
        base_task_config = self.load_task_config(task_name)
        env_overrides = self.env_config.get("task_overrides", {}).get(task_name, {})
        
        return merge_configurations(base_task_config, env_overrides)
    
    def get_metrics_config(self) -> Dict[str, Any]:
        """è·å–æŒ‡æ ‡é…ç½®"""
        base_metrics = self.load_metrics_config()
        env_metrics = self.env_config.get("metrics_overrides", {})
        
        return merge_configurations(base_metrics, env_metrics)

# ä½¿ç”¨ç¤ºä¾‹
config_manager = ConfigManager("production")
task_config = config_manager.get_task_config("function_generation")
metrics_config = config_manager.get_metrics_config()
```

## ğŸš€ é…ç½®è¿ç§»å’Œå‡çº§

### ä»æ–‡ä»¶é…ç½®è¿ç§»åˆ°APIé…ç½®
```python
def migrate_yaml_to_api_config(yaml_file: str) -> Dict[str, Any]:
    """å°†YAMLé…ç½®è¿ç§»åˆ°APIæ ¼å¼"""
    
    with open(yaml_file, 'r') as f:
        yaml_config = yaml.safe_load(f)
    
    # è½¬æ¢ä¸ºAPIé…ç½®æ ¼å¼
    api_config = {
        "task_id": yaml_config["task"],
        "name": yaml_config.get("metadata", {}).get("description", yaml_config["task"]),
        "category": infer_category(yaml_config["task"]),
        "difficulty": "intermediate",  # é»˜è®¤å€¼
        "configuration": {
            "generation_config": yaml_config.get("generation_kwargs", {}),
            "evaluation_config": {
                "metrics": [m["metric"] for m in yaml_config.get("metric_list", [])],
                "evaluation_criteria": extract_criteria(yaml_config.get("metric_list", []))
            },
            "dataset_config": yaml_config.get("dataset_kwargs", {})
        }
    }
    
    return api_config

# æ‰¹é‡è¿ç§»
def migrate_all_yaml_configs():
    yaml_files = glob.glob("lm_eval/tasks/**/*.yaml", recursive=True)
    
    for yaml_file in yaml_files:
        try:
            api_config = migrate_yaml_to_api_config(yaml_file)
            
            # ä¿å­˜ä¸ºJSONé…ç½®
            output_file = f"tasks/custom/{api_config['task_id']}.json"
            with open(output_file, 'w') as f:
                json.dump(api_config, f, indent=2)
                
            print(f"âœ… Migrated {yaml_file} -> {output_file}")
            
        except Exception as e:
            print(f"âŒ Failed to migrate {yaml_file}: {e}")
```

## ğŸ“ æ€»ç»“

Evaluation Engineçš„é…ç½®å®ç°é‡‡ç”¨äº†**åˆ†å±‚æ··åˆé…ç½®æ¶æ„**ï¼š

1. **YAMLæ–‡ä»¶é…ç½®** - ä¸»è¦çš„ä»»åŠ¡å®šä¹‰æ–¹å¼ï¼Œé€‚åˆé™æ€é…ç½®
2. **JSONæ–‡ä»¶é…ç½®** - è¿è¡Œæ—¶é…ç½®å’Œä¸Šä¸‹æ–‡é…ç½®ï¼Œé€‚åˆç»“æ„åŒ–æ•°æ®
3. **Pythonä»£ç é…ç½®** - å¤æ‚é€»è¾‘å’ŒåŠ¨æ€é…ç½®ï¼Œé€‚åˆé«˜çº§åŠŸèƒ½
4. **APIåŠ¨æ€é…ç½®** - è¿è¡Œæ—¶åˆ›å»ºå’Œä¿®æ”¹ï¼Œé€‚åˆè‡ªåŠ¨åŒ–å’Œé›†æˆ

è¿™ç§æ··åˆæ–¹å¼æä¾›äº†ï¼š
- **çµæ´»æ€§** - æ”¯æŒå¤šç§é…ç½®æ–¹å¼
- **å¯æ‰©å±•æ€§** - æ˜“äºæ·»åŠ æ–°çš„é…ç½®ç±»å‹
- **å‘åå…¼å®¹** - ä¿æŒä¸lm-evalçš„å…¼å®¹æ€§
- **åŠ¨æ€æ€§** - æ”¯æŒè¿è¡Œæ—¶é…ç½®ä¿®æ”¹
- **å¯ç»´æŠ¤æ€§** - æ¸…æ™°çš„é…ç½®å±‚æ¬¡å’Œç»§æ‰¿å…³ç³»

é…ç½®æ–‡ä»¶ä¸»è¦å­˜æ”¾åœ¨ï¼š
- `lm_eval/tasks/` - lm-evalåŸç”Ÿä»»åŠ¡é…ç½®
- `evaluation_engine/core/` - æ‰©å±•åŠŸèƒ½é…ç½®
- `tasks/custom/` - APIåˆ›å»ºçš„åŠ¨æ€é…ç½®
- `configs/` - æ¨èçš„ç»Ÿä¸€é…ç½®ç›®å½•ï¼ˆå¯é€‰ï¼‰