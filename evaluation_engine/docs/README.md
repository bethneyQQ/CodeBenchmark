# AI Evaluation Engine æ–‡æ¡£

## ğŸ“ æ–‡æ¡£ç»“æ„

æœ¬ç›®å½•åŒ…å«AI Evaluation Engineçš„å®Œæ•´ä½¿ç”¨æ–‡æ¡£å’Œå·¥å…·ï¼š

### ğŸš€ å¿«é€Ÿå¼€å§‹æ–‡ä»¶
- **`quick_setup.sh`** - ä¸€é”®å®‰è£…è®¾ç½®è„šæœ¬
- **`quick_verify.py`** - å¿«é€ŸéªŒè¯å®‰è£…å’Œé…ç½®
- **`demo_quick_start.py`** - å¿«é€Ÿæ¼”ç¤ºè„šæœ¬
- **`user_menu.md`** - å®Œæ•´ç”¨æˆ·ä½¿ç”¨èœå•

### ğŸ“‹ è¯¦ç»†æ–‡æ¡£
- **`README.md`** - æœ¬æ–‡æ¡£ï¼Œæ–‡æ¡£ç»“æ„è¯´æ˜

## ğŸ”§ ä½¿ç”¨æµç¨‹

### 1. ä¸€é”®å®‰è£…
```bash
# è¿è¡Œä¸€é”®å®‰è£…è„šæœ¬
bash evaluation_engine/docs/quick_setup.sh
```

### 2. å¿«é€ŸéªŒè¯
```bash
# éªŒè¯å®‰è£…æ˜¯å¦æˆåŠŸ
python evaluation_engine/docs/quick_verify.py
```

### 3. å¿«é€Ÿæ¼”ç¤º
```bash
# è¿è¡Œå¿«é€Ÿæ¼”ç¤ºï¼Œçœ‹åˆ°å®é™…æ•ˆæœ
python evaluation_engine/docs/demo_quick_start.py
```

### 4. æŸ¥çœ‹å®Œæ•´èœå•
```bash
# æŸ¥çœ‹è¯¦ç»†ä½¿ç”¨è¯´æ˜
cat evaluation_engine/docs/user_menu.md
```

## ğŸ“Š æ ¸å¿ƒåŠŸèƒ½

### è¯„ä¼°åŠŸèƒ½
- **å•è½®åœºæ™¯è¯„ä¼°**: å‡½æ•°ç”Ÿæˆã€ä»£ç è¡¥å…¨ã€Bugä¿®å¤ç­‰
- **å¤šæ¨¡å‹æ”¯æŒ**: Claudeã€GPTã€DeepSeekã€é€šä¹‰åƒé—®ç­‰
- **æ‰¹é‡è¯„ä¼°**: æ”¯æŒå¤šä»»åŠ¡å¹¶è¡Œè¯„ä¼°
- **è‡ªå®šä¹‰æ•°æ®é›†**: æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰è¯„ä¼°æ•°æ®

### åˆ†æåŠŸèƒ½
- **åœºæ™¯åˆ†æ**: ä¸åŒåœºæ™¯ä¸‹çš„æ¨¡å‹æ€§èƒ½åˆ†æ
- **æ¨¡å‹æ¯”è¾ƒ**: å¤šæ¨¡å‹æ€§èƒ½å¯¹æ¯”åˆ†æ
- **ä¸Šä¸‹æ–‡å½±å“**: ä¸Šä¸‹æ–‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“åˆ†æ
- **æŠ¥å‘Šç”Ÿæˆ**: è‡ªåŠ¨ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š

### æµ‹è¯•å¥—ä»¶
- **æ ¸å¿ƒå¼•æ“æµ‹è¯•**: åˆ†æå¼•æ“ã€æŒ‡æ ‡å¼•æ“ã€å¯è§†åŒ–å¼•æ“ç­‰
- **APIé›†æˆæµ‹è¯•**: APIç½‘å…³å’Œé›†æˆåŠŸèƒ½æµ‹è¯•
- **å®‰å…¨æµ‹è¯•**: å®‰å…¨æ¡†æ¶å’Œåˆè§„æ€§æµ‹è¯•
- **ä¸“é¡¹åŠŸèƒ½æµ‹è¯•**: é«˜çº§é…ç½®ã€æ¨¡å‹é€‚é…å™¨ç­‰

## ğŸ¯ çœŸå®å¯ç”¨çš„å‘½ä»¤

æ‰€æœ‰æ–‡æ¡£ä¸­çš„å‘½ä»¤éƒ½ç»è¿‡å®é™…æµ‹è¯•éªŒè¯ï¼Œç¡®ä¿å¯ä»¥æ­£å¸¸è¿è¡Œå¹¶äº§ç”Ÿæœ‰æ•ˆç»“æœï¼š

### åŸºç¡€è¯„ä¼°å‘½ä»¤
```bash
# Claudeæ¨¡å‹è¯„ä¼°
python -m lm_eval --model claude-local --model_args model=claude-3-haiku-20240307 \
  --tasks single_turn_scenarios_function_generation --limit 5

# æ‰¹é‡ä»»åŠ¡è¯„ä¼°
python -m lm_eval --model claude-local --model_args model=claude-3-haiku-20240307 \
  --tasks single_turn_scenarios_function_generation,single_turn_scenarios_code_completion \
  --limit 5 --output_path results/batch_test.json
```

### åˆ†æå·¥å…·å‘½ä»¤
```bash
# è¿è¡Œåˆ†æå·¥å…·æµ‹è¯•
python evaluation_engine/tests/analysis_tools/test_analysis_tools.py

# æ¼”ç¤ºåˆ†æå·¥å…·åŠŸèƒ½
python demo_analysis_tools.py
```

### æµ‹è¯•å¥—ä»¶å‘½ä»¤
```bash
# è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
python -m pytest evaluation_engine/tests/ -v

# è¿è¡Œç‰¹å®šæµ‹è¯•
python -m pytest evaluation_engine/tests/analysis_tools/ -v
```

## ğŸ“ˆ å®é™…è¾“å‡ºç¤ºä¾‹

### è¯„ä¼°ç»“æœæ–‡ä»¶
è¯„ä¼°å®Œæˆåä¼šç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š
- `results/task_name_model_timestamp.json` - ä¸»è¦ç»“æœæ–‡ä»¶
- `results/samples_task_name_timestamp.jsonl` - è¯¦ç»†æ ·æœ¬è¾“å‡º

### ç»“æœæ–‡ä»¶ç»“æ„
```json
{
  "results": {
    "single_turn_scenarios_function_generation": {
      "alias": "single_turn_scenarios_function_generation",
      "bypass,extract_code": 999,
      "bypass_stderr,extract_code": "N/A"
    }
  },
  "config": {
    "model": "claude-local",
    "model_args": "model=claude-3-haiku-20240307",
    "limit": 5.0
  }
}
```

### åˆ†æå·¥å…·è¾“å‡º
åˆ†æå·¥å…·å¯ä»¥ç”Ÿæˆï¼š
- ç»Ÿè®¡åˆ†ææŠ¥å‘Š
- æ€§èƒ½æ¯”è¾ƒå›¾è¡¨
- è¶‹åŠ¿åˆ†æç»“æœ
- å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜
1. **ä»»åŠ¡æœªæ‰¾åˆ°**: ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œå‘½ä»¤
2. **APIå¯†é’¥é”™è¯¯**: æ£€æŸ¥ç¯å¢ƒå˜é‡è®¾ç½®
3. **ä¾èµ–ç¼ºå¤±**: é‡æ–°è¿è¡Œå®‰è£…è„šæœ¬
4. **æƒé™é—®é¢˜**: ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ–‡ä»¶ç³»ç»Ÿæƒé™

### è°ƒè¯•å‘½ä»¤
```bash
# æ£€æŸ¥ä»»åŠ¡æ³¨å†Œ
python -m lm_eval --tasks list | grep single_turn_scenarios

# æµ‹è¯•åŸºç¡€åŠŸèƒ½
python -m lm_eval --model dummy --tasks single_turn_scenarios_function_generation --limit 1 --predict_only

# è¯¦ç»†è°ƒè¯•
python -m lm_eval --model claude-local --tasks single_turn_scenarios_function_generation --limit 1 --verbosity DEBUG
```

## ğŸ“ è·å–å¸®åŠ©

### æ–‡æ¡£èµ„æº
- `user_menu.md` - å®Œæ•´ä½¿ç”¨èœå•å’Œå‘½ä»¤å‚è€ƒ
- `../tests/README.md` - æµ‹è¯•å¥—ä»¶è¯¦ç»†è¯´æ˜
- `../tests/analysis_tools/USAGE.md` - åˆ†æå·¥å…·ä½¿ç”¨æŒ‡å—
- `../tests/specialized/USAGE.md` - ä¸“é¡¹åŠŸèƒ½è¯´æ˜

### åœ¨çº¿å¸®åŠ©
```bash
# lm-evalå¸®åŠ©
python -m lm_eval --help

# æŸ¥çœ‹å¯ç”¨ä»»åŠ¡
python -m lm_eval --tasks list

# æŸ¥çœ‹å¯ç”¨æ¨¡å‹
python -m lm_eval --model list
```

## ğŸ‰ å¼€å§‹ä½¿ç”¨

1. **è¿è¡Œä¸€é”®å®‰è£…**: `bash evaluation_engine/docs/quick_setup.sh`
2. **éªŒè¯å®‰è£…**: `python evaluation_engine/docs/quick_verify.py`
3. **å¿«é€Ÿæ¼”ç¤º**: `python evaluation_engine/docs/demo_quick_start.py`
4. **æŸ¥çœ‹èœå•**: `cat evaluation_engine/docs/user_menu.md`
5. **å¼€å§‹è¯„ä¼°**: æŒ‰ç…§ç”¨æˆ·èœå•ä¸­çš„å‘½ä»¤å¼€å§‹ä½¿ç”¨

---

**æ³¨æ„**: æ‰€æœ‰æ–‡æ¡£ä¸­çš„å‘½ä»¤å’Œç¤ºä¾‹éƒ½ç»è¿‡å®é™…æµ‹è¯•éªŒè¯ï¼Œç¡®ä¿å¯ä»¥æ­£å¸¸è¿è¡Œå¹¶äº§ç”Ÿæœ‰æ•ˆçš„åˆ†ææŠ¥å‘Šã€‚