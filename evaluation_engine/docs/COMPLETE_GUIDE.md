# AI Evaluation Engine å®Œæ•´ä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—æä¾›äº†AI Evaluation Engineçš„å®Œæ•´ä½¿ç”¨æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸€é”®å®‰è£…ã€APIè°ƒç”¨ã€é«˜çº§é…ç½®å’Œå®Œæ•´çš„æ‰§è¡Œæµç¨‹ã€‚æ‰€æœ‰ç¤ºä¾‹éƒ½ç»è¿‡å®é™…æµ‹è¯•éªŒè¯ï¼Œç¡®ä¿å¯ä»¥æ­£å¸¸è¿è¡Œå¹¶äº§ç”Ÿæœ‰æ•ˆçš„åˆ†ææŠ¥å‘Šã€‚

## ğŸ“ æ–‡æ¡£ç»“æ„

### ğŸš€ å¿«é€Ÿå¼€å§‹æ–‡ä»¶
- **`quick_setup.sh`** - ä¸€é”®å®‰è£…è„šæœ¬ï¼Œè‡ªåŠ¨è®¾ç½®å®Œæ•´ç¯å¢ƒ
- **`quick_verify.py`** - å®‰è£…éªŒè¯è„šæœ¬ï¼Œæ£€æŸ¥æ‰€æœ‰ç»„ä»¶
- **`demo_quick_start.py`** - å¿«é€Ÿæ¼”ç¤ºè„šæœ¬ï¼Œå±•ç¤ºåŸºæœ¬åŠŸèƒ½
- **`user_menu.md`** - å®Œæ•´ç”¨æˆ·èœå•å’Œå‘½ä»¤å‚è€ƒ

### ğŸŒ APIè°ƒç”¨æ–‡ä»¶
- **`api_usage_guide.md`** - è¯¦ç»†çš„APIä½¿ç”¨æŒ‡å—
- **`start_api_server.py`** - APIæœåŠ¡å™¨å¯åŠ¨è„šæœ¬ï¼ˆéœ€è¦åˆ›å»ºï¼‰

### âš™ï¸ é«˜çº§é…ç½®æ–‡ä»¶
- **`advanced_config_examples.py`** - é«˜çº§é…ç½®ç¤ºä¾‹å’ŒA/Bæµ‹è¯•
- **`complete_workflow_example.py`** - å®Œæ•´å·¥ä½œæµç¨‹ç¤ºä¾‹
- **`config_templates.json`** - é…ç½®æ¨¡æ¿æ–‡ä»¶ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰

### ğŸ“š æ–‡æ¡£æ–‡ä»¶
- **`README.md`** - æ–‡æ¡£ç»“æ„è¯´æ˜
- **`INSTALLATION_GUIDE.md`** - è¯¦ç»†å®‰è£…æŒ‡å—
- **`COMPLETE_GUIDE.md`** - æœ¬å®Œæ•´æŒ‡å—

## ğŸ¯ ä½¿ç”¨æµç¨‹

### ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒè®¾ç½®

#### 1.1 è¿è¡Œä¸€é”®å®‰è£…
```bash
# ç»™è„šæœ¬æ·»åŠ æ‰§è¡Œæƒé™
chmod +x evaluation_engine/docs/quick_setup.sh

# è¿è¡Œä¸€é”®å®‰è£…
bash evaluation_engine/docs/quick_setup.sh
```

#### 1.2 é…ç½®APIå¯†é’¥
```bash
# ç¼–è¾‘ç¯å¢ƒé…ç½®æ–‡ä»¶
nano .env

# æˆ–è€…ç›´æ¥è®¾ç½®ç¯å¢ƒå˜é‡
export ANTHROPIC_API_KEY="your_anthropic_key_here"
export OPENAI_API_KEY="your_openai_key_here"
export DEEPSEEK_API_KEY="your_deepseek_key_here"
export DASHSCOPE_API_KEY="your_dashscope_key_here"
```

#### 1.3 éªŒè¯å®‰è£…
```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# è¿è¡ŒéªŒè¯è„šæœ¬
python evaluation_engine/docs/quick_verify.py
```

### ç¬¬äºŒæ­¥ï¼šåŸºç¡€ä½¿ç”¨

#### 2.1 å¿«é€Ÿæ¼”ç¤º
```bash
# è¿è¡Œå¿«é€Ÿæ¼”ç¤º
python evaluation_engine/docs/demo_quick_start.py
```

#### 2.2 åŸºç¡€è¯„ä¼°å‘½ä»¤
```bash
# å‡½æ•°ç”Ÿæˆä»»åŠ¡
python -m lm_eval --model claude-local --model_args model=claude-3-haiku-20240307 \
  --tasks single_turn_scenarios_function_generation --limit 5 \
  --output_path results/function_gen_test.json

# æ‰¹é‡ä»»åŠ¡è¯„ä¼°
python -m lm_eval --model claude-local --model_args model=claude-3-haiku-20240307 \
  --tasks single_turn_scenarios_function_generation,single_turn_scenarios_code_completion \
  --limit 5 --output_path results/batch_test.json --log_samples
```

#### 2.3 åˆ†æå·¥å…·ä½¿ç”¨
```bash
# æµ‹è¯•åˆ†æå·¥å…·
python evaluation_engine/tests/analysis_tools/test_analysis_tools.py

# æ¼”ç¤ºåˆ†æå·¥å…·
python demo_analysis_tools.py
```

### ç¬¬ä¸‰æ­¥ï¼šAPIè°ƒç”¨æ–¹å¼

#### 3.1 å¯åŠ¨APIæœåŠ¡å™¨
```bash
# åˆ›å»ºAPIæœåŠ¡å™¨å¯åŠ¨è„šæœ¬
cat > start_api_server.py << 'EOF'
#!/usr/bin/env python3
import os
import logging
from evaluation_engine.api.gateway import APIGateway
from evaluation_engine.core.unified_framework import UnifiedEvaluationFramework
from evaluation_engine.core.task_registration import TaskRegistry
from evaluation_engine.core.advanced_model_config import AdvancedModelConfigurationManager
from evaluation_engine.core.analysis_engine import AnalysisEngine

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    try:
        framework = UnifiedEvaluationFramework()
        task_registry = TaskRegistry()
        model_config_manager = AdvancedModelConfigurationManager()
        analysis_engine = AnalysisEngine()
        
        gateway = APIGateway(framework, task_registry, model_config_manager, analysis_engine)
        
        logger.info("Starting API server on http://0.0.0.0:8000")
        logger.info("API documentation available at http://0.0.0.0:8000/docs")
        gateway.run(host='0.0.0.0', port=8000)
        
    except Exception as e:
        logger.error(f"Failed to start API server: {e}")

if __name__ == "__main__":
    main()
EOF

# å¯åŠ¨APIæœåŠ¡å™¨
python start_api_server.py
```

#### 3.2 APIè°ƒç”¨ç¤ºä¾‹
```bash
# è·å–è®¿é—®ä»¤ç‰Œ
curl -X POST "http://localhost:8000/auth/login" \
  -H "Content-Type: application/json" \
  -d '{"username": "admin", "password": "admin123"}'

# è®¾ç½®ä»¤ç‰Œ
export ACCESS_TOKEN="your_token_here"

# åˆ›å»ºè¯„ä¼°ä»»åŠ¡
curl -X POST "http://localhost:8000/evaluations" \
  -H "Authorization: Bearer $ACCESS_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "model_id": "claude-3-haiku",
    "task_ids": ["single_turn_scenarios_function_generation"],
    "configuration": {"temperature": 0.7, "max_tokens": 1024}
  }'

# æŸ¥çœ‹è¯„ä¼°çŠ¶æ€
curl -X GET "http://localhost:8000/evaluations/eval_id" \
  -H "Authorization: Bearer $ACCESS_TOKEN"
```

#### 3.3 WebSocketå®æ—¶ç›‘æ§
```javascript
const ws = new WebSocket('ws://localhost:8000/ws?token=' + ACCESS_TOKEN);
ws.onmessage = function(event) {
    const data = JSON.parse(event.data);
    console.log('Progress:', data.data.progress);
};
```

### ç¬¬å››æ­¥ï¼šé«˜çº§é…ç½®

#### 4.1 è¿è¡Œé«˜çº§é…ç½®ç¤ºä¾‹
```bash
# è¿è¡Œå®Œæ•´çš„é«˜çº§é…ç½®ç¤ºä¾‹
python evaluation_engine/docs/advanced_config_examples.py
```

#### 4.2 æ¨¡å‹é…ç½®ç®¡ç†
```python
from evaluation_engine.core.advanced_model_config import (
    ModelConfiguration, 
    TaskType, 
    RateLimitConfig
)

# åˆ›å»ºé«˜çº§æ¨¡å‹é…ç½®
config = ModelConfiguration(
    model_id="claude-3-haiku",
    temperature=0.7,
    max_tokens=2048,
    rate_limit_config=RateLimitConfig(
        requests_per_minute=60,
        tokens_per_minute=100000
    ),
    task_optimizations={
        TaskType.CODE_COMPLETION: {
            "temperature": 0.2,
            "max_tokens": 512
        }
    }
)
```

#### 4.3 A/Bæµ‹è¯•
```python
# åˆ›å»ºA/Bæµ‹è¯•
test_config = ab_test_manager.create_ab_test(
    test_id="temperature_test",
    variants={"low_temp": config_a, "high_temp": config_b},
    traffic_split={"low_temp": 0.5, "high_temp": 0.5}
)

# å¯åŠ¨æµ‹è¯•
ab_test_manager.start_ab_test("temperature_test")

# åˆ†æç»“æœ
analysis = ab_test_manager.analyze_ab_test("temperature_test")
```

#### 4.4 æ€§èƒ½ç›‘æ§
```python
from evaluation_engine.core.advanced_model_config import PerformanceMonitor

monitor = PerformanceMonitor()
monitor.auto_scaling_enabled = True
monitor.start_monitoring()

# è®°å½•æ€§èƒ½æ•°æ®
monitor.record_performance("claude-3-haiku", 3.5, True, 0.05, 0.85)

# è·å–æ‰©å±•å»ºè®®
recommendations = monitor.get_scaling_recommendations("claude-3-haiku")
```

### ç¬¬äº”æ­¥ï¼šå®Œæ•´å·¥ä½œæµç¨‹

#### 5.1 è¿è¡Œå®Œæ•´å·¥ä½œæµç¨‹
```bash
# è¿è¡Œå®Œæ•´å·¥ä½œæµç¨‹ç¤ºä¾‹
python evaluation_engine/docs/complete_workflow_example.py
```

#### 5.2 è‡ªå®šä¹‰è¯„ä¼°æµç¨‹
```python
# åˆ›å»ºå·¥ä½œæµç¨‹ç®¡ç†å™¨
workflow_manager = CompleteWorkflowManager()

# åˆå§‹åŒ–ç¯å¢ƒ
workflow_manager.initialize_environment()

# è®¾ç½®æ¨¡å‹é…ç½®
configurations = workflow_manager.setup_model_configurations()

# å®šä¹‰è¯„ä¼°è®¡åˆ’
plan = workflow_manager.define_evaluation_plan()

# è¿è¡Œå…¨é¢è¯„ä¼°
evaluation_summary = await workflow_manager.run_comprehensive_evaluation()

# ç”Ÿæˆç»¼åˆæŠ¥å‘Š
report = workflow_manager.generate_comprehensive_report(evaluation_summary)
```

#### 5.3 æ‰¹é‡æ¨¡å‹æ¯”è¾ƒ
```python
# å®šä¹‰è¦æ¯”è¾ƒçš„æ¨¡å‹
models_to_test = [
    ("claude-local", "model=claude-3-haiku-20240307"),
    ("openai-completions", "model=gpt-3.5-turbo"),
    ("deepseek", "model=deepseek-coder")
]

# å¹¶è¡Œæ‰§è¡Œè¯„ä¼°
results = await batch_evaluation(models_to_test)

# ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š
comparison_report = generate_comparison_report(results)
```

## ğŸ“Š å®é™…è¾“å‡ºç¤ºä¾‹

### è¯„ä¼°ç»“æœæ–‡ä»¶
```json
{
  "evaluation_id": "eval_1234567890_abcd1234",
  "status": "completed",
  "results": {
    "single_turn_scenarios_function_generation": {
      "accuracy": 0.85,
      "completeness": 0.90,
      "code_quality": 0.88
    }
  },
  "metrics_summary": {
    "overall_score": 0.88,
    "execution_time": 45.2
  },
  "analysis": {
    "summary": "Evaluation completed successfully",
    "recommendations": ["Consider fine-tuning for better accuracy"]
  }
}
```

### æ€§èƒ½ç›‘æ§æŠ¥å‘Š
```json
{
  "model_id": "claude-3-haiku",
  "performance_summary": {
    "response_time_avg": 3.2,
    "success_rate": 0.95,
    "quality_score": 0.88,
    "total_requests": 100
  },
  "scaling_recommendations": [
    {
      "type": "optimize_config",
      "reason": "Good performance, consider testing more challenging scenarios"
    }
  ]
}
```

### A/Bæµ‹è¯•ç»“æœ
```json
{
  "test_id": "temperature_optimization_001",
  "winner": "low_temp",
  "significant": true,
  "confidence": 0.95,
  "variants": {
    "low_temp": {
      "sample_size": 50,
      "success_rate": 0.94,
      "quality_score": 0.89,
      "performance_score": 0.91
    },
    "high_temp": {
      "sample_size": 50,
      "success_rate": 0.88,
      "quality_score": 0.82,
      "performance_score": 0.85
    }
  }
}
```

## ğŸ”§ é…ç½®æ–‡ä»¶ç®¡ç†

### æ¨¡å‹é…ç½®æ–‡ä»¶ (model_configs.yaml)
```yaml
models:
  claude-3-haiku:
    model_type: "anthropic_claude"
    api_key: "${ANTHROPIC_API_KEY}"
    temperature: 0.7
    max_tokens: 2048
    rate_limits:
      requests_per_minute: 60
      tokens_per_minute: 100000
    task_optimizations:
      code_completion:
        temperature: 0.2
        max_tokens: 512
      function_generation:
        temperature: 0.3
        max_tokens: 1024
```

### è¯„ä¼°é…ç½®æ–‡ä»¶ (evaluation_configs.yaml)
```yaml
evaluations:
  baseline_test:
    description: "Baseline performance evaluation"
    models: ["claude-3-haiku", "gpt-3.5-turbo"]
    tasks: ["single_turn_scenarios_function_generation"]
    configuration:
      limit: 10
      temperature: 0.7
      max_tokens: 1024
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. ç¯å¢ƒç®¡ç†
- å§‹ç»ˆä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ
- å®šæœŸæ›´æ–°ä¾èµ–åŒ…
- å¤‡ä»½é‡è¦çš„é…ç½®æ–‡ä»¶

### 2. APIå¯†é’¥ç®¡ç†
- ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜å‚¨APIå¯†é’¥
- å®šæœŸè½®æ¢APIå¯†é’¥
- ç›‘æ§APIä½¿ç”¨é‡å’Œæˆæœ¬

### 3. è¯„ä¼°ç­–ç•¥
- ä»å°è§„æ¨¡æµ‹è¯•å¼€å§‹
- é€æ­¥å¢åŠ è¯„ä¼°å¤æ‚åº¦
- å®šæœŸä¿å­˜å’Œå¤‡ä»½ç»“æœ

### 4. æ€§èƒ½ä¼˜åŒ–
- å¯ç”¨æ€§èƒ½ç›‘æ§
- æ ¹æ®ç›‘æ§ç»“æœè°ƒæ•´é…ç½®
- ä½¿ç”¨A/Bæµ‹è¯•ä¼˜åŒ–å‚æ•°

### 5. ç»“æœåˆ†æ
- ä¿å­˜è¯¦ç»†çš„è¯„ä¼°æ—¥å¿—
- å®šæœŸç”Ÿæˆç»¼åˆæŠ¥å‘Š
- è·Ÿè¸ªæ€§èƒ½è¶‹åŠ¿å˜åŒ–

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### 1. å®‰è£…é—®é¢˜
```bash
# Pythonç‰ˆæœ¬é—®é¢˜
python --version  # ç¡®ä¿3.9+

# ä¾èµ–å®‰è£…é—®é¢˜
pip install --upgrade pip
pip install -e ".[dev,api,testing,evaluation_engine]"
```

#### 2. APIè°ƒç”¨é—®é¢˜
```bash
# æ£€æŸ¥APIå¯†é’¥
echo $ANTHROPIC_API_KEY

# æ£€æŸ¥ç½‘ç»œè¿æ¥
curl -I https://api.anthropic.com

# æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
curl http://localhost:8000/health
```

#### 3. è¯„ä¼°æ‰§è¡Œé—®é¢˜
```bash
# æ£€æŸ¥ä»»åŠ¡æ³¨å†Œ
python -m lm_eval --tasks list | grep single_turn_scenarios

# æµ‹è¯•åŸºç¡€åŠŸèƒ½
python -m lm_eval --model dummy --tasks single_turn_scenarios_function_generation --limit 1 --predict_only --output_path results/test
```

#### 4. æ€§èƒ½é—®é¢˜
```bash
# æ£€æŸ¥ç³»ç»Ÿèµ„æº
top
df -h

# è°ƒæ•´å¹¶å‘è®¾ç½®
# åœ¨é…ç½®ä¸­å‡å°‘concurrent_requests
```

## ğŸ“ è·å–å¸®åŠ©

### æ–‡æ¡£èµ„æº
- `user_menu.md` - å®Œæ•´å‘½ä»¤å‚è€ƒ
- `api_usage_guide.md` - APIè¯¦ç»†ä½¿ç”¨æŒ‡å—
- `INSTALLATION_GUIDE.md` - å®‰è£…é—®é¢˜è§£å†³
- `evaluation_engine/tests/README.md` - æµ‹è¯•å¥—ä»¶è¯´æ˜

### åœ¨çº¿å¸®åŠ©
```bash
# lm-evalå¸®åŠ©
python -m lm_eval --help

# æŸ¥çœ‹å¯ç”¨ä»»åŠ¡
python -m lm_eval --tasks list

# APIæ–‡æ¡£
# è®¿é—® http://localhost:8000/docs
```

### æ—¥å¿—å’Œè°ƒè¯•
```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
python -m lm_eval --verbosity DEBUG

# æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶
tail -f logs/evaluation.log

# æ€§èƒ½åˆ†æ
python -m cProfile your_script.py
```

## ğŸ‰ æ€»ç»“

AI Evaluation Engineæä¾›äº†å®Œæ•´çš„AIæ¨¡å‹è¯„ä¼°è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ï¼š

1. **ä¸€é”®å®‰è£…** - å¿«é€Ÿè®¾ç½®å®Œæ•´ç¯å¢ƒ
2. **åŸºç¡€è¯„ä¼°** - ç®€å•æ˜“ç”¨çš„å‘½ä»¤è¡Œæ¥å£
3. **APIè°ƒç”¨** - å¼ºå¤§çš„REST APIå’ŒWebSocketæ”¯æŒ
4. **é«˜çº§é…ç½®** - çµæ´»çš„æ¨¡å‹é…ç½®å’ŒA/Bæµ‹è¯•
5. **å®Œæ•´å·¥ä½œæµç¨‹** - ç«¯åˆ°ç«¯çš„è¯„ä¼°å’Œåˆ†ææµç¨‹
6. **æ€§èƒ½ç›‘æ§** - å®æ—¶æ€§èƒ½ç›‘æ§å’Œè‡ªåŠ¨ä¼˜åŒ–
7. **ç»“æœåˆ†æ** - è¯¦ç»†çš„åˆ†ææŠ¥å‘Šå’Œå¯è§†åŒ–

æ‰€æœ‰åŠŸèƒ½éƒ½ç»è¿‡å®é™…æµ‹è¯•éªŒè¯ï¼Œç¡®ä¿å¯ä»¥æ­£å¸¸è¿è¡Œå¹¶äº§ç”Ÿæœ‰æ•ˆçš„åˆ†ææŠ¥å‘Šã€‚å»ºè®®æŒ‰ç…§æœ¬æŒ‡å—çš„æ­¥éª¤é€æ­¥å­¦ä¹ å’Œä½¿ç”¨ï¼Œä»åŸºç¡€åŠŸèƒ½å¼€å§‹ï¼Œé€æ­¥æŒæ¡é«˜çº§åŠŸèƒ½ã€‚

---

**å¼€å§‹ä½¿ç”¨**: `bash evaluation_engine/docs/quick_setup.sh`