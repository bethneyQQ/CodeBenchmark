#!/usr/bin/env python3
"""
AI Evaluation Engine é«˜çº§é…ç½®ç¤ºä¾‹

è¿™ä¸ªæ–‡ä»¶åŒ…å«äº†å„ç§é«˜çº§é…ç½®çš„å®Œæ•´ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ï¼š
1. é…ç½®ä¸åŒç±»å‹çš„æ¨¡å‹
2. è®¾ç½®ä»»åŠ¡ç‰¹å®šçš„ä¼˜åŒ–å‚æ•°
3. å®ç°A/Bæµ‹è¯•
4. é…ç½®æ€§èƒ½ç›‘æ§å’Œè‡ªåŠ¨æ‰©å±•
5. æ‰¹é‡è¯„ä¼°å’Œæ¯”è¾ƒåˆ†æ
"""

import os
import json
import asyncio
from datetime import datetime
from typing import Dict, List, Any

# å¯¼å…¥evaluation engineç»„ä»¶
from evaluation_engine.core.unified_framework import (
    UnifiedEvaluationFramework, 
    EvaluationRequest,
    EvaluationMode,
    BusinessScenario
)
from evaluation_engine.core.advanced_model_config import (
    AdvancedModelConfigurationManager,
    ModelConfiguration,
    TaskType,
    OptimizationStrategy,
    RateLimitConfig,
    ABTestManager,
    PerformanceMonitor
)
from evaluation_engine.core.model_adapters import ModelType


class AdvancedConfigurationExamples:
    """é«˜çº§é…ç½®ç¤ºä¾‹ç±»"""
    
    def __init__(self):
        self.framework = UnifiedEvaluationFramework()
        self.config_manager = AdvancedModelConfigurationManager()
        self.ab_test_manager = ABTestManager()
        self.performance_monitor = PerformanceMonitor()
        
    def setup_claude_configurations(self) -> Dict[str, ModelConfiguration]:
        """è®¾ç½®Claudeæ¨¡å‹çš„å„ç§é…ç½®"""
        
        configurations = {}
        
        # 1. åŸºç¡€Claudeé…ç½®
        base_claude_config = ModelConfiguration(
            model_id="claude-3-haiku",
            model_type=ModelType.ANTHROPIC_CLAUDE,
            
            # ç”Ÿæˆå‚æ•°
            temperature=0.7,
            max_tokens=2048,
            top_p=0.9,
            frequency_penalty=0.0,
            presence_penalty=0.0,
            stop_sequences=["```", "\n\n"],
            
            # APIé…ç½®
            api_key=os.getenv("ANTHROPIC_API_KEY"),
            timeout=30.0,
            
            # é€Ÿç‡é™åˆ¶
            rate_limit_config=RateLimitConfig(
                requests_per_minute=60,
                tokens_per_minute=100000,
                concurrent_requests=5,
                retry_attempts=3,
                backoff_factor=2.0
            ),
            
            # æˆæœ¬ç®¡ç†
            max_cost_per_request=1.0,
            daily_budget=100.0,
            
            # æ€§èƒ½ç›®æ ‡
            target_response_time=5.0,
            target_success_rate=0.95
        )
        
        # 2. ä»£ç è¡¥å…¨ä¼˜åŒ–é…ç½®
        code_completion_config = ModelConfiguration(**base_claude_config.to_dict())
        code_completion_config.model_id = "claude-3-haiku-code-completion"
        code_completion_config.temperature = 0.2
        code_completion_config.max_tokens = 512
        code_completion_config.stop_sequences = ["\n\n", "```", "def ", "class "]
        code_completion_config.task_optimizations = {
            TaskType.CODE_COMPLETION: {
                "temperature": 0.2,
                "max_tokens": 512,
                "top_p": 0.8,
                "stop_sequences": ["\n\n", "```"]
            }
        }
        
        # 3. å‡½æ•°ç”Ÿæˆä¼˜åŒ–é…ç½®
        function_generation_config = ModelConfiguration(**base_claude_config.to_dict())
        function_generation_config.model_id = "claude-3-haiku-function-gen"
        function_generation_config.temperature = 0.3
        function_generation_config.max_tokens = 1024
        function_generation_config.task_optimizations = {
            TaskType.FUNCTION_GENERATION: {
                "temperature": 0.3,
                "max_tokens": 1024,
                "top_p": 0.9,
                "stop_sequences": ["\n\ndef ", "\n\nclass "]
            }
        }
        
        # 4. Bugä¿®å¤ä¼˜åŒ–é…ç½®
        bug_fix_config = ModelConfiguration(**base_claude_config.to_dict())
        bug_fix_config.model_id = "claude-3-haiku-bug-fix"
        bug_fix_config.temperature = 0.1
        bug_fix_config.max_tokens = 1024
        bug_fix_config.top_p = 0.8
        bug_fix_config.task_optimizations = {
            TaskType.BUG_FIX: {
                "temperature": 0.1,
                "max_tokens": 1024,
                "top_p": 0.8,
                "stop_sequences": ["```"]
            }
        }
        
        # 5. é«˜æ€§èƒ½é…ç½®ï¼ˆæ›´å¿«å“åº”ï¼‰
        high_performance_config = ModelConfiguration(**base_claude_config.to_dict())
        high_performance_config.model_id = "claude-3-haiku-high-perf"
        high_performance_config.max_tokens = 512
        high_performance_config.target_response_time = 3.0
        high_performance_config.rate_limit_config.concurrent_requests = 10
        
        # 6. æˆæœ¬ä¼˜åŒ–é…ç½®
        cost_optimized_config = ModelConfiguration(**base_claude_config.to_dict())
        cost_optimized_config.model_id = "claude-3-haiku-cost-opt"
        cost_optimized_config.max_tokens = 256
        cost_optimized_config.max_cost_per_request = 0.5
        cost_optimized_config.daily_budget = 50.0
        
        configurations.update({
            "base": base_claude_config,
            "code_completion": code_completion_config,
            "function_generation": function_generation_config,
            "bug_fix": bug_fix_config,
            "high_performance": high_performance_config,
            "cost_optimized": cost_optimized_config
        })
        
        return configurations
    
    def setup_multi_model_configurations(self) -> Dict[str, ModelConfiguration]:
        """è®¾ç½®å¤šç§æ¨¡å‹çš„é…ç½®"""
        
        configurations = {}
        
        # Claudeé…ç½®
        claude_config = ModelConfiguration(
            model_id="claude-3-haiku",
            model_type=ModelType.ANTHROPIC_CLAUDE,
            temperature=0.7,
            max_tokens=2048,
            api_key=os.getenv("ANTHROPIC_API_KEY"),
            rate_limit_config=RateLimitConfig(
                requests_per_minute=60,
                tokens_per_minute=100000
            )
        )
        
        # OpenAIé…ç½®
        openai_config = ModelConfiguration(
            model_id="gpt-3.5-turbo",
            model_type=ModelType.OPENAI_GPT,
            temperature=0.7,
            max_tokens=2048,
            api_key=os.getenv("OPENAI_API_KEY"),
            rate_limit_config=RateLimitConfig(
                requests_per_minute=60,
                tokens_per_minute=90000
            ),
            max_cost_per_request=0.5
        )
        
        # DeepSeeké…ç½®
        deepseek_config = ModelConfiguration(
            model_id="deepseek-coder",
            model_type=ModelType.DEEPSEEK,
            temperature=0.7,
            max_tokens=2048,
            api_key=os.getenv("DEEPSEEK_API_KEY"),
            rate_limit_config=RateLimitConfig(
                requests_per_minute=100,
                tokens_per_minute=200000
            ),
            max_cost_per_request=0.1
        )
        
        # é€šä¹‰åƒé—®é…ç½®
        qwen_config = ModelConfiguration(
            model_id="qwen-plus",
            model_type=ModelType.DASHSCOPE,
            temperature=0.7,
            max_tokens=2048,
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            rate_limit_config=RateLimitConfig(
                requests_per_minute=60,
                tokens_per_minute=120000
            ),
            max_cost_per_request=0.3
        )
        
        configurations.update({
            "claude": claude_config,
            "openai": openai_config,
            "deepseek": deepseek_config,
            "qwen": qwen_config
        })
        
        return configurations
    
    def setup_ab_testing_example(self):
        """è®¾ç½®A/Bæµ‹è¯•ç¤ºä¾‹"""
        
        print("ğŸ§ª è®¾ç½®A/Bæµ‹è¯•ç¤ºä¾‹")
        
        # åˆ›å»ºä¸åŒæ¸©åº¦è®¾ç½®çš„å˜ä½“
        low_temp_config = ModelConfiguration(
            model_id="claude-3-haiku",
            model_type=ModelType.ANTHROPIC_CLAUDE,
            temperature=0.2,
            max_tokens=1024,
            api_key=os.getenv("ANTHROPIC_API_KEY")
        )
        
        medium_temp_config = ModelConfiguration(
            model_id="claude-3-haiku",
            model_type=ModelType.ANTHROPIC_CLAUDE,
            temperature=0.5,
            max_tokens=1024,
            api_key=os.getenv("ANTHROPIC_API_KEY")
        )
        
        high_temp_config = ModelConfiguration(
            model_id="claude-3-haiku",
            model_type=ModelType.ANTHROPIC_CLAUDE,
            temperature=0.8,
            max_tokens=1024,
            api_key=os.getenv("ANTHROPIC_API_KEY")
        )
        
        # åˆ›å»ºA/Bæµ‹è¯•
        test_config = self.ab_test_manager.create_ab_test(
            test_id="temperature_optimization_001",
            description="ä¼˜åŒ–ä»£ç è¡¥å…¨ä»»åŠ¡çš„æ¸©åº¦å‚æ•°",
            variants={
                "low_temp": low_temp_config,
                "medium_temp": medium_temp_config,
                "high_temp": high_temp_config
            },
            traffic_split={
                "low_temp": 0.33,
                "medium_temp": 0.33,
                "high_temp": 0.34
            },
            success_metric="quality_score",
            minimum_samples=30,  # é™ä½æ ·æœ¬è¦æ±‚ä»¥ä¾¿æ¼”ç¤º
            confidence_level=0.95,
            max_duration_hours=2  # 2å°æ—¶æµ‹è¯•
        )
        
        # å¯åŠ¨æµ‹è¯•
        self.ab_test_manager.start_ab_test("temperature_optimization_001")
        print("âœ… A/Bæµ‹è¯•å·²å¯åŠ¨")
        
        return test_config
    
    def run_ab_test_simulation(self, test_id: str, num_samples: int = 30):
        """æ¨¡æ‹Ÿè¿è¡ŒA/Bæµ‹è¯•"""
        
        print(f"ğŸ”„ æ¨¡æ‹Ÿè¿è¡ŒA/Bæµ‹è¯•: {test_id}")
        
        import random
        import time
        
        for i in range(num_samples):
            # é€‰æ‹©å˜ä½“
            variant_name, config = self.ab_test_manager.select_variant(test_id)
            
            # æ¨¡æ‹Ÿè¯„ä¼°ç»“æœ
            # ä½æ¸©åº¦é€šå¸¸æœ‰æ›´å¥½çš„ä»£ç è´¨é‡ä½†å¯èƒ½ç¼ºä¹åˆ›é€ æ€§
            if variant_name == "low_temp":
                quality = random.uniform(0.8, 0.95)
                response_time = random.uniform(2.0, 4.0)
                success = random.random() > 0.05
            elif variant_name == "medium_temp":
                quality = random.uniform(0.75, 0.90)
                response_time = random.uniform(2.5, 5.0)
                success = random.random() > 0.08
            else:  # high_temp
                quality = random.uniform(0.65, 0.85)
                response_time = random.uniform(3.0, 6.0)
                success = random.random() > 0.12
            
            cost = random.uniform(0.01, 0.05)
            
            # è®°å½•ç»“æœ
            self.ab_test_manager.record_test_result(
                test_id=test_id,
                variant_name=variant_name,
                response_time=response_time,
                success=success,
                cost=cost,
                quality=quality
            )
            
            if (i + 1) % 10 == 0:
                print(f"  å·²å®Œæˆ {i + 1}/{num_samples} ä¸ªæ ·æœ¬")
        
        print("âœ… A/Bæµ‹è¯•æ¨¡æ‹Ÿå®Œæˆ")
    
    def analyze_ab_test_results(self, test_id: str):
        """åˆ†æA/Bæµ‹è¯•ç»“æœ"""
        
        print(f"ğŸ“Š åˆ†æA/Bæµ‹è¯•ç»“æœ: {test_id}")
        
        analysis = self.ab_test_manager.analyze_ab_test(test_id)
        
        print(f"\næµ‹è¯•æè¿°: {analysis['description']}")
        print(f"æµ‹è¯•çŠ¶æ€: {'è¿›è¡Œä¸­' if analysis['is_active'] else 'å·²å®Œæˆ'}")
        print(f"è·èƒœå˜ä½“: {analysis['winner']}")
        print(f"ç»Ÿè®¡æ˜¾è‘—æ€§: {'æ˜¯' if analysis['significant'] else 'å¦'}")
        print(f"ç½®ä¿¡åº¦: {analysis['confidence']:.1%}")
        
        print("\nå„å˜ä½“è¯¦ç»†ç»“æœ:")
        print("-" * 60)
        
        for variant_name, metrics in analysis['variants'].items():
            print(f"\n{variant_name}:")
            print(f"  æ ·æœ¬æ•°é‡: {metrics['sample_size']}")
            print(f"  å¹³å‡å“åº”æ—¶é—´: {metrics['response_time_avg']:.2f}s")
            print(f"  æˆåŠŸç‡: {metrics['success_rate']:.1%}")
            print(f"  å¹³å‡æˆæœ¬: ${metrics['cost_per_request']:.4f}")
            print(f"  è´¨é‡åˆ†æ•°: {metrics['quality_score']:.3f}")
            print(f"  ç»¼åˆæ€§èƒ½åˆ†æ•°: {metrics['performance_score']:.3f}")
        
        # è·å–æœ€ä½³é…ç½®
        best_config = self.ab_test_manager.get_best_configuration(test_id)
        if best_config:
            print(f"\nğŸ† æœ€ä½³é…ç½®:")
            print(f"  æ¸©åº¦: {best_config.temperature}")
            print(f"  æœ€å¤§ä»¤ç‰Œæ•°: {best_config.max_tokens}")
        
        return analysis
    
    def setup_performance_monitoring(self):
        """è®¾ç½®æ€§èƒ½ç›‘æ§"""
        
        print("ğŸ“ˆ è®¾ç½®æ€§èƒ½ç›‘æ§")
        
        # é…ç½®ç›‘æ§é˜ˆå€¼
        self.performance_monitor.scaling_thresholds = {
            'response_time_high': 8.0,  # 8ç§’å“åº”æ—¶é—´é˜ˆå€¼
            'error_rate_high': 0.05,    # 5%é”™è¯¯ç‡é˜ˆå€¼
            'success_rate_low': 0.9     # 90%æˆåŠŸç‡é˜ˆå€¼
        }
        
        # å¯ç”¨è‡ªåŠ¨æ‰©å±•
        self.performance_monitor.auto_scaling_enabled = True
        
        # å¼€å§‹ç›‘æ§
        self.performance_monitor.start_monitoring()
        
        print("âœ… æ€§èƒ½ç›‘æ§å·²å¯åŠ¨")
    
    def simulate_performance_data(self, model_id: str, num_requests: int = 50):
        """æ¨¡æ‹Ÿæ€§èƒ½æ•°æ®"""
        
        print(f"ğŸ“Š ä¸ºæ¨¡å‹ {model_id} æ¨¡æ‹Ÿ {num_requests} ä¸ªè¯·æ±‚çš„æ€§èƒ½æ•°æ®")
        
        import random
        import time
        
        for i in range(num_requests):
            # æ¨¡æ‹Ÿä¸åŒçš„æ€§èƒ½åœºæ™¯
            if i < 20:  # å‰20ä¸ªè¯·æ±‚è¡¨ç°è‰¯å¥½
                response_time = random.uniform(2.0, 4.0)
                success = random.random() > 0.02
                cost = random.uniform(0.01, 0.03)
                quality = random.uniform(0.8, 0.95)
            elif i < 35:  # ä¸­é—´15ä¸ªè¯·æ±‚æ€§èƒ½ä¸‹é™
                response_time = random.uniform(5.0, 8.0)
                success = random.random() > 0.08
                cost = random.uniform(0.02, 0.05)
                quality = random.uniform(0.7, 0.85)
            else:  # æœ€å15ä¸ªè¯·æ±‚æ¢å¤æ­£å¸¸
                response_time = random.uniform(3.0, 5.0)
                success = random.random() > 0.03
                cost = random.uniform(0.015, 0.035)
                quality = random.uniform(0.75, 0.9)
            
            # è®°å½•æ€§èƒ½æ•°æ®
            self.performance_monitor.record_performance(
                model_id=model_id,
                response_time=response_time,
                success=success,
                cost=cost,
                quality=quality
            )
            
            if (i + 1) % 10 == 0:
                print(f"  å·²è®°å½• {i + 1}/{num_requests} ä¸ªè¯·æ±‚")
        
        print("âœ… æ€§èƒ½æ•°æ®æ¨¡æ‹Ÿå®Œæˆ")
    
    def analyze_performance_data(self, model_id: str):
        """åˆ†ææ€§èƒ½æ•°æ®"""
        
        print(f"ğŸ“ˆ åˆ†ææ¨¡å‹ {model_id} çš„æ€§èƒ½æ•°æ®")
        
        # è·å–æ€§èƒ½æ‘˜è¦
        summary = self.performance_monitor.get_performance_summary(model_id)
        
        print(f"\næ€§èƒ½æ‘˜è¦:")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {summary['response_time_avg']:.2f}s")
        print(f"  95%å“åº”æ—¶é—´: {summary['response_time_p95']:.2f}s")
        print(f"  æˆåŠŸç‡: {summary['success_rate']:.1%}")
        print(f"  é”™è¯¯ç‡: {summary['error_rate']:.1%}")
        print(f"  å¹³å‡æˆæœ¬: ${summary['cost_per_request']:.4f}")
        print(f"  è´¨é‡åˆ†æ•°: {summary['quality_score']:.3f}")
        print(f"  æ€»è¯·æ±‚æ•°: {summary['total_requests']}")
        
        # è·å–æ‰©å±•å»ºè®®
        recommendations = self.performance_monitor.get_scaling_recommendations(model_id)
        
        if recommendations:
            print(f"\nğŸ”§ æ‰©å±•å»ºè®®:")
            for rec in recommendations:
                print(f"  â€¢ {rec['reason']}: {rec['action']}")
                print(f"    å½“å‰å€¼: {rec['current_value']:.3f}, é˜ˆå€¼: {rec['threshold']:.3f}")
        else:
            print(f"\nâœ… æ€§èƒ½è‰¯å¥½ï¼Œæ— éœ€æ‰©å±•è°ƒæ•´")
        
        return summary, recommendations
    
    async def run_batch_evaluation_example(self):
        """è¿è¡Œæ‰¹é‡è¯„ä¼°ç¤ºä¾‹"""
        
        print("ğŸš€ è¿è¡Œæ‰¹é‡è¯„ä¼°ç¤ºä¾‹")
        
        # å®šä¹‰è¦æµ‹è¯•çš„æ¨¡å‹å’Œé…ç½®
        test_scenarios = [
            {
                "name": "claude_conservative",
                "model": "claude-local",
                "model_args": "model=claude-3-haiku-20240307",
                "config": {"temperature": 0.3, "max_gen_toks": 512}
            },
            {
                "name": "claude_balanced",
                "model": "claude-local", 
                "model_args": "model=claude-3-haiku-20240307",
                "config": {"temperature": 0.7, "max_gen_toks": 1024}
            },
            {
                "name": "claude_creative",
                "model": "claude-local",
                "model_args": "model=claude-3-haiku-20240307", 
                "config": {"temperature": 0.9, "max_gen_toks": 1024}
            }
        ]
        
        # å®šä¹‰æµ‹è¯•ä»»åŠ¡
        test_tasks = [
            "single_turn_scenarios_function_generation",
            "single_turn_scenarios_code_completion"
        ]
        
        results = {}
        
        # å¹¶è¡Œæ‰§è¡Œè¯„ä¼°
        from concurrent.futures import ThreadPoolExecutor
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = []
            
            for scenario in test_scenarios:
                # åˆ›å»ºè¯„ä¼°è¯·æ±‚
                request = EvaluationRequest(
                    model=scenario["model"],
                    tasks=test_tasks,
                    limit=3,  # é™åˆ¶æ ·æœ¬æ•°é‡
                    gen_kwargs=scenario["config"],
                    output_base_path=f"results/batch_{scenario['name']}",
                    log_samples=True,
                    verbosity="INFO"
                )
                
                # æäº¤è¯„ä¼°ä»»åŠ¡
                future = executor.submit(self.framework.evaluate, request)
                futures.append((scenario["name"], future))
            
            # æ”¶é›†ç»“æœ
            for scenario_name, future in futures:
                try:
                    result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                    results[scenario_name] = result
                    
                    if result.status.value == "completed":
                        exec_time = self.framework._calculate_execution_time(result)
                        avg_score = sum(result.metrics_summary.values()) / len(result.metrics_summary) if result.metrics_summary else 0
                        print(f"âœ… {scenario_name}: å¹³å‡åˆ†æ•° {avg_score:.3f}, æ—¶é—´ {exec_time:.1f}s")
                    else:
                        print(f"âŒ {scenario_name}: {result.error}")
                        
                except Exception as e:
                    print(f"ğŸ’¥ {scenario_name}: {e}")
        
        # ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š
        self.generate_comparison_report(results, test_scenarios)
        
        return results
    
    def generate_comparison_report(self, results: Dict, scenarios: List[Dict]):
        """ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š"""
        
        print("\nğŸ“Š ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š")
        print("=" * 60)
        
        # æ”¶é›†æ€§èƒ½æ•°æ®
        performance_data = []
        
        for scenario in scenarios:
            scenario_name = scenario["name"]
            if scenario_name in results:
                result = results[scenario_name]
                
                if result.status.value == "completed" and result.metrics_summary:
                    avg_score = sum(result.metrics_summary.values()) / len(result.metrics_summary)
                    exec_time = self.framework._calculate_execution_time(result)
                    
                    performance_data.append({
                        "name": scenario_name,
                        "config": scenario["config"],
                        "avg_score": avg_score,
                        "exec_time": exec_time,
                        "tasks_completed": len(result.request.tasks)
                    })
        
        # æ’åºå¹¶æ˜¾ç¤ºç»“æœ
        performance_data.sort(key=lambda x: x["avg_score"], reverse=True)
        
        print("ğŸ† æ€§èƒ½æ’å:")
        for i, data in enumerate(performance_data, 1):
            print(f"\n{i}. {data['name']}")
            print(f"   å¹³å‡åˆ†æ•°: {data['avg_score']:.3f}")
            print(f"   æ‰§è¡Œæ—¶é—´: {data['exec_time']:.1f}s")
            print(f"   é…ç½®: {data['config']}")
        
        # ç”Ÿæˆå»ºè®®
        if performance_data:
            best_performer = performance_data[0]
            fastest = min(performance_data, key=lambda x: x["exec_time"])
            
            print(f"\nğŸ’¡ å»ºè®®:")
            print(f"â€¢ æœ€ä½³æ€§èƒ½é…ç½®: {best_performer['name']} (åˆ†æ•°: {best_performer['avg_score']:.3f})")
            print(f"â€¢ æœ€å¿«æ‰§è¡Œé…ç½®: {fastest['name']} (æ—¶é—´: {fastest['exec_time']:.1f}s)")
            
            if best_performer != fastest:
                print(f"â€¢ å¦‚éœ€å¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦ï¼Œè€ƒè™‘è°ƒæ•´å‚æ•°ä»‹äºä¸¤è€…ä¹‹é—´")
    
    def save_configuration_templates(self):
        """ä¿å­˜é…ç½®æ¨¡æ¿"""
        
        print("ğŸ’¾ ä¿å­˜é…ç½®æ¨¡æ¿")
        
        # Claudeé…ç½®æ¨¡æ¿
        claude_templates = self.setup_claude_configurations()
        
        # å¤šæ¨¡å‹é…ç½®æ¨¡æ¿
        multi_model_templates = self.setup_multi_model_configurations()
        
        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        templates = {
            "claude_configurations": {k: v.to_dict() for k, v in claude_templates.items()},
            "multi_model_configurations": {k: v.to_dict() for k, v in multi_model_templates.items()},
            "metadata": {
                "created_at": datetime.now().isoformat(),
                "description": "AI Evaluation Engine configuration templates",
                "version": "1.0.0"
            }
        }
        
        with open("evaluation_engine/docs/config_templates.json", "w") as f:
            json.dump(templates, f, indent=2, default=str)
        
        print("âœ… é…ç½®æ¨¡æ¿å·²ä¿å­˜åˆ° evaluation_engine/docs/config_templates.json")


async def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    
    print("ğŸ¯ AI Evaluation Engine é«˜çº§é…ç½®ç¤ºä¾‹")
    print("=" * 60)
    
    # åˆ›å»ºç¤ºä¾‹å®ä¾‹
    examples = AdvancedConfigurationExamples()
    
    try:
        # 1. è®¾ç½®æ¨¡å‹é…ç½®
        print("\n1ï¸âƒ£ è®¾ç½®æ¨¡å‹é…ç½®")
        claude_configs = examples.setup_claude_configurations()
        multi_model_configs = examples.setup_multi_model_configurations()
        
        # æ³¨å†Œé…ç½®åˆ°ç®¡ç†å™¨
        for config_name, config in claude_configs.items():
            examples.config_manager.register_model_configuration(f"claude_{config_name}", config)
        
        for config_name, config in multi_model_configs.items():
            examples.config_manager.register_model_configuration(config_name, config)
        
        print(f"âœ… å·²æ³¨å†Œ {len(claude_configs) + len(multi_model_configs)} ä¸ªæ¨¡å‹é…ç½®")
        
        # 2. A/Bæµ‹è¯•ç¤ºä¾‹
        print("\n2ï¸âƒ£ A/Bæµ‹è¯•ç¤ºä¾‹")
        test_config = examples.setup_ab_testing_example()
        examples.run_ab_test_simulation("temperature_optimization_001", 30)
        analysis = examples.analyze_ab_test_results("temperature_optimization_001")
        
        # 3. æ€§èƒ½ç›‘æ§ç¤ºä¾‹
        print("\n3ï¸âƒ£ æ€§èƒ½ç›‘æ§ç¤ºä¾‹")
        examples.setup_performance_monitoring()
        examples.simulate_performance_data("claude-3-haiku", 50)
        summary, recommendations = examples.analyze_performance_data("claude-3-haiku")
        
        # 4. æ‰¹é‡è¯„ä¼°ç¤ºä¾‹ï¼ˆå¦‚æœæœ‰APIå¯†é’¥ï¼‰
        if os.getenv("ANTHROPIC_API_KEY"):
            print("\n4ï¸âƒ£ æ‰¹é‡è¯„ä¼°ç¤ºä¾‹")
            batch_results = await examples.run_batch_evaluation_example()
        else:
            print("\n4ï¸âƒ£ è·³è¿‡æ‰¹é‡è¯„ä¼°ç¤ºä¾‹ï¼ˆéœ€è¦ANTHROPIC_API_KEYï¼‰")
        
        # 5. ä¿å­˜é…ç½®æ¨¡æ¿
        print("\n5ï¸âƒ£ ä¿å­˜é…ç½®æ¨¡æ¿")
        examples.save_configuration_templates()
        
        print("\nğŸ‰ æ‰€æœ‰é«˜çº§é…ç½®ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ è¿è¡Œç¤ºä¾‹æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†èµ„æº
        examples.performance_monitor.stop_monitoring()


if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹
    asyncio.run(main())