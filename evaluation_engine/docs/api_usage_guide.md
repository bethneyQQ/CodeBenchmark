# AI Evaluation Engine API ä½¿ç”¨æŒ‡å—

## ğŸŒ API è°ƒç”¨æ–¹å¼

### 1. å¯åŠ¨APIæœåŠ¡å™¨

#### æ–¹å¼ä¸€ï¼šç›´æ¥å¯åŠ¨
```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# å¯åŠ¨APIæœåŠ¡å™¨
python -c "
from evaluation_engine.api.gateway import APIGateway
from evaluation_engine.core.unified_framework import UnifiedEvaluationFramework
from evaluation_engine.core.task_registration import TaskRegistry
from evaluation_engine.core.advanced_model_config import AdvancedModelConfigurationManager
from evaluation_engine.core.analysis_engine import AnalysisEngine

# åˆå§‹åŒ–ç»„ä»¶
framework = UnifiedEvaluationFramework()
task_registry = TaskRegistry()
model_config_manager = AdvancedModelConfigurationManager()
analysis_engine = AnalysisEngine()

# åˆ›å»ºAPIç½‘å…³
gateway = APIGateway(framework, task_registry, model_config_manager, analysis_engine)

# å¯åŠ¨æœåŠ¡å™¨
gateway.run(host='0.0.0.0', port=8000)
"
```

#### æ–¹å¼äºŒï¼šä½¿ç”¨é…ç½®æ–‡ä»¶å¯åŠ¨
```bash
# åˆ›å»ºå¯åŠ¨è„šæœ¬
cat > start_api_server.py << 'EOF'
#!/usr/bin/env python3
"""
AI Evaluation Engine API Server
"""

import os
import logging
from evaluation_engine.api.gateway import APIGateway
from evaluation_engine.core.unified_framework import UnifiedEvaluationFramework
from evaluation_engine.core.task_registration import TaskRegistry
from evaluation_engine.core.advanced_model_config import AdvancedModelConfigurationManager
from evaluation_engine.core.analysis_engine import AnalysisEngine

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    required_env_vars = ['ANTHROPIC_API_KEY', 'OPENAI_API_KEY', 'DEEPSEEK_API_KEY', 'DASHSCOPE_API_KEY']
    missing_vars = [var for var in required_env_vars if not os.getenv(var)]
    
    if missing_vars:
        logger.warning(f"Missing environment variables: {missing_vars}")
        logger.info("Some API functionality may be limited")
    
    try:
        # åˆå§‹åŒ–ç»„ä»¶
        logger.info("Initializing evaluation framework...")
        framework = UnifiedEvaluationFramework()
        
        logger.info("Initializing task registry...")
        task_registry = TaskRegistry()
        
        logger.info("Initializing model configuration manager...")
        model_config_manager = AdvancedModelConfigurationManager()
        
        logger.info("Initializing analysis engine...")
        analysis_engine = AnalysisEngine()
        
        # åˆ›å»ºAPIç½‘å…³
        logger.info("Creating API gateway...")
        gateway = APIGateway(framework, task_registry, model_config_manager, analysis_engine)
        
        # å¯åŠ¨æœåŠ¡å™¨
        logger.info("Starting API server on http://0.0.0.0:8000")
        logger.info("API documentation available at http://0.0.0.0:8000/docs")
        gateway.run(host='0.0.0.0', port=8000, reload=False)
        
    except Exception as e:
        logger.error(f"Failed to start API server: {e}")
        raise

if __name__ == "__main__":
    main()
EOF

# è¿è¡Œå¯åŠ¨è„šæœ¬
python start_api_server.py
```

### 2. API è®¤è¯

#### è·å–è®¿é—®ä»¤ç‰Œ
```bash
# ç™»å½•è·å–ä»¤ç‰Œ
curl -X POST "http://localhost:8000/auth/login" \
  -H "Content-Type: application/json" \
  -d '{
    "username": "admin",
    "password": "admin123"
  }'

# å“åº”ç¤ºä¾‹
{
  "access_token": "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9...",
  "token_type": "bearer",
  "expires_in": 3600,
  "refresh_token": "refresh_token_here",
  "user_info": {
    "user_id": "admin",
    "username": "admin",
    "roles": ["admin"],
    "permissions": ["evaluation:create", "analytics:read"]
  }
}
```

#### ä½¿ç”¨ä»¤ç‰Œè®¿é—®API
```bash
# è®¾ç½®ä»¤ç‰Œå˜é‡
export ACCESS_TOKEN="eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9..."

# ä½¿ç”¨ä»¤ç‰Œè®¿é—®API
curl -X GET "http://localhost:8000/health" \
  -H "Authorization: Bearer $ACCESS_TOKEN"
```

### 3. åŸºç¡€APIè°ƒç”¨

#### ç³»ç»Ÿå¥åº·æ£€æŸ¥
```bash
curl -X GET "http://localhost:8000/health"

# å“åº”
{
  "status": "healthy",
  "timestamp": "2024-01-01T12:00:00Z",
  "version": "1.0.0",
  "active_evaluations": 0
}
```

#### åˆ—å‡ºå¯ç”¨ä»»åŠ¡
```bash
curl -X GET "http://localhost:8000/tasks" \
  -H "Authorization: Bearer $ACCESS_TOKEN" \
  -G -d "limit=10" -d "category=single_turn"

# å“åº”
{
  "items": [
    {
      "task_id": "single_turn_scenarios_function_generation",
      "name": "Function Generation",
      "category": "single_turn",
      "difficulty": "intermediate",
      "description": "Generate Python functions based on specifications",
      "languages": ["python"],
      "tags": ["coding", "generation"],
      "estimated_duration": 30
    }
  ],
  "total": 18,
  "page": 1,
  "page_size": 10
}
```

#### åˆ—å‡ºå¯ç”¨æ¨¡å‹
```bash
curl -X GET "http://localhost:8000/models" \
  -H "Authorization: Bearer $ACCESS_TOKEN"

# å“åº”
[
  {
    "model_id": "claude-3-haiku",
    "name": "Claude 3 Haiku",
    "provider": "anthropic",
    "version": "20240307",
    "capabilities": ["text_generation", "code_completion"],
    "supported_tasks": ["single_turn_scenarios", "multi_turn_scenarios"],
    "rate_limits": {
      "requests_per_minute": 60,
      "tokens_per_minute": 100000
    },
    "cost_per_token": 0.00025
  }
]
```

### 4. åˆ›å»ºå’Œç®¡ç†è¯„ä¼°

#### åˆ›å»ºè¯„ä¼°ä»»åŠ¡
```bash
curl -X POST "http://localhost:8000/evaluations" \
  -H "Authorization: Bearer $ACCESS_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "model_id": "claude-3-haiku",
    "task_ids": [
      "single_turn_scenarios_function_generation",
      "single_turn_scenarios_code_completion"
    ],
    "configuration": {
      "temperature": 0.7,
      "max_tokens": 1024,
      "limit": 5
    },
    "context_mode": "full_context",
    "metadata": {
      "experiment_name": "baseline_test",
      "description": "Baseline performance test"
    }
  }'

# å“åº”
{
  "evaluation_id": "eval_1234567890_abcd1234",
  "status": "created",
  "message": "Evaluation created successfully",
  "created_at": "2024-01-01T12:00:00Z"
}
```

#### æŸ¥çœ‹è¯„ä¼°çŠ¶æ€
```bash
curl -X GET "http://localhost:8000/evaluations/eval_1234567890_abcd1234" \
  -H "Authorization: Bearer $ACCESS_TOKEN"

# å“åº”
{
  "evaluation_id": "eval_1234567890_abcd1234",
  "status": "running",
  "progress": 0.6,
  "current_task": "single_turn_scenarios_code_completion",
  "completed_tasks": 1,
  "total_tasks": 2,
  "start_time": "2024-01-01T12:00:00Z",
  "estimated_completion": "2024-01-01T12:05:00Z",
  "error_message": null
}
```

#### è·å–è¯„ä¼°ç»“æœ
```bash
curl -X GET "http://localhost:8000/results/eval_1234567890_abcd1234" \
  -H "Authorization: Bearer $ACCESS_TOKEN" \
  -G -d "include_details=true"

# å“åº”
{
  "evaluation_id": "eval_1234567890_abcd1234",
  "model_id": "claude-3-haiku",
  "task_results": [
    {
      "task_id": "single_turn_scenarios_function_generation",
      "status": "completed",
      "score": 0.85,
      "metrics": {
        "accuracy": 0.8,
        "completeness": 0.9,
        "code_quality": 0.85
      },
      "execution_time": 45.2,
      "output": "Generated function code...",
      "error_message": null
    }
  ],
  "summary_metrics": {
    "overall_score": 0.83,
    "average_execution_time": 42.5
  },
  "completed_at": "2024-01-01T12:04:30Z",
  "execution_time": 270.5
}
```

### 5. WebSocket å®æ—¶é€šä¿¡

#### JavaScript WebSocket å®¢æˆ·ç«¯
```javascript
// è¿æ¥WebSocket
const ws = new WebSocket('ws://localhost:8000/ws?token=' + ACCESS_TOKEN);

// ç›‘å¬æ¶ˆæ¯
ws.onmessage = function(event) {
    const data = JSON.parse(event.data);
    console.log('Received:', data);
    
    if (data.type === 'evaluation_progress') {
        updateProgressBar(data.data.progress);
    } else if (data.type === 'evaluation_completed') {
        showResults(data.data.evaluation_id);
    }
};

// è®¢é˜…è¯„ä¼°æ›´æ–°
ws.onopen = function() {
    ws.send(JSON.stringify({
        type: 'subscribe_evaluation',
        evaluation_id: 'eval_1234567890_abcd1234'
    }));
};

// é”™è¯¯å¤„ç†
ws.onerror = function(error) {
    console.error('WebSocket error:', error);
};
```

#### Python WebSocket å®¢æˆ·ç«¯
```python
import asyncio
import websockets
import json

async def websocket_client():
    uri = f"ws://localhost:8000/ws?token={ACCESS_TOKEN}"
    
    async with websockets.connect(uri) as websocket:
        # è®¢é˜…è¯„ä¼°æ›´æ–°
        await websocket.send(json.dumps({
            "type": "subscribe_evaluation",
            "evaluation_id": "eval_1234567890_abcd1234"
        }))
        
        # ç›‘å¬æ¶ˆæ¯
        async for message in websocket:
            data = json.loads(message)
            print(f"Received: {data}")
            
            if data["type"] == "evaluation_completed":
                print(f"Evaluation {data['data']['evaluation_id']} completed!")
                break

# è¿è¡Œå®¢æˆ·ç«¯
asyncio.run(websocket_client())
```

## âš™ï¸ é«˜çº§é…ç½®

### 1. æ¨¡å‹é…ç½®ç®¡ç†

#### åˆ›å»ºé«˜çº§æ¨¡å‹é…ç½®
```python
from evaluation_engine.core.advanced_model_config import (
    ModelConfiguration, 
    TaskType, 
    OptimizationStrategy,
    RateLimitConfig
)

# åˆ›å»ºåŸºç¡€é…ç½®
base_config = ModelConfiguration(
    model_id="claude-3-haiku",
    model_type=ModelType.ANTHROPIC_CLAUDE,
    
    # ç”Ÿæˆå‚æ•°
    temperature=0.7,
    max_tokens=2048,
    top_p=0.9,
    frequency_penalty=0.0,
    presence_penalty=0.0,
    stop_sequences=["```", "\n\n"],
    
    # APIé…ç½®
    api_key="your_api_key_here",
    timeout=30.0,
    
    # é€Ÿç‡é™åˆ¶
    rate_limit_config=RateLimitConfig(
        requests_per_minute=60,
        tokens_per_minute=100000,
        concurrent_requests=5
    ),
    
    # æˆæœ¬ç®¡ç†
    max_cost_per_request=1.0,
    daily_budget=100.0,
    
    # æ€§èƒ½ç›®æ ‡
    target_response_time=5.0,
    target_success_rate=0.95
)

# ä»»åŠ¡ç‰¹å®šä¼˜åŒ–
base_config.task_optimizations = {
    TaskType.CODE_COMPLETION: {
        "temperature": 0.2,
        "max_tokens": 512,
        "stop_sequences": ["\n\n", "```"]
    },
    TaskType.FUNCTION_GENERATION: {
        "temperature": 0.3,
        "max_tokens": 1024,
        "stop_sequences": ["\n\ndef ", "\n\nclass "]
    },
    TaskType.BUG_FIX: {
        "temperature": 0.1,
        "max_tokens": 1024,
        "top_p": 0.8
    }
}
```

#### æ³¨å†Œå’Œä½¿ç”¨é…ç½®
```python
from evaluation_engine.core.advanced_model_config import AdvancedModelConfigurationManager

# åˆ›å»ºé…ç½®ç®¡ç†å™¨
config_manager = AdvancedModelConfigurationManager()

# æ³¨å†Œé…ç½®
config_manager.register_model_configuration("claude-3-haiku", base_config)

# è·å–ä¼˜åŒ–é…ç½®
optimized_config = config_manager.get_optimized_configuration(
    model_id="claude-3-haiku",
    task_type=TaskType.CODE_COMPLETION,
    optimization_strategy=OptimizationStrategy.PERFORMANCE
)

print(f"Optimized temperature: {optimized_config.temperature}")
print(f"Optimized max_tokens: {optimized_config.max_tokens}")
```

### 2. A/B æµ‹è¯•é…ç½®

#### åˆ›å»ºA/Bæµ‹è¯•
```python
from evaluation_engine.core.advanced_model_config import ABTestManager

# åˆ›å»ºA/Bæµ‹è¯•ç®¡ç†å™¨
ab_test_manager = ABTestManager()

# å®šä¹‰æµ‹è¯•å˜ä½“
variant_a = ModelConfiguration(
    model_id="claude-3-haiku",
    temperature=0.3,
    max_tokens=1024
)

variant_b = ModelConfiguration(
    model_id="claude-3-haiku", 
    temperature=0.7,
    max_tokens=1024
)

# åˆ›å»ºA/Bæµ‹è¯•
test_config = ab_test_manager.create_ab_test(
    test_id="temperature_test_001",
    description="Test different temperature settings for code completion",
    variants={
        "low_temp": variant_a,
        "high_temp": variant_b
    },
    traffic_split={
        "low_temp": 0.5,
        "high_temp": 0.5
    },
    success_metric="quality_score",
    minimum_samples=100,
    confidence_level=0.95,
    max_duration_hours=24
)

# å¯åŠ¨æµ‹è¯•
ab_test_manager.start_ab_test("temperature_test_001")

# åœ¨è¯„ä¼°ä¸­ä½¿ç”¨A/Bæµ‹è¯•
variant_name, config = ab_test_manager.select_variant("temperature_test_001")
print(f"Selected variant: {variant_name}")
```

#### åˆ†æA/Bæµ‹è¯•ç»“æœ
```python
# åˆ†ææµ‹è¯•ç»“æœ
analysis = ab_test_manager.analyze_ab_test("temperature_test_001")

print(f"Test winner: {analysis['winner']}")
print(f"Statistical significance: {analysis['significant']}")
print(f"Confidence level: {analysis['confidence']}")

for variant, metrics in analysis['variants'].items():
    print(f"\n{variant}:")
    print(f"  Sample size: {metrics['sample_size']}")
    print(f"  Success rate: {metrics['success_rate']:.2%}")
    print(f"  Quality score: {metrics['quality_score']:.3f}")
    print(f"  Performance score: {metrics['performance_score']:.3f}")

# è·å–æœ€ä½³é…ç½®
best_config = ab_test_manager.get_best_configuration("temperature_test_001")
if best_config:
    print(f"Best configuration temperature: {best_config.temperature}")
```

### 3. æ€§èƒ½ç›‘æ§å’Œè‡ªåŠ¨æ‰©å±•

#### é…ç½®æ€§èƒ½ç›‘æ§
```python
from evaluation_engine.core.advanced_model_config import PerformanceMonitor

# åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
monitor = PerformanceMonitor()

# é…ç½®æ‰©å±•é˜ˆå€¼
monitor.scaling_thresholds = {
    'response_time_high': 8.0,  # 8ç§’å“åº”æ—¶é—´é˜ˆå€¼
    'error_rate_high': 0.05,    # 5%é”™è¯¯ç‡é˜ˆå€¼
    'success_rate_low': 0.9     # 90%æˆåŠŸç‡é˜ˆå€¼
}

# å¯ç”¨è‡ªåŠ¨æ‰©å±•
monitor.auto_scaling_enabled = True

# å¼€å§‹ç›‘æ§
monitor.start_monitoring()

# è®°å½•æ€§èƒ½æ•°æ®
monitor.record_performance(
    model_id="claude-3-haiku",
    response_time=3.5,
    success=True,
    cost=0.05,
    quality=0.85
)

# è·å–æ€§èƒ½æ‘˜è¦
summary = monitor.get_performance_summary("claude-3-haiku")
print(f"Average response time: {summary['response_time_avg']:.2f}s")
print(f"Success rate: {summary['success_rate']:.2%}")
print(f"Quality score: {summary['quality_score']:.3f}")

# è·å–æ‰©å±•å»ºè®®
recommendations = monitor.get_scaling_recommendations("claude-3-haiku")
for rec in recommendations:
    print(f"Recommendation: {rec['action']}")
    print(f"Reason: {rec['reason']}")
```

### 4. å®Œæ•´çš„è¯„ä¼°é…ç½®

#### åˆ›å»ºå®Œæ•´çš„è¯„ä¼°è¯·æ±‚
```python
from evaluation_engine.core.unified_framework import EvaluationRequest, UnifiedEvaluationFramework

# åˆ›å»ºè¯„ä¼°æ¡†æ¶
framework = UnifiedEvaluationFramework()

# åˆ›å»ºå®Œæ•´çš„è¯„ä¼°è¯·æ±‚
request = EvaluationRequest(
    model="claude-local",
    tasks=[
        "single_turn_scenarios_function_generation",
        "single_turn_scenarios_code_completion",
        "single_turn_scenarios_bug_fix"
    ],
    
    # åŸºç¡€å‚æ•°
    limit=10,
    num_fewshot=0,
    batch_size=1,
    device=None,
    
    # ç¼“å­˜é…ç½®
    use_cache=True,
    cache_requests=True,
    rewrite_requests_cache=False,
    delete_requests_cache=False,
    
    # è¾“å‡ºé…ç½®
    write_out=True,
    output_base_path="results/api_evaluation",
    log_samples=True,
    show_config=True,
    
    # ç”Ÿæˆå‚æ•°
    gen_kwargs={
        "temperature": 0.7,
        "max_gen_toks": 1024,
        "do_sample": False
    },
    
    # å…¶ä»–é…ç½®
    predict_only=False,
    verbosity="INFO",
    random_seed=42,
    numpy_random_seed=1234,
    torch_random_seed=1234,
    fewshot_random_seed=1234
)

# æ‰§è¡Œè¯„ä¼°
result = framework.evaluate(request)

print(f"Evaluation ID: {result.evaluation_id}")
print(f"Status: {result.status.value}")
print(f"Execution time: {framework._calculate_execution_time(result):.2f}s")

if result.status.value == "completed":
    print(f"Results: {result.results}")
    print(f"Analysis: {result.analysis}")
```

### 5. æ‰¹é‡è¯„ä¼°å’Œæ¯”è¾ƒ

#### æ‰¹é‡æ¨¡å‹æ¯”è¾ƒ
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def batch_evaluation():
    """æ‰¹é‡è¯„ä¼°å¤šä¸ªæ¨¡å‹"""
    
    models_to_test = [
        ("claude-local", "model=claude-3-haiku-20240307"),
        ("claude-local", "model=claude-3-5-sonnet-20241022"),
        ("openai-completions", "model=gpt-3.5-turbo"),
        ("deepseek", "model=deepseek-coder")
    ]
    
    tasks = [
        "single_turn_scenarios_function_generation",
        "single_turn_scenarios_code_completion"
    ]
    
    results = {}
    
    # å¹¶è¡Œæ‰§è¡Œè¯„ä¼°
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = []
        
        for model, model_args in models_to_test:
            request = EvaluationRequest(
                model=model,
                tasks=tasks,
                limit=5,
                gen_kwargs={"temperature": 0.7, "max_gen_toks": 1024},
                output_base_path=f"results/batch_{model.replace('-', '_')}"
            )
            
            future = executor.submit(framework.evaluate, request)
            futures.append((f"{model}_{model_args}", future))
        
        # æ”¶é›†ç»“æœ
        for model_name, future in futures:
            try:
                result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                results[model_name] = result
                print(f"âœ… {model_name}: {result.status.value}")
            except Exception as e:
                print(f"âŒ {model_name}: {e}")
    
    return results

# è¿è¡Œæ‰¹é‡è¯„ä¼°
batch_results = asyncio.run(batch_evaluation())

# æ¯”è¾ƒç»“æœ
print("\nğŸ“Š æ¨¡å‹æ¯”è¾ƒç»“æœ:")
print("-" * 60)

for model_name, result in batch_results.items():
    if result.status.value == "completed" and result.metrics_summary:
        avg_score = sum(result.metrics_summary.values()) / len(result.metrics_summary)
        exec_time = framework._calculate_execution_time(result)
        
        print(f"{model_name}:")
        print(f"  å¹³å‡åˆ†æ•°: {avg_score:.3f}")
        print(f"  æ‰§è¡Œæ—¶é—´: {exec_time:.1f}s")
        print(f"  ä»»åŠ¡æ•°é‡: {len(result.request.tasks)}")
        print()
```

## ğŸ”§ é…ç½®æ–‡ä»¶ç®¡ç†

### 1. åˆ›å»ºé…ç½®æ–‡ä»¶

#### æ¨¡å‹é…ç½®æ–‡ä»¶ (model_configs.yaml)
```yaml
models:
  claude-3-haiku:
    model_type: "anthropic_claude"
    api_key: "${ANTHROPIC_API_KEY}"
    temperature: 0.7
    max_tokens: 2048
    top_p: 0.9
    rate_limits:
      requests_per_minute: 60
      tokens_per_minute: 100000
      concurrent_requests: 5
    cost_management:
      max_cost_per_request: 1.0
      daily_budget: 100.0
    performance_targets:
      target_response_time: 5.0
      target_success_rate: 0.95
    task_optimizations:
      code_completion:
        temperature: 0.2
        max_tokens: 512
      function_generation:
        temperature: 0.3
        max_tokens: 1024
      bug_fix:
        temperature: 0.1
        max_tokens: 1024

  gpt-3.5-turbo:
    model_type: "openai_gpt"
    api_key: "${OPENAI_API_KEY}"
    temperature: 0.7
    max_tokens: 2048
    rate_limits:
      requests_per_minute: 60
      tokens_per_minute: 90000
    cost_management:
      max_cost_per_request: 0.5
      daily_budget: 50.0

  deepseek-coder:
    model_type: "deepseek"
    api_key: "${DEEPSEEK_API_KEY}"
    temperature: 0.7
    max_tokens: 2048
    rate_limits:
      requests_per_minute: 100
      tokens_per_minute: 200000
    cost_management:
      max_cost_per_request: 0.1
      daily_budget: 20.0
```

#### è¯„ä¼°é…ç½®æ–‡ä»¶ (evaluation_configs.yaml)
```yaml
evaluations:
  baseline_test:
    description: "Baseline performance evaluation"
    models:
      - "claude-3-haiku"
      - "gpt-3.5-turbo"
    tasks:
      - "single_turn_scenarios_function_generation"
      - "single_turn_scenarios_code_completion"
      - "single_turn_scenarios_bug_fix"
    configuration:
      limit: 10
      num_fewshot: 0
      batch_size: 1
      log_samples: true
      output_base_path: "results/baseline"
    
  performance_comparison:
    description: "Comprehensive performance comparison"
    models:
      - "claude-3-haiku"
      - "claude-3-5-sonnet"
      - "gpt-3.5-turbo"
      - "deepseek-coder"
    tasks:
      - "single_turn_scenarios_python"
    configuration:
      limit: 20
      temperature: 0.7
      max_tokens: 1024
      output_base_path: "results/comparison"

ab_tests:
  temperature_optimization:
    description: "Optimize temperature for code completion"
    base_model: "claude-3-haiku"
    variants:
      low_temp:
        temperature: 0.2
      medium_temp:
        temperature: 0.5
      high_temp:
        temperature: 0.8
    traffic_split:
      low_temp: 0.33
      medium_temp: 0.33
      high_temp: 0.34
    success_metric: "quality_score"
    minimum_samples: 50
    max_duration_hours: 12
```

### 2. ä½¿ç”¨é…ç½®æ–‡ä»¶

#### åŠ è½½å’Œä½¿ç”¨é…ç½®
```python
import yaml
import os
from evaluation_engine.core.advanced_model_config import ModelConfiguration

def load_model_configs(config_file: str) -> dict:
    """åŠ è½½æ¨¡å‹é…ç½®æ–‡ä»¶"""
    with open(config_file, 'r') as f:
        config_data = yaml.safe_load(f)
    
    # æ›¿æ¢ç¯å¢ƒå˜é‡
    def replace_env_vars(obj):
        if isinstance(obj, dict):
            return {k: replace_env_vars(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [replace_env_vars(item) for item in obj]
        elif isinstance(obj, str) and obj.startswith('${') and obj.endswith('}'):
            env_var = obj[2:-1]
            return os.getenv(env_var, obj)
        else:
            return obj
    
    return replace_env_vars(config_data)

# åŠ è½½é…ç½®
config_data = load_model_configs('model_configs.yaml')

# åˆ›å»ºæ¨¡å‹é…ç½®å¯¹è±¡
model_configs = {}
for model_id, config in config_data['models'].items():
    model_configs[model_id] = ModelConfiguration.from_dict({
        'model_id': model_id,
        **config
    })

# æ³¨å†Œé…ç½®
config_manager = AdvancedModelConfigurationManager()
for model_id, config in model_configs.items():
    config_manager.register_model_configuration(model_id, config)

print(f"Loaded {len(model_configs)} model configurations")
```

## ğŸ“Š å®Œæ•´çš„æ‰§è¡Œæµç¨‹

### 1. ç«¯åˆ°ç«¯è¯„ä¼°æµç¨‹

```python
#!/usr/bin/env python3
"""
å®Œæ•´çš„AIè¯„ä¼°æ‰§è¡Œæµç¨‹ç¤ºä¾‹
"""

import asyncio
import json
import yaml
from datetime import datetime
from pathlib import Path

from evaluation_engine.core.unified_framework import UnifiedEvaluationFramework, EvaluationRequest
from evaluation_engine.core.advanced_model_config import AdvancedModelConfigurationManager
from evaluation_engine.core.task_registration import TaskRegistry

async def complete_evaluation_workflow():
    """å®Œæ•´çš„è¯„ä¼°å·¥ä½œæµç¨‹"""
    
    print("ğŸš€ å¼€å§‹å®Œæ•´è¯„ä¼°æµç¨‹")
    print("=" * 60)
    
    # 1. åˆå§‹åŒ–ç»„ä»¶
    print("1ï¸âƒ£ åˆå§‹åŒ–è¯„ä¼°ç»„ä»¶...")
    framework = UnifiedEvaluationFramework()
    config_manager = AdvancedModelConfigurationManager()
    task_registry = TaskRegistry()
    
    # 2. åŠ è½½é…ç½®
    print("2ï¸âƒ£ åŠ è½½æ¨¡å‹é…ç½®...")
    # è¿™é‡Œå¯ä»¥åŠ è½½é…ç½®æ–‡ä»¶æˆ–ä½¿ç”¨é»˜è®¤é…ç½®
    
    # 3. å®šä¹‰è¯„ä¼°è®¡åˆ’
    evaluation_plan = {
        "experiment_name": "comprehensive_evaluation",
        "description": "Comprehensive AI model evaluation",
        "models": [
            {
                "name": "claude-3-haiku",
                "model": "claude-local",
                "model_args": "model=claude-3-haiku-20240307"
            },
            {
                "name": "gpt-3.5-turbo", 
                "model": "openai-completions",
                "model_args": "model=gpt-3.5-turbo"
            }
        ],
        "task_groups": [
            {
                "name": "basic_coding",
                "tasks": [
                    "single_turn_scenarios_function_generation",
                    "single_turn_scenarios_code_completion"
                ]
            },
            {
                "name": "advanced_coding",
                "tasks": [
                    "single_turn_scenarios_bug_fix",
                    "single_turn_scenarios_algorithm_implementation"
                ]
            }
        ],
        "configurations": [
            {
                "name": "conservative",
                "temperature": 0.3,
                "max_tokens": 1024
            },
            {
                "name": "balanced",
                "temperature": 0.7,
                "max_tokens": 1024
            }
        ]
    }
    
    # 4. æ‰§è¡Œè¯„ä¼°
    print("3ï¸âƒ£ æ‰§è¡Œè¯„ä¼°ä»»åŠ¡...")
    results = {}
    
    for model_info in evaluation_plan["models"]:
        model_name = model_info["name"]
        print(f"\nğŸ“Š è¯„ä¼°æ¨¡å‹: {model_name}")
        
        for task_group in evaluation_plan["task_groups"]:
            group_name = task_group["name"]
            print(f"  ğŸ“‹ ä»»åŠ¡ç»„: {group_name}")
            
            for config in evaluation_plan["configurations"]:
                config_name = config["name"]
                print(f"    âš™ï¸ é…ç½®: {config_name}")
                
                # åˆ›å»ºè¯„ä¼°è¯·æ±‚
                request = EvaluationRequest(
                    model=model_info["model"],
                    tasks=task_group["tasks"],
                    limit=3,  # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥åŠ å¿«æ¼”ç¤º
                    gen_kwargs={
                        "temperature": config["temperature"],
                        "max_gen_toks": config["max_tokens"]
                    },
                    output_base_path=f"results/{model_name}_{group_name}_{config_name}",
                    log_samples=True,
                    verbosity="INFO"
                )
                
                # æ‰§è¡Œè¯„ä¼°
                try:
                    result = framework.evaluate(request)
                    
                    # å­˜å‚¨ç»“æœ
                    result_key = f"{model_name}_{group_name}_{config_name}"
                    results[result_key] = result
                    
                    if result.status.value == "completed":
                        exec_time = framework._calculate_execution_time(result)
                        avg_score = sum(result.metrics_summary.values()) / len(result.metrics_summary) if result.metrics_summary else 0
                        print(f"      âœ… å®Œæˆ - å¹³å‡åˆ†æ•°: {avg_score:.3f}, æ—¶é—´: {exec_time:.1f}s")
                    else:
                        print(f"      âŒ å¤±è´¥: {result.error}")
                        
                except Exception as e:
                    print(f"      ğŸ’¥ å¼‚å¸¸: {e}")
    
    # 5. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    print("\n4ï¸âƒ£ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
    report = generate_comprehensive_report(results, evaluation_plan)
    
    # 6. ä¿å­˜æŠ¥å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"results/comprehensive_report_{timestamp}.json"
    
    Path("results").mkdir(exist_ok=True)
    with open(report_file, 'w') as f:
        json.dump(report, f, indent=2, default=str)
    
    print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    # 7. æ˜¾ç¤ºæ‘˜è¦
    print("\n5ï¸âƒ£ è¯„ä¼°æ‘˜è¦:")
    print("=" * 60)
    display_evaluation_summary(report)
    
    return results, report

def generate_comprehensive_report(results: dict, evaluation_plan: dict) -> dict:
    """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
    
    report = {
        "experiment_info": {
            "name": evaluation_plan["experiment_name"],
            "description": evaluation_plan["description"],
            "timestamp": datetime.now().isoformat(),
            "total_evaluations": len(results)
        },
        "model_performance": {},
        "task_analysis": {},
        "configuration_analysis": {},
        "recommendations": []
    }
    
    # åˆ†ææ¨¡å‹æ€§èƒ½
    model_scores = {}
    for result_key, result in results.items():
        if result.status.value == "completed" and result.metrics_summary:
            parts = result_key.split('_')
            model_name = parts[0]
            
            if model_name not in model_scores:
                model_scores[model_name] = []
            
            avg_score = sum(result.metrics_summary.values()) / len(result.metrics_summary)
            model_scores[model_name].append(avg_score)
    
    # è®¡ç®—æ¨¡å‹å¹³å‡æ€§èƒ½
    for model_name, scores in model_scores.items():
        report["model_performance"][model_name] = {
            "average_score": sum(scores) / len(scores),
            "best_score": max(scores),
            "worst_score": min(scores),
            "consistency": 1.0 - (max(scores) - min(scores)),  # ç®€å•çš„ä¸€è‡´æ€§æŒ‡æ ‡
            "total_evaluations": len(scores)
        }
    
    # ç”Ÿæˆå»ºè®®
    if model_scores:
        best_model = max(model_scores.keys(), 
                        key=lambda m: sum(model_scores[m]) / len(model_scores[m]))
        report["recommendations"].append(f"æœ€ä½³æ•´ä½“æ€§èƒ½æ¨¡å‹: {best_model}")
        
        for model_name, perf in report["model_performance"].items():
            if perf["consistency"] < 0.8:
                report["recommendations"].append(f"{model_name} æ€§èƒ½ä¸å¤Ÿç¨³å®šï¼Œå»ºè®®è°ƒä¼˜å‚æ•°")
    
    return report

def display_evaluation_summary(report: dict):
    """æ˜¾ç¤ºè¯„ä¼°æ‘˜è¦"""
    
    print(f"å®éªŒåç§°: {report['experiment_info']['name']}")
    print(f"æ€»è¯„ä¼°æ•°: {report['experiment_info']['total_evaluations']}")
    print()
    
    print("ğŸ“Š æ¨¡å‹æ€§èƒ½æ’å:")
    model_perf = report["model_performance"]
    sorted_models = sorted(model_perf.items(), 
                          key=lambda x: x[1]["average_score"], 
                          reverse=True)
    
    for i, (model_name, perf) in enumerate(sorted_models, 1):
        print(f"  {i}. {model_name}")
        print(f"     å¹³å‡åˆ†æ•°: {perf['average_score']:.3f}")
        print(f"     æœ€ä½³åˆ†æ•°: {perf['best_score']:.3f}")
        print(f"     ä¸€è‡´æ€§: {perf['consistency']:.3f}")
        print()
    
    print("ğŸ’¡ å»ºè®®:")
    for rec in report["recommendations"]:
        print(f"  â€¢ {rec}")

# è¿è¡Œå®Œæ•´æµç¨‹
if __name__ == "__main__":
    results, report = asyncio.run(complete_evaluation_workflow())
    print("\nğŸ‰ è¯„ä¼°æµç¨‹å®Œæˆï¼")
```

### 2. è¿è¡Œå®Œæ•´æµç¨‹

```bash
# 1. ç¡®ä¿ç¯å¢ƒå·²è®¾ç½®
source venv/bin/activate
export ANTHROPIC_API_KEY="your_key_here"
export OPENAI_API_KEY="your_key_here"

# 2. è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹
python complete_evaluation_workflow.py

# 3. æŸ¥çœ‹ç»“æœ
ls -la results/
cat results/comprehensive_report_*.json | jq '.'
```

è¿™ä¸ªå®Œæ•´çš„æŒ‡å—æ¶µç›–äº†ï¼š

1. **APIæœåŠ¡å™¨å¯åŠ¨å’Œé…ç½®**
2. **è®¤è¯å’Œæˆæƒæœºåˆ¶**
3. **åŸºç¡€å’Œé«˜çº§APIè°ƒç”¨**
4. **WebSocketå®æ—¶é€šä¿¡**
5. **é«˜çº§æ¨¡å‹é…ç½®ç®¡ç†**
6. **A/Bæµ‹è¯•å’Œæ€§èƒ½ç›‘æ§**
7. **æ‰¹é‡è¯„ä¼°å’Œæ¯”è¾ƒ**
8. **é…ç½®æ–‡ä»¶ç®¡ç†**
9. **å®Œæ•´çš„ç«¯åˆ°ç«¯æ‰§è¡Œæµç¨‹**

æ‰€æœ‰ç¤ºä¾‹éƒ½æ˜¯åŸºäºå®é™…çš„ä»£ç ç»“æ„ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨å’Œæ‰©å±•ã€‚