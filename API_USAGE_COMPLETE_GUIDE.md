# AI Evaluation Engine API å®Œæ•´ä½¿ç”¨æŒ‡å—

## ğŸ¯ æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»å¦‚ä½•å¯åŠ¨AI Evaluation Engine APIæœåŠ¡å™¨ï¼Œå¹¶é€šè¿‡APIæ‰§è¡ŒçœŸå®çš„lm-evalè¯„ä¼°ä»»åŠ¡ã€‚

## ğŸ“‹ ç›®å½•

1. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
2. [å¯åŠ¨APIæœåŠ¡å™¨](#å¯åŠ¨apiæœåŠ¡å™¨)
3. [APIè®¤è¯](#apiè®¤è¯)
4. [è·å–å¯ç”¨ä»»åŠ¡å’Œæ¨¡å‹](#è·å–å¯ç”¨ä»»åŠ¡å’Œæ¨¡å‹)
5. [åˆ›å»ºå’Œæ‰§è¡Œè¯„ä¼°ä»»åŠ¡](#åˆ›å»ºå’Œæ‰§è¡Œè¯„ä¼°ä»»åŠ¡)
6. [ç›‘æ§è¯„ä¼°è¿›åº¦](#ç›‘æ§è¯„ä¼°è¿›åº¦)
7. [è·å–è¯„ä¼°ç»“æœ](#è·å–è¯„ä¼°ç»“æœ)
8. [å®Œæ•´ç¤ºä¾‹](#å®Œæ•´ç¤ºä¾‹)
9. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

## ğŸ”§ ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…ä¾èµ–

```bash
# å®‰è£…åŸºç¡€ä¾èµ–
pip install fastapi uvicorn pydantic PyJWT python-multipart

# ç¡®ä¿lm-evalæ¡†æ¶å·²å®‰è£…
pip install lm-eval
```

### 2. é…ç½®APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰

å¦‚æœè¦ä½¿ç”¨çœŸå®çš„AIæ¨¡å‹ï¼Œéœ€è¦é…ç½®ç›¸åº”çš„APIå¯†é’¥ï¼š

```bash
# åˆ›å»º.envæ–‡ä»¶æˆ–è®¾ç½®ç¯å¢ƒå˜é‡
export ANTHROPIC_API_KEY="your_anthropic_key"
export OPENAI_API_KEY="your_openai_key"
export DEEPSEEK_API_KEY="your_deepseek_key"
export DASHSCOPE_API_KEY="your_dashscope_key"
```

### 3. éªŒè¯ç¯å¢ƒ

```bash
# éªŒè¯lm-evalå®‰è£…
python -m lm_eval --help

# éªŒè¯ä»»åŠ¡å¯ç”¨æ€§
python -c "from lm_eval.tasks import TaskManager; tm = TaskManager(); print(f'å¯ç”¨ä»»åŠ¡æ•°: {len(tm.all_tasks)}')"
```

## ğŸš€ å¯åŠ¨APIæœåŠ¡å™¨

### æ–¹æ³•1ï¼šä½¿ç”¨çœŸå®è¯„ä¼°æœåŠ¡å™¨ï¼ˆæ¨èï¼‰

```bash
# å¯åŠ¨çœŸå®çš„APIæœåŠ¡å™¨
python real_api_server.py
```

### æ–¹æ³•2ï¼šä½¿ç”¨ç®€åŒ–æµ‹è¯•æœåŠ¡å™¨

```bash
# å¯åŠ¨ç®€åŒ–ç‰ˆæœåŠ¡å™¨ï¼ˆä»…ç”¨äºæµ‹è¯•APIæ¥å£ï¼‰
python simple_api_server.py
```

### æœåŠ¡å™¨å¯åŠ¨ä¿¡æ¯

å¯åŠ¨æˆåŠŸåï¼Œä½ ä¼šçœ‹åˆ°ç±»ä¼¼è¾“å‡ºï¼š

```
ğŸš€ å¯åŠ¨çœŸå®çš„ AI Evaluation Engine API æœåŠ¡å™¨
============================================================
ğŸŒ æœåŠ¡å™¨é…ç½®:
   - ä¸»æœº: 0.0.0.0
   - ç«¯å£: 8000
   - APIæ–‡æ¡£: http://localhost:8000/docs
   - å¥åº·æ£€æŸ¥: http://localhost:8000/health
ğŸ” é»˜è®¤ç”¨æˆ·è´¦å·:
   - ç®¡ç†å‘˜: admin / admin123
   - è¯„ä¼°å‘˜: evaluator / eval123
ğŸ“‹ å¯ç”¨ä»»åŠ¡:
   - single_turn_scenarios_function_generation: Function Generation
   - single_turn_scenarios_code_completion: Code Completion
   - single_turn_scenarios_bug_fix: Bug Fix
   ... è¿˜æœ‰ 15 ä¸ªä»»åŠ¡
ğŸ¤– å¯ç”¨æ¨¡å‹:
   - claude-local: Claude (Local)
   - openai-completions: GPT-3.5 Turbo
   - deepseek: DeepSeek Coder
ğŸš€ å¯åŠ¨æœåŠ¡å™¨...
```

## ğŸ” APIè®¤è¯

### 1. å¥åº·æ£€æŸ¥ï¼ˆæ— éœ€è®¤è¯ï¼‰

```bash
curl -X GET http://localhost:8000/health
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
  "status": "healthy",
  "timestamp": "2024-01-01T12:00:00.000000",
  "version": "1.0.0-real",
  "active_evaluations": 0,
  "available_tasks": 18,
  "available_models": 3
}
```

### 2. ç”¨æˆ·ç™»å½•è·å–è®¿é—®ä»¤ç‰Œ

```bash
curl -X POST http://localhost:8000/auth/login \
  -H "Content-Type: application/json" \
  -d '{
    "username": "admin",
    "password": "admin123"
  }'
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
  "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "token_type": "bearer",
  "expires_in": 3600,
  "user_info": {
    "user_id": "admin_001",
    "username": "admin",
    "roles": ["admin"]
  }
}
```

### 3. ä¿å­˜è®¿é—®ä»¤ç‰Œ

```bash
# ä¿å­˜ä»¤ç‰Œåˆ°ç¯å¢ƒå˜é‡
export ACCESS_TOKEN="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."

# æˆ–è€…åœ¨åç»­è¯·æ±‚ä¸­ç›´æ¥ä½¿ç”¨
TOKEN="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
```

## ğŸ“‹ è·å–å¯ç”¨ä»»åŠ¡å’Œæ¨¡å‹

### 1. è·å–ä»»åŠ¡åˆ—è¡¨

```bash
curl -X GET "http://localhost:8000/tasks?limit=10&category=single_turn" \
  -H "Authorization: Bearer $ACCESS_TOKEN"
```

å“åº”ç¤ºä¾‹ï¼š
```json
[
  {
    "task_id": "single_turn_scenarios_function_generation",
    "name": "Function Generation",
    "category": "single_turn",
    "difficulty": "intermediate",
    "description": "Evaluate function generation capabilities",
    "languages": ["python"],
    "tags": ["coding", "function_generation"],
    "estimated_duration": 60
  },
  {
    "task_id": "single_turn_scenarios_code_completion",
    "name": "Code Completion",
    "category": "single_turn",
    "difficulty": "beginner",
    "description": "Evaluate code completion capabilities",
    "languages": ["python"],
    "tags": ["coding", "code_completion"],
    "estimated_duration": 60
  }
]
```

### 2. è·å–æ¨¡å‹åˆ—è¡¨

```bash
curl -X GET http://localhost:8000/models \
  -H "Authorization: Bearer $ACCESS_TOKEN"
```

å“åº”ç¤ºä¾‹ï¼š
```json
[
  {
    "model_id": "claude-local",
    "name": "Claude (Local)",
    "provider": "anthropic",
    "version": "3-haiku",
    "capabilities": ["text_generation", "code_completion"],
    "supported_tasks": ["single_turn_scenarios"],
    "rate_limits": {
      "requests_per_minute": 60,
      "tokens_per_minute": 100000
    },
    "cost_per_token": 0.00025,
    "model_args": "model=claude-3-haiku-20240307"
  }
]
```

## ğŸ¯ åˆ›å»ºå’Œæ‰§è¡Œè¯„ä¼°ä»»åŠ¡

### 1. åˆ›å»ºè¯„ä¼°ä»»åŠ¡

```bash
curl -X POST http://localhost:8000/evaluations \
  -H "Authorization: Bearer $ACCESS_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "model_id": "claude-local",
    "task_ids": [
      "single_turn_scenarios_function_generation",
      "single_turn_scenarios_code_completion"
    ],
    "configuration": {
      "temperature": 0.7,
      "max_tokens": 1024,
      "limit": 3
    },
    "metadata": {
      "experiment_name": "api_demo_evaluation",
      "description": "é€šè¿‡APIæ‰§è¡Œçš„æ¼”ç¤ºè¯„ä¼°",
      "tags": ["demo", "api"]
    }
  }'
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
  "evaluation_id": "eval_a1b2c3d4e5f6",
  "status": "created",
  "message": "Evaluation created and started",
  "created_at": "2024-01-01T12:00:00.000000"
}
```

### 2. ä»»åŠ¡é…ç½®å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `model_id` | string | æ˜¯ | è¦ä½¿ç”¨çš„æ¨¡å‹ID |
| `task_ids` | array | æ˜¯ | è¦æ‰§è¡Œçš„ä»»åŠ¡IDåˆ—è¡¨ |
| `configuration.limit` | integer | å¦ | æ¯ä¸ªä»»åŠ¡çš„æ ·æœ¬æ•°é‡é™åˆ¶ï¼ˆé»˜è®¤3ï¼‰ |
| `configuration.temperature` | float | å¦ | æ¨¡å‹æ¸©åº¦å‚æ•°ï¼ˆ0.0-1.0ï¼‰ |
| `configuration.max_tokens` | integer | å¦ | æœ€å¤§ç”Ÿæˆtokenæ•° |
| `metadata` | object | å¦ | è¯„ä¼°å…ƒæ•°æ® |

## ğŸ“Š ç›‘æ§è¯„ä¼°è¿›åº¦

### 1. æŸ¥çœ‹è¯„ä¼°çŠ¶æ€

```bash
EVAL_ID="eval_a1b2c3d4e5f6"

curl -X GET http://localhost:8000/evaluations/$EVAL_ID \
  -H "Authorization: Bearer $ACCESS_TOKEN"
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
  "evaluation_id": "eval_a1b2c3d4e5f6",
  "status": "running",
  "progress": 0.5,
  "model_id": "claude-local",
  "task_ids": [
    "single_turn_scenarios_function_generation",
    "single_turn_scenarios_code_completion"
  ],
  "created_at": "2024-01-01T12:00:00.000000",
  "start_time": "2024-01-01T12:00:05.000000",
  "error": null
}
```

### 2. çŠ¶æ€è¯´æ˜

| çŠ¶æ€ | è¯´æ˜ |
|------|------|
| `created` | è¯„ä¼°å·²åˆ›å»ºï¼Œç­‰å¾…æ‰§è¡Œ |
| `running` | è¯„ä¼°æ­£åœ¨æ‰§è¡Œä¸­ |
| `completed` | è¯„ä¼°å·²å®Œæˆ |
| `failed` | è¯„ä¼°æ‰§è¡Œå¤±è´¥ |

### 3. è½®è¯¢çŠ¶æ€ï¼ˆShellè„šæœ¬ç¤ºä¾‹ï¼‰

```bash
#!/bin/bash
EVAL_ID="eval_a1b2c3d4e5f6"
TOKEN="your_access_token"

echo "ç›‘æ§è¯„ä¼°è¿›åº¦..."
while true; do
    STATUS=$(curl -s -X GET http://localhost:8000/evaluations/$EVAL_ID \
        -H "Authorization: Bearer $TOKEN" | \
        python3 -c "import sys,json; print(json.load(sys.stdin)['status'])")
    
    echo "å½“å‰çŠ¶æ€: $STATUS"
    
    if [[ "$STATUS" == "completed" || "$STATUS" == "failed" ]]; then
        break
    fi
    
    sleep 10
done

echo "è¯„ä¼°å®Œæˆï¼ŒçŠ¶æ€: $STATUS"
```

## ğŸ“ˆ è·å–è¯„ä¼°ç»“æœ

### 1. è·å–åŸºç¡€ç»“æœ

```bash
curl -X GET http://localhost:8000/results/$EVAL_ID \
  -H "Authorization: Bearer $ACCESS_TOKEN"
```

### 2. è·å–è¯¦ç»†ç»“æœ

```bash
curl -X GET "http://localhost:8000/results/$EVAL_ID?include_details=true" \
  -H "Authorization: Bearer $ACCESS_TOKEN"
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
  "evaluation_id": "eval_a1b2c3d4e5f6",
  "model_id": "claude-local",
  "task_results": [
    {
      "task_id": "single_turn_scenarios_function_generation",
      "status": "completed",
      "score": 0.85,
      "metrics": {
        "accuracy": 0.8,
        "completeness": 0.9,
        "quality": 0.85
      },
      "execution_time": 45.2
    },
    {
      "task_id": "single_turn_scenarios_code_completion",
      "status": "completed",
      "score": 0.78,
      "metrics": {
        "accuracy": 0.75,
        "completeness": 0.8,
        "quality": 0.78
      },
      "execution_time": 32.1
    }
  ],
  "summary_metrics": {
    "overall_score": 0.815,
    "total_tasks": 2,
    "completed_tasks": 2
  }
}
```

## ğŸ”„ å®Œæ•´ç¤ºä¾‹

### Pythonè„šæœ¬ç¤ºä¾‹

```python
#!/usr/bin/env python3
"""
å®Œæ•´çš„APIä½¿ç”¨ç¤ºä¾‹
"""

import requests
import json
import time

class LMEvalAPIClient:
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.session = requests.Session()
    
    def login(self, username="admin", password="admin123"):
        """ç™»å½•è·å–è®¿é—®ä»¤ç‰Œ"""
        response = self.session.post(
            f"{self.base_url}/auth/login",
            json={"username": username, "password": password}
        )
        
        if response.status_code == 200:
            data = response.json()
            token = data["access_token"]
            self.session.headers.update({"Authorization": f"Bearer {token}"})
            print(f"âœ… ç™»å½•æˆåŠŸ: {data['user_info']['username']}")
            return True
        else:
            print(f"âŒ ç™»å½•å¤±è´¥: {response.text}")
            return False
    
    def get_tasks(self, category=None, limit=10):
        """è·å–ä»»åŠ¡åˆ—è¡¨"""
        params = {"limit": limit}
        if category:
            params["category"] = category
        
        response = self.session.get(f"{self.base_url}/tasks", params=params)
        if response.status_code == 200:
            return response.json()
        return []
    
    def get_models(self):
        """è·å–æ¨¡å‹åˆ—è¡¨"""
        response = self.session.get(f"{self.base_url}/models")
        if response.status_code == 200:
            return response.json()
        return []
    
    def create_evaluation(self, model_id, task_ids, config=None):
        """åˆ›å»ºè¯„ä¼°ä»»åŠ¡"""
        payload = {
            "model_id": model_id,
            "task_ids": task_ids,
            "configuration": config or {"limit": 3, "temperature": 0.7},
            "metadata": {"experiment_name": "python_api_demo"}
        }
        
        response = self.session.post(f"{self.base_url}/evaluations", json=payload)
        if response.status_code == 200:
            return response.json()["evaluation_id"]
        else:
            print(f"âŒ åˆ›å»ºè¯„ä¼°å¤±è´¥: {response.text}")
            return None
    
    def wait_for_completion(self, evaluation_id, timeout=300):
        """ç­‰å¾…è¯„ä¼°å®Œæˆ"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            response = self.session.get(f"{self.base_url}/evaluations/{evaluation_id}")
            if response.status_code == 200:
                status_data = response.json()
                status = status_data["status"]
                
                print(f"çŠ¶æ€: {status}")
                
                if status in ["completed", "failed"]:
                    return status
                
                time.sleep(10)
            else:
                print(f"âŒ è·å–çŠ¶æ€å¤±è´¥: {response.text}")
                break
        
        return "timeout"
    
    def get_results(self, evaluation_id, include_details=True):
        """è·å–è¯„ä¼°ç»“æœ"""
        params = {"include_details": include_details}
        response = self.session.get(f"{self.base_url}/results/{evaluation_id}", params=params)
        
        if response.status_code == 200:
            return response.json()
        else:
            print(f"âŒ è·å–ç»“æœå¤±è´¥: {response.text}")
            return None

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ LM-Eval API å®Œæ•´ç¤ºä¾‹")
    print("=" * 50)
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = LMEvalAPIClient()
    
    # 1. ç™»å½•
    if not client.login():
        return
    
    # 2. è·å–å¯ç”¨èµ„æº
    print("\nğŸ“‹ è·å–å¯ç”¨ä»»åŠ¡...")
    tasks = client.get_tasks(category="single_turn", limit=5)
    for task in tasks:
        print(f"  - {task['task_id']}: {task['name']}")
    
    print("\nğŸ¤– è·å–å¯ç”¨æ¨¡å‹...")
    models = client.get_models()
    for model in models:
        print(f"  - {model['model_id']}: {model['name']}")
    
    # 3. åˆ›å»ºè¯„ä¼°
    print("\nğŸš€ åˆ›å»ºè¯„ä¼°ä»»åŠ¡...")
    evaluation_id = client.create_evaluation(
        model_id="claude-local",
        task_ids=[
            "single_turn_scenarios_function_generation",
            "single_turn_scenarios_code_completion"
        ],
        config={"limit": 2, "temperature": 0.7}
    )
    
    if not evaluation_id:
        return
    
    print(f"âœ… è¯„ä¼°ä»»åŠ¡å·²åˆ›å»º: {evaluation_id}")
    
    # 4. ç­‰å¾…å®Œæˆ
    print("\nâ³ ç­‰å¾…è¯„ä¼°å®Œæˆ...")
    final_status = client.wait_for_completion(evaluation_id)
    
    # 5. è·å–ç»“æœ
    if final_status == "completed":
        print("\nğŸ“Š è·å–è¯„ä¼°ç»“æœ...")
        results = client.get_results(evaluation_id)
        
        if results:
            print(f"æ€»ä½“åˆ†æ•°: {results['summary_metrics']['overall_score']:.3f}")
            print("ä»»åŠ¡ç»“æœ:")
            for task_result in results["task_results"]:
                print(f"  - {task_result['task_id']}: {task_result['score']:.3f}")
    else:
        print(f"âŒ è¯„ä¼°æœªæˆåŠŸå®Œæˆ: {final_status}")

if __name__ == "__main__":
    main()
```

### Bashè„šæœ¬ç¤ºä¾‹

```bash
#!/bin/bash

# é…ç½®
BASE_URL="http://localhost:8000"
USERNAME="admin"
PASSWORD="admin123"

echo "ğŸ¯ LM-Eval API å®Œæ•´ç¤ºä¾‹"
echo "=========================="

# 1. ç™»å½•è·å–ä»¤ç‰Œ
echo "ğŸ” æ­£åœ¨ç™»å½•..."
LOGIN_RESPONSE=$(curl -s -X POST "$BASE_URL/auth/login" \
  -H "Content-Type: application/json" \
  -d "{\"username\": \"$USERNAME\", \"password\": \"$PASSWORD\"}")

ACCESS_TOKEN=$(echo "$LOGIN_RESPONSE" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    print(data['access_token'])
except:
    print('')
")

if [ -z "$ACCESS_TOKEN" ]; then
    echo "âŒ ç™»å½•å¤±è´¥"
    exit 1
fi

echo "âœ… ç™»å½•æˆåŠŸ"

# 2. è·å–ä»»åŠ¡åˆ—è¡¨
echo -e "\nğŸ“‹ è·å–ä»»åŠ¡åˆ—è¡¨..."
curl -s -X GET "$BASE_URL/tasks?limit=5" \
  -H "Authorization: Bearer $ACCESS_TOKEN" | \
  python3 -c "
import sys, json
data = json.load(sys.stdin)
for task in data:
    print(f\"  - {task['task_id']}: {task['name']}\")
"

# 3. åˆ›å»ºè¯„ä¼°
echo -e "\nğŸš€ åˆ›å»ºè¯„ä¼°ä»»åŠ¡..."
EVAL_RESPONSE=$(curl -s -X POST "$BASE_URL/evaluations" \
  -H "Authorization: Bearer $ACCESS_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "model_id": "claude-local",
    "task_ids": ["single_turn_scenarios_function_generation"],
    "configuration": {"limit": 2, "temperature": 0.7},
    "metadata": {"experiment_name": "bash_api_demo"}
  }')

EVAL_ID=$(echo "$EVAL_RESPONSE" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    print(data['evaluation_id'])
except:
    print('')
")

if [ -z "$EVAL_ID" ]; then
    echo "âŒ åˆ›å»ºè¯„ä¼°å¤±è´¥"
    exit 1
fi

echo "âœ… è¯„ä¼°ä»»åŠ¡å·²åˆ›å»º: $EVAL_ID"

# 4. ç›‘æ§è¿›åº¦
echo -e "\nâ³ ç›‘æ§è¯„ä¼°è¿›åº¦..."
while true; do
    STATUS_RESPONSE=$(curl -s -X GET "$BASE_URL/evaluations/$EVAL_ID" \
      -H "Authorization: Bearer $ACCESS_TOKEN")
    
    STATUS=$(echo "$STATUS_RESPONSE" | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    print(data['status'])
except:
    print('error')
")
    
    echo "å½“å‰çŠ¶æ€: $STATUS"
    
    if [[ "$STATUS" == "completed" || "$STATUS" == "failed" ]]; then
        break
    fi
    
    sleep 10
done

# 5. è·å–ç»“æœ
if [ "$STATUS" == "completed" ]; then
    echo -e "\nğŸ“Š è·å–è¯„ä¼°ç»“æœ..."
    curl -s -X GET "$BASE_URL/results/$EVAL_ID?include_details=false" \
      -H "Authorization: Bearer $ACCESS_TOKEN" | \
      python3 -c "
import sys, json
data = json.load(sys.stdin)
print(f\"æ€»ä½“åˆ†æ•°: {data['summary_metrics']['overall_score']:.3f}\")
print(\"ä»»åŠ¡ç»“æœ:\")
for task in data['task_results']:
    print(f\"  - {task['task_id']}: {task['score']:.3f}\")
"
else
    echo "âŒ è¯„ä¼°å¤±è´¥"
fi

echo -e "\nâœ¨ ç¤ºä¾‹å®Œæˆï¼"
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. æœåŠ¡å™¨å¯åŠ¨å¤±è´¥

**é—®é¢˜**: `ModuleNotFoundError: No module named 'fastapi'`

**è§£å†³æ–¹æ¡ˆ**:
```bash
pip install fastapi uvicorn pydantic PyJWT python-multipart
```

#### 2. ä»»åŠ¡å‘ç°å¤±è´¥

**é—®é¢˜**: å¯ç”¨ä»»åŠ¡æ•°ä¸º0

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥lm-evalå®‰è£…
python -m lm_eval --help

# æ£€æŸ¥ä»»åŠ¡ç›®å½•
ls -la lm_eval/tasks/single_turn_scenarios/

# é‡æ–°å®‰è£…lm-eval
pip uninstall lm-eval
pip install lm-eval
```

#### 3. æ¨¡å‹è°ƒç”¨å¤±è´¥

**é—®é¢˜**: è¯„ä¼°ä»»åŠ¡å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯æ˜¾ç¤ºAPIå¯†é’¥é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**:
```bash
# è®¾ç½®APIå¯†é’¥
export ANTHROPIC_API_KEY="your_key"
export OPENAI_API_KEY="your_key"

# æˆ–è€…ä½¿ç”¨dummyæ¨¡å‹è¿›è¡Œæµ‹è¯•
# åœ¨è¯„ä¼°è¯·æ±‚ä¸­ä½¿ç”¨ "model_id": "dummy"
```

#### 4. è®¤è¯å¤±è´¥

**é—®é¢˜**: `401 Unauthorized`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ä»¤ç‰Œæ˜¯å¦è¿‡æœŸï¼ˆæœ‰æ•ˆæœŸ1å°æ—¶ï¼‰
# é‡æ–°ç™»å½•è·å–æ–°ä»¤ç‰Œ
curl -X POST http://localhost:8000/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username": "admin", "password": "admin123"}'
```

### è°ƒè¯•æŠ€å·§

#### 1. å¯ç”¨è¯¦ç»†æ—¥å¿—

```bash
# è®¾ç½®æ—¥å¿—çº§åˆ«
export PYTHONPATH=.
python -c "
import logging
logging.basicConfig(level=logging.DEBUG)
" real_api_server.py
```

#### 2. æ£€æŸ¥APIæ–‡æ¡£

è®¿é—® http://localhost:8000/docs æŸ¥çœ‹å®Œæ•´çš„APIæ–‡æ¡£å’Œäº¤äº’å¼æµ‹è¯•ç•Œé¢ã€‚

#### 3. éªŒè¯è¯·æ±‚æ ¼å¼

```bash
# ä½¿ç”¨jqéªŒè¯JSONæ ¼å¼
echo '{"model_id": "claude-local", "task_ids": ["single_turn_scenarios_function_generation"]}' | jq .
```

## ğŸ“š è¿›é˜¶ç”¨æ³•

### 1. æ‰¹é‡è¯„ä¼°

```python
# æ‰¹é‡åˆ›å»ºå¤šä¸ªè¯„ä¼°ä»»åŠ¡
models = ["claude-local", "openai-completions", "deepseek"]
tasks = ["single_turn_scenarios_function_generation", "single_turn_scenarios_code_completion"]

evaluation_ids = []
for model in models:
    eval_id = client.create_evaluation(model, tasks)
    if eval_id:
        evaluation_ids.append(eval_id)

# ç­‰å¾…æ‰€æœ‰è¯„ä¼°å®Œæˆ
for eval_id in evaluation_ids:
    status = client.wait_for_completion(eval_id)
    print(f"è¯„ä¼° {eval_id}: {status}")
```

### 2. è‡ªå®šä¹‰é…ç½®

```python
# é«˜çº§é…ç½®ç¤ºä¾‹
config = {
    "limit": 10,           # æ¯ä¸ªä»»åŠ¡çš„æ ·æœ¬æ•°
    "temperature": 0.3,    # è¾ƒä½æ¸©åº¦ï¼Œæ›´ç¡®å®šæ€§çš„è¾“å‡º
    "max_tokens": 2048,    # æ›´å¤§çš„è¾“å‡ºé•¿åº¦
    "batch_size": 1,       # æ‰¹å¤„ç†å¤§å°
    "num_fewshot": 0       # few-shotç¤ºä¾‹æ•°é‡
}

eval_id = client.create_evaluation("claude-local", task_ids, config)
```

### 3. ç»“æœåˆ†æ

```python
# åˆ†æå¤šä¸ªè¯„ä¼°ç»“æœ
def analyze_results(evaluation_ids):
    results = []
    for eval_id in evaluation_ids:
        result = client.get_results(eval_id)
        if result:
            results.append(result)
    
    # è®¡ç®—å¹³å‡åˆ†æ•°
    avg_scores = {}
    for result in results:
        model_id = result["model_id"]
        overall_score = result["summary_metrics"]["overall_score"]
        
        if model_id not in avg_scores:
            avg_scores[model_id] = []
        avg_scores[model_id].append(overall_score)
    
    # è¾“å‡ºæ¯”è¾ƒç»“æœ
    for model_id, scores in avg_scores.items():
        avg_score = sum(scores) / len(scores)
        print(f"{model_id}: å¹³å‡åˆ†æ•° {avg_score:.3f}")
```

## ğŸ“– ç›¸å…³æ–‡æ¡£

- [lm-evalå®˜æ–¹æ–‡æ¡£](https://github.com/EleutherAI/lm-evaluation-harness)
- [FastAPIæ–‡æ¡£](https://fastapi.tiangolo.com/)
- [APIäº¤äº’å¼æ–‡æ¡£](http://localhost:8000/docs)ï¼ˆæœåŠ¡å™¨å¯åŠ¨åè®¿é—®ï¼‰

## ğŸ¤ æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·ï¼š

1. æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—è¾“å‡º
2. è®¿é—® http://localhost:8000/docs æŸ¥çœ‹APIæ–‡æ¡£
3. è¿è¡Œå¥åº·æ£€æŸ¥ç¡®è®¤æœåŠ¡çŠ¶æ€
4. æŸ¥çœ‹æœ¬æŒ‡å—çš„æ•…éšœæ’é™¤éƒ¨åˆ†

---

**æœ€åæ›´æ–°**: 2024å¹´1æœˆ1æ—¥
**ç‰ˆæœ¬**: 1.0.0