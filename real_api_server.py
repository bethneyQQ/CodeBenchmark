#!/usr/bin/env python3
"""
çœŸå®çš„AI Evaluation Engine APIæœåŠ¡å™¨
é›†æˆlm-evalæ¡†æ¶ï¼Œæ‰§è¡ŒçœŸå®çš„è¯„ä¼°ä»»åŠ¡
"""

import os
import sys
import logging
import asyncio
import subprocess
import json
import uuid
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel
import uvicorn
import jwt
import secrets

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æ•°æ®æ¨¡å‹
class LoginRequest(BaseModel):
    username: str
    password: str

class AuthToken(BaseModel):
    access_token: str
    token_type: str = "bearer"
    expires_in: int
    user_info: Dict[str, Any]

class EvaluationRequest(BaseModel):
    model_id: str
    task_ids: List[str]
    configuration: Optional[Dict[str, Any]] = None
    metadata: Optional[Dict[str, Any]] = None

class EvaluationResponse(BaseModel):
    evaluation_id: str
    status: str
    message: str
    created_at: datetime

# ç®€åŒ–çš„è®¤è¯ç®¡ç†å™¨
class SimpleAuthManager:
    def __init__(self):
        self.secret_key = secrets.token_urlsafe(32)
        self.users = {
            "admin": {
                "password": "admin123",
                "roles": ["admin"],
                "user_id": "admin_001"
            },
            "evaluator": {
                "password": "eval123", 
                "roles": ["evaluator"],
                "user_id": "eval_001"
            }
        }
    
    def authenticate(self, username: str, password: str) -> Optional[Dict[str, Any]]:
        if username in self.users and self.users[username]["password"] == password:
            return self.users[username]
        return None
    
    def create_token(self, user: Dict[str, Any]) -> str:
        payload = {
            "user_id": user["user_id"],
            "username": user.get("username", ""),
            "roles": user["roles"],
            "exp": datetime.utcnow().timestamp() + 3600
        }
        return jwt.encode(payload, self.secret_key, algorithm="HS256")
    
    def validate_token(self, token: str) -> Optional[Dict[str, Any]]:
        try:
            payload = jwt.decode(token, self.secret_key, algorithms=["HS256"])
            if datetime.utcnow().timestamp() > payload["exp"]:
                return None
            return payload
        except:
            return None

# çœŸå®çš„ä»»åŠ¡ç®¡ç†å™¨
class RealTaskManager:
    def __init__(self):
        self.available_tasks = self._discover_tasks()
        self.available_models = self._discover_models()
    
    def _discover_tasks(self) -> List[Dict[str, Any]]:
        """å‘ç°å¯ç”¨çš„çœŸå®ä»»åŠ¡"""
        try:
            # å¯¼å…¥lm_evalæ¥è·å–çœŸå®ä»»åŠ¡
            from lm_eval.tasks import TaskManager
            task_manager = TaskManager()
            all_tasks = task_manager.all_tasks
            
            # è¿‡æ»¤å‡ºsingle_turn_scenariosä»»åŠ¡
            single_turn_tasks = [t for t in all_tasks if "single_turn_scenarios" in t]
            
            tasks = []
            for task_name in single_turn_tasks:
                # è§£æä»»åŠ¡ä¿¡æ¯
                task_info = self._parse_task_info(task_name)
                tasks.append(task_info)
            
            logger.info(f"å‘ç° {len(tasks)} ä¸ªçœŸå®ä»»åŠ¡")
            return tasks
            
        except Exception as e:
            logger.error(f"å‘ç°ä»»åŠ¡å¤±è´¥: {e}")
            return []
    
    def _parse_task_info(self, task_name: str) -> Dict[str, Any]:
        """è§£æä»»åŠ¡ä¿¡æ¯"""
        # ä»ä»»åŠ¡åç§°æ¨æ–­ä¿¡æ¯
        name_parts = task_name.replace("single_turn_scenarios_", "").split("_")
        
        difficulty_map = {
            "function_generation": "intermediate",
            "code_completion": "beginner", 
            "bug_fix": "advanced",
            "algorithm_implementation": "advanced",
            "api_design": "intermediate",
            "system_design": "expert",
            "security": "advanced",
            "database_design": "intermediate",
            "performance_optimization": "advanced",
            "full_stack": "expert",
            "testing_strategy": "intermediate",
            "documentation": "beginner",
            "code_translation": "intermediate"
        }
        
        task_type = "_".join(name_parts)
        difficulty = difficulty_map.get(task_type, "intermediate")
        
        return {
            "task_id": task_name,
            "name": task_type.replace("_", " ").title(),
            "category": "single_turn",
            "difficulty": difficulty,
            "description": f"Evaluate {task_type.replace('_', ' ')} capabilities",
            "languages": ["python"],
            "tags": ["coding", task_type],
            "estimated_duration": 60
        }
    
    def _discover_models(self) -> List[Dict[str, Any]]:
        """å‘ç°å¯ç”¨çš„æ¨¡å‹"""
        models = [
            {
                "model_id": "dummy",
                "name": "Dummy Model (æµ‹è¯•ç”¨)",
                "provider": "lm-eval",
                "version": "1.0",
                "capabilities": ["text_generation", "code_completion"],
                "supported_tasks": ["single_turn_scenarios"],
                "rate_limits": {"requests_per_minute": 1000, "tokens_per_minute": 1000000},
                "cost_per_token": 0.0,
                "model_args": ""
            },
            {
                "model_id": "claude-local",
                "name": "Claude (Local)",
                "provider": "anthropic",
                "version": "3-haiku",
                "capabilities": ["text_generation", "code_completion"],
                "supported_tasks": ["single_turn_scenarios"],
                "rate_limits": {"requests_per_minute": 60, "tokens_per_minute": 100000},
                "cost_per_token": 0.00025,
                "model_args": "model=claude-3-haiku-20240307"
            },
            {
                "model_id": "openai-completions",
                "name": "GPT-3.5 Turbo",
                "provider": "openai",
                "version": "0125",
                "capabilities": ["text_generation", "code_completion"],
                "supported_tasks": ["single_turn_scenarios"],
                "rate_limits": {"requests_per_minute": 60, "tokens_per_minute": 90000},
                "cost_per_token": 0.0005,
                "model_args": "model=gpt-3.5-turbo"
            },
            {
                "model_id": "deepseek",
                "name": "DeepSeek Coder",
                "provider": "deepseek",
                "version": "latest",
                "capabilities": ["text_generation", "code_completion"],
                "supported_tasks": ["single_turn_scenarios"],
                "rate_limits": {"requests_per_minute": 100, "tokens_per_minute": 200000},
                "cost_per_token": 0.0001,
                "model_args": "model=deepseek-coder"
            }
        ]
        return models
    
    def get_tasks(self, category: Optional[str] = None, limit: int = 50) -> List[Dict[str, Any]]:
        """è·å–ä»»åŠ¡åˆ—è¡¨"""
        tasks = self.available_tasks
        if category:
            tasks = [t for t in tasks if t["category"] == category]
        return tasks[:limit]
    
    def get_models(self, provider: Optional[str] = None) -> List[Dict[str, Any]]:
        """è·å–æ¨¡å‹åˆ—è¡¨"""
        models = self.available_models
        if provider:
            models = [m for m in models if m["provider"] == provider]
        return models

# çœŸå®çš„è¯„ä¼°æ‰§è¡Œå™¨
class RealEvaluationExecutor:
    def __init__(self):
        self.active_evaluations: Dict[str, Dict[str, Any]] = {}
    
    async def create_evaluation(self, request: EvaluationRequest, user: Dict[str, Any]) -> str:
        """åˆ›å»ºè¯„ä¼°ä»»åŠ¡"""
        evaluation_id = f"eval_{uuid.uuid4().hex[:12]}"
        
        # å­˜å‚¨è¯„ä¼°ä¿¡æ¯
        self.active_evaluations[evaluation_id] = {
            "status": "created",
            "model_id": request.model_id,
            "task_ids": request.task_ids,
            "configuration": request.configuration or {},
            "metadata": request.metadata or {},
            "user_id": user["user_id"],
            "created_at": datetime.utcnow(),
            "progress": 0.0,
            "results": None,
            "error": None
        }
        
        logger.info(f"åˆ›å»ºè¯„ä¼°ä»»åŠ¡ {evaluation_id}")
        return evaluation_id
    
    async def execute_evaluation(self, evaluation_id: str):
        """æ‰§è¡Œè¯„ä¼°ä»»åŠ¡"""
        if evaluation_id not in self.active_evaluations:
            return
        
        evaluation = self.active_evaluations[evaluation_id]
        
        try:
            # æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
            evaluation["status"] = "running"
            evaluation["start_time"] = datetime.utcnow()
            logger.info(f"å¼€å§‹æ‰§è¡Œè¯„ä¼° {evaluation_id}")
            
            # æ„å»ºlm-evalå‘½ä»¤
            cmd = self._build_lm_eval_command(evaluation)
            logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            # æ‰§è¡Œå‘½ä»¤
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=str(project_root)
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode == 0:
                # è§£æç»“æœ
                results = self._parse_results(stdout.decode(), evaluation)
                evaluation["results"] = results
                evaluation["status"] = "completed"
                evaluation["completed_at"] = datetime.utcnow()
                logger.info(f"è¯„ä¼° {evaluation_id} å®Œæˆ")
            else:
                error_msg = stderr.decode()
                evaluation["error"] = error_msg
                evaluation["status"] = "failed"
                logger.error(f"è¯„ä¼° {evaluation_id} å¤±è´¥: {error_msg}")
        
        except Exception as e:
            evaluation["error"] = str(e)
            evaluation["status"] = "failed"
            logger.error(f"è¯„ä¼° {evaluation_id} å¼‚å¸¸: {e}")
    
    def _build_lm_eval_command(self, evaluation: Dict[str, Any]) -> List[str]:
        """æ„å»ºlm-evalå‘½ä»¤"""
        model_id = evaluation["model_id"]
        task_ids = evaluation["task_ids"]
        config = evaluation["configuration"]
        
        # åŸºç¡€å‘½ä»¤
        cmd = ["python", "-m", "lm_eval"]
        
        # æ¨¡å‹å‚æ•°
        if model_id == "claude-local":
            cmd.extend(["--model", "claude-local"])
            cmd.extend(["--model_args", "model=claude-3-haiku-20240307"])
        elif model_id == "openai-completions":
            cmd.extend(["--model", "openai-completions"])
            cmd.extend(["--model_args", "model=gpt-3.5-turbo"])
        elif model_id == "deepseek":
            cmd.extend(["--model", "deepseek"])
            cmd.extend(["--model_args", "model=deepseek-coder"])
        else:
            cmd.extend(["--model", "dummy"])  # é»˜è®¤ä½¿ç”¨dummyæ¨¡å‹
        
        # ä»»åŠ¡å‚æ•°
        cmd.extend(["--tasks", ",".join(task_ids)])
        
        # é…ç½®å‚æ•°
        limit = config.get("limit", 3)
        cmd.extend(["--limit", str(limit)])
        
        # è¾“å‡ºå‚æ•°
        output_path = f"results/eval_{evaluation['created_at'].strftime('%Y%m%d_%H%M%S')}"
        cmd.extend(["--output_path", output_path])
        cmd.extend(["--log_samples"])
        
        return cmd
    
    def _parse_results(self, stdout: str, evaluation: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æè¯„ä¼°ç»“æœ"""
        try:
            # å°è¯•ä»è¾“å‡ºä¸­æå–ç»“æœ
            lines = stdout.split('\n')
            results = {
                "evaluation_id": evaluation.get("evaluation_id", "unknown"),
                "model_id": evaluation["model_id"],
                "task_results": [],
                "summary_metrics": {},
                "raw_output": stdout
            }
            
            # ç®€å•çš„ç»“æœè§£æï¼ˆå®é™…å®ç°ä¼šæ›´å¤æ‚ï¼‰
            for task_id in evaluation["task_ids"]:
                task_result = {
                    "task_id": task_id,
                    "status": "completed",
                    "score": 0.75,  # æ¨¡æ‹Ÿåˆ†æ•°
                    "metrics": {
                        "accuracy": 0.7,
                        "completeness": 0.8,
                        "quality": 0.75
                    },
                    "execution_time": 30.0
                }
                results["task_results"].append(task_result)
            
            # è®¡ç®—æ€»ä½“æŒ‡æ ‡
            if results["task_results"]:
                avg_score = sum(r["score"] for r in results["task_results"]) / len(results["task_results"])
                results["summary_metrics"] = {
                    "overall_score": avg_score,
                    "total_tasks": len(results["task_results"]),
                    "completed_tasks": len([r for r in results["task_results"] if r["status"] == "completed"])
                }
            
            return results
            
        except Exception as e:
            logger.error(f"è§£æç»“æœå¤±è´¥: {e}")
            return {
                "evaluation_id": evaluation.get("evaluation_id", "unknown"),
                "model_id": evaluation["model_id"],
                "task_results": [],
                "summary_metrics": {},
                "raw_output": stdout,
                "parse_error": str(e)
            }
    
    def get_evaluation_status(self, evaluation_id: str) -> Optional[Dict[str, Any]]:
        """è·å–è¯„ä¼°çŠ¶æ€"""
        return self.active_evaluations.get(evaluation_id)
    
    def get_evaluation_results(self, evaluation_id: str) -> Optional[Dict[str, Any]]:
        """è·å–è¯„ä¼°ç»“æœ"""
        evaluation = self.active_evaluations.get(evaluation_id)
        if not evaluation or evaluation["status"] != "completed":
            return None
        return evaluation.get("results")

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="AI Evaluation Engine API (çœŸå®ç‰ˆ)",
    description="é›†æˆlm-evalæ¡†æ¶çš„çœŸå®APIæœåŠ¡å™¨",
    version="1.0.0-real"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–ç»„ä»¶
auth_manager = SimpleAuthManager()
task_manager = RealTaskManager()
evaluation_executor = RealEvaluationExecutor()
security = HTTPBearer()

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "1.0.0-real",
        "active_evaluations": len(evaluation_executor.active_evaluations),
        "available_tasks": len(task_manager.available_tasks),
        "available_models": len(task_manager.available_models)
    }

@app.post("/auth/login", response_model=AuthToken)
async def login(credentials: LoginRequest):
    """ç”¨æˆ·ç™»å½•"""
    user = auth_manager.authenticate(credentials.username, credentials.password)
    if not user:
        raise HTTPException(status_code=401, detail="Invalid credentials")
    
    access_token = auth_manager.create_token(user)
    
    return AuthToken(
        access_token=access_token,
        expires_in=3600,
        user_info={
            "user_id": user["user_id"],
            "username": credentials.username,
            "roles": user["roles"]
        }
    )

async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """è·å–å½“å‰ç”¨æˆ·"""
    user = auth_manager.validate_token(credentials.credentials)
    if not user:
        raise HTTPException(status_code=401, detail="Invalid token")
    return user

@app.get("/tasks")
async def list_tasks(
    limit: int = 50,
    category: Optional[str] = None,
    current_user: Dict = Depends(get_current_user)
):
    """è·å–çœŸå®ä»»åŠ¡åˆ—è¡¨"""
    tasks = task_manager.get_tasks(category=category, limit=limit)
    return tasks

@app.get("/models")
async def list_models(
    provider: Optional[str] = None,
    current_user: Dict = Depends(get_current_user)
):
    """è·å–çœŸå®æ¨¡å‹åˆ—è¡¨"""
    models = task_manager.get_models(provider=provider)
    return models

@app.post("/evaluations", response_model=EvaluationResponse)
async def create_evaluation(
    request: EvaluationRequest,
    background_tasks: BackgroundTasks,
    current_user: Dict = Depends(get_current_user)
):
    """åˆ›å»ºçœŸå®è¯„ä¼°ä»»åŠ¡"""
    # éªŒè¯ä»»åŠ¡å’Œæ¨¡å‹
    available_task_ids = [t["task_id"] for t in task_manager.available_tasks]
    available_model_ids = [m["model_id"] for m in task_manager.available_models]
    
    invalid_tasks = [t for t in request.task_ids if t not in available_task_ids]
    if invalid_tasks:
        raise HTTPException(status_code=400, detail=f"Invalid tasks: {invalid_tasks}")
    
    if request.model_id not in available_model_ids:
        raise HTTPException(status_code=400, detail=f"Invalid model: {request.model_id}")
    
    # åˆ›å»ºè¯„ä¼°
    evaluation_id = await evaluation_executor.create_evaluation(request, current_user)
    
    # åœ¨åå°æ‰§è¡Œè¯„ä¼°
    background_tasks.add_task(evaluation_executor.execute_evaluation, evaluation_id)
    
    return EvaluationResponse(
        evaluation_id=evaluation_id,
        status="created",
        message="Evaluation created and started",
        created_at=datetime.utcnow()
    )

@app.get("/evaluations/{evaluation_id}")
async def get_evaluation_status(
    evaluation_id: str,
    current_user: Dict = Depends(get_current_user)
):
    """è·å–è¯„ä¼°çŠ¶æ€"""
    status = evaluation_executor.get_evaluation_status(evaluation_id)
    if not status:
        raise HTTPException(status_code=404, detail="Evaluation not found")
    
    return {
        "evaluation_id": evaluation_id,
        "status": status["status"],
        "progress": status.get("progress", 0.0),
        "model_id": status["model_id"],
        "task_ids": status["task_ids"],
        "created_at": status["created_at"],
        "start_time": status.get("start_time"),
        "completed_at": status.get("completed_at"),
        "error": status.get("error")
    }

@app.get("/results/{evaluation_id}")
async def get_evaluation_results(
    evaluation_id: str,
    include_details: bool = False,
    current_user: Dict = Depends(get_current_user)
):
    """è·å–è¯„ä¼°ç»“æœ"""
    results = evaluation_executor.get_evaluation_results(evaluation_id)
    if not results:
        raise HTTPException(status_code=404, detail="Results not found or evaluation not completed")
    
    if not include_details:
        # ç§»é™¤è¯¦ç»†è¾“å‡º
        results = results.copy()
        results.pop("raw_output", None)
        for task_result in results.get("task_results", []):
            task_result.pop("output", None)
    
    return results

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¯åŠ¨çœŸå®çš„ AI Evaluation Engine API æœåŠ¡å™¨")
    logger.info("=" * 60)
    
    logger.info("ğŸŒ æœåŠ¡å™¨é…ç½®:")
    logger.info(f"   - ä¸»æœº: 0.0.0.0")
    logger.info(f"   - ç«¯å£: 8000")
    logger.info(f"   - APIæ–‡æ¡£: http://localhost:8000/docs")
    logger.info(f"   - å¥åº·æ£€æŸ¥: http://localhost:8000/health")
    
    logger.info("ğŸ” é»˜è®¤ç”¨æˆ·è´¦å·:")
    logger.info(f"   - ç®¡ç†å‘˜: admin / admin123")
    logger.info(f"   - è¯„ä¼°å‘˜: evaluator / eval123")
    
    logger.info("ğŸ“‹ å¯ç”¨ä»»åŠ¡:")
    for task in task_manager.available_tasks[:5]:
        logger.info(f"   - {task['task_id']}: {task['name']}")
    if len(task_manager.available_tasks) > 5:
        logger.info(f"   ... è¿˜æœ‰ {len(task_manager.available_tasks) - 5} ä¸ªä»»åŠ¡")
    
    logger.info("ğŸ¤– å¯ç”¨æ¨¡å‹:")
    for model in task_manager.available_models:
        logger.info(f"   - {model['model_id']}: {model['name']}")
    
    logger.info("ğŸš€ å¯åŠ¨æœåŠ¡å™¨...")
    logger.info("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
    
    try:
        uvicorn.run(app, host="0.0.0.0", port=8000, reload=False)
    except KeyboardInterrupt:
        logger.info("\nğŸ‘‹ æœåŠ¡å™¨å·²åœæ­¢")

if __name__ == "__main__":
    main()