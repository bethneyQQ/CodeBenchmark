{
  "results": {
    "single_turn_scenarios_code_completion": {
      "alias": "single_turn_scenarios_code_completion",
      "bypass,extract_code": 999,
      "bypass_stderr,extract_code": "N/A"
    }
  },
  "group_subtasks": {
    "single_turn_scenarios_code_completion": []
  },
  "configs": {
    "single_turn_scenarios_code_completion": {
      "task": "single_turn_scenarios_code_completion",
      "tag": [
        "single_turn_scenarios",
        "code_completion",
        "basic_scenario"
      ],
      "custom_dataset": "def load_dataset(metadata: Optional[Dict] = None, **kwargs) -> datasets.Dataset:\n    \"\"\"Load the problems.jsonl dataset with validation.\n    \n    Args:\n        metadata: Optional metadata for filtering\n        \n    Returns:\n        datasets.Dataset: Loaded and validated dataset\n        \n    Raises:\n        FileNotFoundError: If problems.jsonl file is not found\n        ValueError: If no valid problems are found in the dataset\n    \"\"\"\n    current_dir = Path(__file__).parent\n    problems_file = current_dir / \"problems.jsonl\"\n    \n    if not problems_file.exists():\n        raise FileNotFoundError(f\"Problems file not found: {problems_file}\")\n    \n    # Load JSONL data with comprehensive validation\n    data = []\n    validation_errors = []\n    \n    with open(problems_file, 'r', encoding='utf-8') as f:\n        for line_num, line in enumerate(f, 1):\n            line = line.strip()\n            if not line:\n                continue\n                \n            try:\n                problem = json.loads(line)\n                \n                # Validate problem schema\n                if validate_problem_schema(problem):\n                    data.append(problem)\n                else:\n                    validation_errors.append(f\"Line {line_num}: Schema validation failed for problem {problem.get('id', 'unknown')}\")\n                    \n            except json.JSONDecodeError as e:\n                validation_errors.append(f\"Line {line_num}: Invalid JSON - {e}\")\n                continue\n            except Exception as e:\n                validation_errors.append(f\"Line {line_num}: Unexpected error - {e}\")\n                continue\n    \n    # Log validation errors\n    if validation_errors:\n        eval_logger.warning(f\"Found {len(validation_errors)} validation errors:\")\n        for error in validation_errors[:10]:  # Log first 10 errors\n            eval_logger.warning(f\"  {error}\")\n        if len(validation_errors) > 10:\n            eval_logger.warning(f\"  ... and {len(validation_errors) - 10} more errors\")\n    \n    if not data:\n        raise ValueError(\"No valid problems found in dataset\")\n    \n    # Convert to HuggingFace dataset\n    dataset = datasets.Dataset.from_list(data)\n    \n    # Apply metadata filtering if provided\n    # Check for metadata in kwargs first, then use direct metadata parameter\n    filter_metadata = kwargs.get('metadata', metadata)\n    if filter_metadata:\n        dataset = filter_by_metadata(dataset, filter_metadata)\n    \n    eval_logger.info(f\"Loaded {len(dataset)} problems from {problems_file} ({len(validation_errors)} validation errors)\")\n    \n    # Return dataset in the format expected by lm-eval framework\n    return {\"test\": dataset}\n",
      "dataset_kwargs": {
        "metadata": {
          "scenario": "code_completion"
        }
      },
      "test_split": "test",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    \"\"\"Process documents for evaluation with comprehensive preprocessing.\n    \n    Args:\n        dataset: Raw dataset from load_dataset\n        \n    Returns:\n        datasets.Dataset: Processed dataset with applied context and validation\n    \"\"\"\n    def _process_doc(doc: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Process a single document.\"\"\"\n        try:\n            # Create a copy to avoid modifying original\n            processed_doc = doc.copy()\n            \n            # Validate essential fields\n            required_fields = ['id', 'prompt', 'context_mode']\n            for field in required_fields:\n                if field not in processed_doc:\n                    eval_logger.error(f\"Missing required field '{field}' in document {processed_doc.get('id', 'unknown')}\")\n                    # Use default values for missing fields\n                    if field == 'context_mode':\n                        processed_doc[field] = 'no_context'\n                    elif field == 'prompt':\n                        processed_doc[field] = ''\n            \n            # Apply context template based on context_mode\n            processed_doc = apply_context_template(processed_doc, processed_doc.get('context_mode', 'no_context'))\n            \n            # Ensure formatted_prompt exists\n            if 'formatted_prompt' not in processed_doc:\n                processed_doc['formatted_prompt'] = processed_doc.get('prompt', '')\n                \n            # Add processing metadata\n            processed_doc['_processed'] = True\n            \n            return processed_doc\n            \n        except Exception as e:\n            eval_logger.error(f\"Failed to process document {doc.get('id', 'unknown')}: {e}\")\n            # Return minimal viable document\n            return {\n                'id': doc.get('id', 'unknown'),\n                'prompt': doc.get('prompt', ''),\n                'formatted_prompt': doc.get('prompt', ''),\n                'context_mode': doc.get('context_mode', 'no_context'),\n                '_processed': False,\n                '_error': str(e)\n            }\n    \n    # Process all documents in the dataset\n    try:\n        processed_dataset = dataset.map(_process_doc)\n        eval_logger.info(f\"Processed {len(processed_dataset)} documents\")\n        return processed_dataset\n    except Exception as e:\n        eval_logger.error(f\"Failed to process dataset: {e}\")\n        return dataset\n",
      "doc_to_text": "def doc_to_text(doc: Dict[str, Any]) -> str:\n    \"\"\"Convert document to text for model input with context injection.\n    \n    Args:\n        doc: Processed document with applied context\n        \n    Returns:\n        str: Formatted text for model input\n        \n    Note:\n        This function expects the document to have been processed by process_docs()\n        which applies the appropriate context template.\n    \"\"\"\n    # Use formatted_prompt if available (from context application)\n    if 'formatted_prompt' in doc and doc['formatted_prompt']:\n        return doc['formatted_prompt']\n    \n    # Fallback to original prompt\n    prompt = doc.get('prompt', '')\n    if not prompt:\n        eval_logger.warning(f\"No prompt found in document {doc.get('id', 'unknown')}\")\n        return \"\"\n    \n    return prompt\n",
      "doc_to_target": "def doc_to_target(doc: Dict[str, Any]) -> str:\n    \"\"\"Extract target/reference from document for evaluation.\n    \n    Args:\n        doc: Document containing reference implementation\n        \n    Returns:\n        str: Target reference implementation or empty string if not available\n        \n    Note:\n        Returns the first reference if multiple references are provided.\n        This is used for metrics that compare against reference implementations.\n    \"\"\"\n    reference = doc.get('reference', [])\n    \n    # Handle list of references\n    if isinstance(reference, list):\n        if reference:\n            # Return first non-empty reference\n            for ref in reference:\n                if isinstance(ref, str) and ref.strip():\n                    return ref.strip()\n        return \"\"\n    \n    # Handle single string reference\n    elif isinstance(reference, str):\n        return reference.strip()\n    \n    # No reference available\n    else:\n        eval_logger.debug(f\"No reference found in document {doc.get('id', 'unknown')}\")\n        return \"\"\n",
      "unsafe_code": false,
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "bypass"
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "temperature": 0.0,
        "max_gen_toks": 512,
        "until": [],
        "do_sample": false
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "extract_code",
          "filter": [
            {
              "function": "custom",
              "filter_fn": "<function extract_code_response at 0x31ff8b920>"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "scenario": "code_completion",
        "description": "Code completion tasks across multiple programming languages",
        "supported_languages": [
          "python",
          "javascript",
          "typescript",
          "java",
          "cpp",
          "go",
          "rust"
        ],
        "model": "claude-3-haiku-20240307"
      }
    }
  },
  "versions": {
    "single_turn_scenarios_code_completion": 1.0
  },
  "n-shot": {
    "single_turn_scenarios_code_completion": 0
  },
  "higher_is_better": {
    "single_turn_scenarios_code_completion": {
      "bypass": true
    }
  },
  "n-samples": {
    "single_turn_scenarios_code_completion": {
      "original": 2,
      "effective": 1
    }
  },
  "config": {
    "model": "claude-local",
    "model_args": "model=claude-3-haiku-20240307",
    "batch_size": 1,
    "batch_sizes": [],
    "device": null,
    "use_cache": null,
    "limit": 1.0,
    "bootstrap_iters": 100000,
    "gen_kwargs": null,
    "random_seed": 0,
    "numpy_seed": 1234,
    "torch_seed": 1234,
    "fewshot_seed": 1234
  },
  "git_hash": "9153dffd",
  "date": 1758863256.430366,
  "pretty_env_info": "PyTorch version: 2.7.1\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: macOS 15.7 (arm64)\nGCC version: Could not collect\nClang version: 17.0.0 (clang-1700.3.19.1)\nCMake version: Could not collect\nLibc version: N/A\n\nPython version: 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:54:21) [Clang 16.0.6 ] (64-bit runtime)\nPython platform: macOS-15.7-arm64-arm-64bit\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nApple M1 Pro\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] optree==0.14.0\n[pip3] torch==2.7.1\n[pip3] torchmetrics==1.8.0\n[pip3] torchtext==0.18.0\n[conda] numpy                        1.26.4           pypi_0              pypi\n[conda] optree                       0.14.0           pypi_0              pypi\n[conda] torch                        2.7.1            pypi_0              pypi\n[conda] torchmetrics                 1.8.0            pypi_0              pypi\n[conda] torchtext                    0.18.0           pypi_0              pypi",
  "transformers_version": "4.54.0",
  "lm_eval_version": "0.4.9.1",
  "upper_git_hash": null,
  "task_hashes": {
    "single_turn_scenarios_code_completion": "03995b4236295cd2c5a2b06402f991d0aca2c15f89b238dbd6bd6fb8a346cd4e"
  },
  "model_source": "claude-local",
  "model_name": "claude-3-haiku-20240307",
  "model_name_sanitized": "claude-3-haiku-20240307",
  "system_instruction": null,
  "system_instruction_sha": null,
  "fewshot_as_multiturn": false,
  "chat_template": null,
  "chat_template_sha": null,
  "start_time": 90892.351975291,
  "end_time": 90900.218151916,
  "total_evaluation_time_seconds": "7.86617662498611"
}